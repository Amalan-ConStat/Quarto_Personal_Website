[
  {
    "objectID": "tidytuesday/index.html",
    "href": "tidytuesday/index.html",
    "title": "#TidyTuesday",
    "section": "",
    "text": "Welcome to #Tidytuesday related blog posts.\n\n\n\n\n\n\n\n  \n\n\n\n\nWeek 43: Horror Movies\n\n\n\n\n\n\n\nTidyTuesday\n\n\n2019\n\n\n\n\n\n\n\n\n\n\n\nOctober 22, 2019\n\n\n\n\n\n\n  \n\n\n\n\nWeek 34: Nuclear Explosions\n\n\n\n\n\n\n\nTidyTuesday\n\n\n2019\n\n\n\n\n\n\n\n\n\n\n\nAugust 20, 2019\n\n\n\n\n\n\n  \n\n\n\n\nWeek 33: Roman Emperors\n\n\n\n\n\n\n\nTidyTuesday\n\n\n2019\n\n\n\n\n\n\n\n\n\n\n\nAugust 13, 2019\n\n\n\n\n\n\n  \n\n\n\n\nWeek 29: R4DS Users\n\n\n\n\n\n\n\nTidyTuesday\n\n\n2019\n\n\n\n\n\n\n\n\n\n\n\nJuly 16, 2019\n\n\n\n\n\n\n  \n\n\n\n\nWeek 28: FIFA Womens World Cup\n\n\n\n\n\n\n\nTidyTuesday\n\n\n2019\n\n\n\n\n\n\n\n\n\n\n\nJuly 10, 2019\n\n\n\n\n\n\n  \n\n\n\n\nWeek 27: Media Franchise Powerhouses\n\n\n\n\n\n\n\nTidyTuesday\n\n\n2019\n\n\n\n\n\n\n\n\n\n\n\nJuly 2, 2019\n\n\n\n\n\n\n  \n\n\n\n\nWeek 26: Unidentified Flying Objects\n\n\n\n\n\n\n\nTidyTuesday\n\n\n2019\n\n\n\n\n\n\n\n\n\n\n\nJune 24, 2019\n\n\n\n\n\n\n  \n\n\n\n\nWeek 25: Birds, Ohhhh Canada\n\n\n\n\n\n\n\nTidyTuesday\n\n\n2019\n\n\n\n\n\n\n\n\n\n\n\nJune 18, 2019\n\n\n\n\n\n\n  \n\n\n\n\nWeek 24: Meteorite Data\n\n\n\n\n\n\n\nTidyTuesday\n\n\n2019\n\n\n\n\n\n\n\n\n\n\n\nJune 10, 2019\n\n\n\n\n\n\n  \n\n\n\n\nWeek 23 : Ramen Ratings\n\n\n\n\n\n\n\nTidyTuesday\n\n\n2019\n\n\n\n\n\n\n\n\n\n\n\nJune 5, 2019\n\n\n\n\n\n\n  \n\n\n\n\nWeek 15 : Tennis Tournaments\n\n\n\n\n\n\n\nTidyTuesday\n\n\n2019\n\n\n\n\n\n\n\n\n\n\n\nApril 9, 2019\n\n\n\n\n\n\n  \n\n\n\n\nWeek 14 : Seattle Bikes\n\n\n\n\n\n\n\nTidyTuesday\n\n\n2019\n\n\n\n\n\n\n\n\n\n\n\nApril 2, 2019\n\n\n\n\n\n\n  \n\n\n\n\nWeek 13 : Pets In Seattle\n\n\n\n\n\n\n\nTidyTuesday\n\n\n2019\n\n\n\n\n\n\n\n\n\n\n\nMarch 27, 2019\n\n\n\n\n\n\n  \n\n\n\n\nWeek 11 : Board Games\n\n\n\n\n\n\n\nTidyTuesday\n\n\n2019\n\n\n\n\n\n\n\n\n\n\n\nMarch 12, 2019\n\n\n\n\n\n\n  \n\n\n\n\nWeek 10: Women in Workforce\n\n\n\n\n\n\n\nTidyTuesday\n\n\n2019\n\n\n\n\n\n\n\n\n\n\n\nMarch 6, 2019\n\n\n\n\n\n\n  \n\n\n\n\nWeek 9 : French Train Delays\n\n\n\n\n\n\n\nTidyTuesday\n\n\n2019\n\n\n\n\n\n\n\n\n\n\n\nFebruary 28, 2019\n\n\n\n\n\n\n  \n\n\n\n\nWeek 8 : Phds Awarded in USA between 2008 and 2017\n\n\n\n\n\n\n\nTidyTuesday\n\n\n2019\n\n\n\n\n\n\n\n\n\n\n\nFebruary 19, 2019\n\n\n\n\n\n\n  \n\n\n\n\nWeek 7: Spending On Science Stuff\n\n\n\n\n\n\n\nTidyTuesday\n\n\n2019\n\n\n\n\n\n\n\n\n\n\n\nFebruary 11, 2019\n\n\n\n\n\n\n  \n\n\n\n\nWeek 6 : Mortgage, Recession and States\n\n\n\n\n\n\n\nTidyTuesday\n\n\n2019\n\n\n\n\n\n\n\n\n\n\n\nFebruary 6, 2019\n\n\n\n\n\n\n  \n\n\n\n\nWeek 5: Dairy Products in USA\n\n\n\n\n\n\n\nTidyTuesday\n\n\n2019\n\n\n\n\n\n\n\n\n\n\n\nJanuary 29, 2019\n\n\n\n\n\n\n  \n\n\n\n\nWeek 4: Prison Data\n\n\n\n\n\n\n\nTidyTuesday\n\n\n2019\n\n\n\n\n\n\n\n\n\n\n\nJanuary 22, 2019\n\n\n\n\n\n\n  \n\n\n\n\nWeek 3: Space Agencies and Launches\n\n\n\n\n\n\n\nTidyTuesday\n\n\n2019\n\n\n\n\n\n\n\n\n\n\n\nJanuary 16, 2019\n\n\n\n\n\n\n  \n\n\n\n\nWeek 2: IMDB TV Shows Data\n\n\n\n\n\n\n\nTidyTuesday\n\n\n2019\n\n\n\n\n\n\n\n\n\n\n\nJanuary 8, 2019\n\n\n\n\n\n\n  \n\n\n\n\n2019 Week 1 : #TidyTuesday Tweets\n\n\n\n\n\n\n\nTidyTuesday\n\n\n2019\n\n\n\n\n\n\n\n\n\n\n\nJanuary 1, 2019\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nWeek 38: Sea Creatures\n\n\n\n\n\n\n\nTidyTuesday\n\n\n2018\n\n\n\n\n\n\n\n\n\n\n\nDecember 18, 2018\n\n\n\n\n\n\n  \n\n\n\n\nWeek 37: NYC Restaurants\n\n\n\n\n\n\n\nTidyTuesday\n\n\n2018\n\n\n\n\n\n\n\n\n\n\n\nDecember 13, 2018\n\n\n\n\n\n\n  \n\n\n\n\nWeek 36: Medium Posts\n\n\n\n\n\n\n\nTidyTuesday\n\n\n2018\n\n\n\n\n\n\n\n\n\n\n\nDecember 8, 2018\n\n\n\n\n\n\n  \n\n\n\n\nWeek 35: Baltimore Bridges\n\n\n\n\n\n\n\nTidyTuesday\n\n\n2018\n\n\n\n\n\n\n\n\n\n\n\nNovember 28, 2018\n\n\n\n\n\n\n  \n\n\n\n\nWeek 34 : Thanksgiving\n\n\n\n\n\n\n\nTidyTuesday\n\n\n2018\n\n\n\n\n\n\n\n\n\n\n\nNovember 26, 2018\n\n\n\n\n\n\n  \n\n\n\n\nWeek 33 : Malaria Deaths\n\n\n\n\n\n\n\nTidyTuesday\n\n\n2018\n\n\n\n\n\n\n\n\n\n\n\nNovember 21, 2018\n\n\n\n\n\n\n  \n\n\n\n\nWeek 31 : R and Package Downloads\n\n\n\n\n\n\n\nTidyTuesday\n\n\n2018\n\n\n\n\n\n\n\n\n\n\n\nNovember 15, 2018\n\n\n\n\n\n\n  \n\n\n\n\nWeek 30: Movie Profit\n\n\n\n\n\n\n\nTidyTuesday\n\n\n2018\n\n\n\n\n\n\n\n\n\n\n\nNovember 14, 2018\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "tidytuesday/2019/Week_8/index.html",
    "href": "tidytuesday/2019/Week_8/index.html",
    "title": "Week 8 : Phds Awarded in USA between 2008 and 2017",
    "section": "",
    "text": "# load the packages\nlibrary(tidyverse)\nlibrary(ggthemr)\nlibrary(readr)\nlibrary(gganimate)\nlibrary(ggridges)\nlibrary(ggalluvial)\n\nggthemr(\"flat\")\n\n#load the data\nphdlist &lt;- read_csv(\"phd_by_field.csv\")\nFive variables are representing this entire data-set and three of them are factors while one column represents the year and the final column is for counts. There are few missing values. We can focus on Phds awarded from 2008 to 2017 in perspective of Broad Field, Major Field and Field.\nDataset\nGitHub Code\n{{% tweet \"1097868689385418752\" %}}\nBroad Field and Major Field are considered specially but not the column Field as it would be difficult to plot based on the amount of categories."
  },
  {
    "objectID": "tidytuesday/2019/Week_8/index.html#all-fields",
    "href": "tidytuesday/2019/Week_8/index.html#all-fields",
    "title": "Week 8 : Phds Awarded in USA between 2008 and 2017",
    "section": "All fields",
    "text": "All fields\n\np&lt;-ggplot(phdlist,aes(x=str_wrap(broad_field,20),y=n_phds))+\n          geom_boxplot()+\n          xlab(\"Broad Field\")+ylab(\"No of Phds\")+\n          transition_time(year)+ease_aes(\"linear\")+\n          ggtitle(\"Boxplot to Number of Phds in Broad Field\",\n              subtitle = \"Year : {round(frame_time)}\")+\n          theme(axis.text.x = element_text(hjust=1,angle = 90))\n\nanimate(p,nframes=9,fps=1)"
  },
  {
    "objectID": "tidytuesday/2019/Week_8/index.html#dropping-psychology-and-social-sciences",
    "href": "tidytuesday/2019/Week_8/index.html#dropping-psychology-and-social-sciences",
    "title": "Week 8 : Phds Awarded in USA between 2008 and 2017",
    "section": "Dropping Psychology and Social Sciences",
    "text": "Dropping Psychology and Social Sciences\n\np&lt;-ggplot(subset(phdlist,broad_field != \"Psychology and social sciences\"),\n          aes(x=str_wrap(broad_field,20),y=n_phds))+\n          geom_boxplot()+\n          xlab(\"Broad Field\")+ylab(\"No of Phds\")+\n          transition_time(year)+ease_aes(\"linear\")+\n          ggtitle(\"Boxplot to Number of Phds in Broad Field\",\n              subtitle = \"Year : {round(frame_time)}\")+\n          theme(axis.text.x = element_text(hjust=1,angle = 90))\n\nanimate(p,nframes=9,fps=1)"
  },
  {
    "objectID": "tidytuesday/2019/Week_8/index.html#major-fields-with-box-plot",
    "href": "tidytuesday/2019/Week_8/index.html#major-fields-with-box-plot",
    "title": "Week 8 : Phds Awarded in USA between 2008 and 2017",
    "section": "Major Fields with Box plot",
    "text": "Major Fields with Box plot\n\np&lt;-ggplot(phdlist,aes(x=str_wrap(major_field,20),y=n_phds,fill=broad_field))+\n          geom_boxplot()+coord_flip()+\n          xlab(\"Major Field\")+ylab(\"No of Phds\")+\n          transition_time(year)+ease_aes(\"linear\")+\n          ggtitle(\"Boxplot to Number of Phds in Major Field\",\n              subtitle = \"Year : {round(frame_time)}\")+\n          theme(axis.text.x = element_text(hjust=1,angle = 90),\n                legend.position = \"bottom\")+\n          labs(fill=\"Broad Field\")\n\nanimate(p,nframes=9,fps=1)"
  },
  {
    "objectID": "tidytuesday/2019/Week_8/index.html#major-fields-without-psychology-but-still-in-a-boxplot",
    "href": "tidytuesday/2019/Week_8/index.html#major-fields-without-psychology-but-still-in-a-boxplot",
    "title": "Week 8 : Phds Awarded in USA between 2008 and 2017",
    "section": "Major Fields without Psychology but still in a Boxplot",
    "text": "Major Fields without Psychology but still in a Boxplot\n\nq&lt;-ggplot(subset(phdlist,major_field != \"Psychology\"),\n          aes(x=str_wrap(major_field,20),y=n_phds,fill=broad_field))+\n          geom_boxplot()+coord_flip()+\n          xlab(\"Major Field\")+ylab(\"No of Phds\")+\n          transition_time(year)+ease_aes(\"linear\")+\n          ggtitle(\"Boxplot to Number of Phds in Major Field\",\n              subtitle = \"Year : {round(frame_time)}\")+\n          theme(axis.text.x = element_text(hjust=1,angle = 90),\n                legend.position = \"bottom\")+\n          labs(fill=\"Broad Field\")\n\nanimate(q,nframes=9,fps=1)"
  },
  {
    "objectID": "tidytuesday/2019/Week_8/index.html#mathematics-and-computer-science-as-a-broad-field",
    "href": "tidytuesday/2019/Week_8/index.html#mathematics-and-computer-science-as-a-broad-field",
    "title": "Week 8 : Phds Awarded in USA between 2008 and 2017",
    "section": "Mathematics and Computer Science as a Broad field",
    "text": "Mathematics and Computer Science as a Broad field\nMathematics and Statistics has a gradual increase until 2012, but wavers higher and lower in the next years, but in 2016 there is a sudden increase of which would lead to around 700 Phds awarded. Next year this decreases to 500 Phds.\nComparing the 2 major fields “Computer and Information Sciences” with “Mathematics and Statistics” indicate the strong gap between them awarding Phds. “Computer and Information Sciences” award more than twice the amount of Phds what “Mathematics and Statistics” award each year.\n“Computer and Information Sciences” also hold a clear pattern with the Phds awarded.\n\nsubset(phdlist,broad_field == \"Mathematics and computer sciences\") %&gt;%\n      \nggplot(.,aes(x=factor(year),y=n_phds,fill=major_field))+\n       geom_bar(stat=\"identity\",position = \"dodge\")+\n       theme(legend.position = \"bottom\")+\n       xlab(\"Major Field\")+ylab(\"Number of Phds\")+\n       ggtitle(\"Number of Phds awarded under Mathematics and CS\",\n               subtitle = \"Year : 2008 to 2017\")+\n      scale_y_continuous(breaks=seq(0,1700,100),labels=seq(0,1700,100))+\n          labs(fill=\"Major Field\")"
  },
  {
    "objectID": "tidytuesday/2019/Week_8/index.html#mathematics-and-computer-science-as-a-major-field",
    "href": "tidytuesday/2019/Week_8/index.html#mathematics-and-computer-science-as-a-major-field",
    "title": "Week 8 : Phds Awarded in USA between 2008 and 2017",
    "section": "Mathematics and Computer Science as a Major Field",
    "text": "Mathematics and Computer Science as a Major Field\nBox plot indicates the clear variation among these two major fields over years which could be used for comparison. The sudden peak in year 2012 for “Computer and Information Sciences” interests me alot. It should be noted that “Mathematics and Statistics” has more outliers than “Computer and Information Sciences”.\n\np&lt;-ggplot(subset(phdlist,broad_field == \"Mathematics and computer sciences\"),\n          aes(x=str_wrap(major_field,20),y=n_phds))+\n          geom_boxplot()+\n          xlab(\"Major Field\")+ylab(\"No of Phds\")+\n          transition_time(year)+ease_aes(\"linear\")+\n          ggtitle(\"Boxplot to Number of Phds in Major Field\",\n              subtitle = \"Year : {round(frame_time)}\")\n\nanimate(p,nframes=9,fps=1)\n\n\n\n\nBelow is a ridge plot to describe the same thing which would clearly indicate the data spread.\n\nggplot(subset(phdlist,broad_field == \"Mathematics and computer sciences\"),\n          aes(y=str_wrap(major_field,20),x=n_phds))+\n          geom_density_ridges()+\n          xlab(\"No of Phds\")+ ylab(\"Major Field\")+\n          theme(legend.position = \"bottom\")+\n          ggtitle(\"Ridge plot for Major Fields in Mathematics and Computer Sciences\",\n                  subtitle = \"Year : 2008 to 2017\")"
  },
  {
    "objectID": "tidytuesday/2019/Week_8/index.html#major-field-of-mathematics-and-computer-science-but-now-all-fields",
    "href": "tidytuesday/2019/Week_8/index.html#major-field-of-mathematics-and-computer-science-but-now-all-fields",
    "title": "Week 8 : Phds Awarded in USA between 2008 and 2017",
    "section": "Major Field of Mathematics and Computer Science but now all Fields",
    "text": "Major Field of Mathematics and Computer Science but now all Fields\nConsidering the sub categories of the chosen broad field in a box plot did not work quite well, but clearly this plot indicates the Computer Science Phds being awarded with highest amount over the years. Would that mean the boom of Artificial Intelligence in Computer Science.\n\np&lt;-ggplot(subset(phdlist,broad_field == \"Mathematics and computer sciences\"),\n          aes(x=str_wrap(field,20),y=n_phds,fill=major_field))+\n          geom_boxplot()+coord_flip()+\n          xlab(\"Field\")+ylab(\"No of Phds\")+\n          transition_time(year)+ease_aes(\"linear\")+\n          ggtitle(\"Boxplot to Number of Phds in Field\",\n              subtitle = \"Year : {round(frame_time)}\")+\n          theme(legend.position = \"bottom\")+\n          labs(fill=\"Major Field\")\n\nanimate(p,nframes=9,fps=1)\n\n\n\n\nTo get a clear view here is the ridge plot, where Computer Science is very strong for “Computer and Information Sciences”. It should be noted though there is only three other fields in this major field which are “Information Science systems”, “Computer and Information Sciences, other” and “Computer and Information sciences, general”.\nMore than 10 fields for the Major field “Mathematics and Statistics”, where higher counts occur to “Statistics(Mathematics)”, “Applied mathematics” and “Mathematics and Statistics,general”. Still non of these fields have passed the 1000 Phds awarded mark.\n\nggplot(subset(phdlist,broad_field == \"Mathematics and computer sciences\"),\n          aes(y=str_wrap(field,20),x=n_phds,fill=major_field))+\n          geom_density_ridges()+\n          xlab(\"No of Phds\")+ ylab(\"Field\")+\n          theme(legend.position = \"bottom\")+\n          ggtitle(\"Ridge plot for Fields in Mathematics and Computer Sciences\",\n                  subtitle = \"Year : 2008 to 2017\")+\n          labs(fill=\"Major Field\")"
  },
  {
    "objectID": "tidytuesday/2019/Week_6/index.html",
    "href": "tidytuesday/2019/Week_6/index.html",
    "title": "Week 6 : Mortgage, Recession and States",
    "section": "",
    "text": "# load the packages\nlibrary(readr)\nlibrary(tidyverse)\nlibrary(bbplot)\nlibrary(gganimate)\nlibrary(magrittr)\nlibrary(lubridate)\n\n# load the data\nmortgage &lt;- read_csv(\"mortgage.csv\", \n                     col_types = cols(adjustable_margin_5_1_hybrid = col_double(), \n                     adjustable_rate_5_1_hybrid = col_double(), \n                     fees_and_pts_15_yr = col_double(), fees_and_pts_30_yr = col_double(), \n                     fees_and_pts_5_1_hybrid = col_double(), \n                     fixed_rate_15_yr = col_double(), \n                     spread_30_yr_fixed_and_5_1_adjustable = col_double())\n                     )\nrecessions &lt;- read_csv(\"recessions.csv\")\nstate_hpi &lt;- read_csv(\"state_hpi.csv\")\nWeek 6 has three data-sets, which are mortgage, recession and state_hpi. Number of variables in each data-set is less than 10. You can acquire the data-set from here.\nGitHub Code\n{{% tweet \"1093047376879775744\" %}}\nAccording to the description there is not much of variation in the recession data-set, but this is not the case in other two data-sets."
  },
  {
    "objectID": "tidytuesday/2019/Week_6/index.html#fixed-rate-30-years-from-1971-to-2018",
    "href": "tidytuesday/2019/Week_6/index.html#fixed-rate-30-years-from-1971-to-2018",
    "title": "Week 6 : Mortgage, Recession and States",
    "section": "Fixed Rate 30 Years from 1971 to 2018",
    "text": "Fixed Rate 30 Years from 1971 to 2018\nEach week the Fixed Rate of 30 Years has been set and I am exploring how it changes in each year from 1971 to 2018. We can clearly see in the early Weeks of 1980 it has significantly increased higher than 17.5%, but in early 1970 it was only 7.5%.\nBy 1990 it has dropped to 7.5% and this pattern continues further until year 2018 where in December the Fixed Rate of 30 Years is slightly less than 5%.\nEach year there can be one of the below patterns I mentioned if the year is divided into two half’s.\n\nFirst and Second Half of the Year hold the same Percentage points.\nFirst Half of the Year has Higher percentage Points than the second half.\nVice versa of 2.\n\n\np&lt;-ggplot(mortgage,aes(x=factor(year(date)),y=fixed_rate_30_yr,color=week(date)))+\n          geom_jitter()+transition_time(year(date))+ease_aes(\"linear\")+\n          shadow_mark()+xlab(\"Year\")+ylab(\"Fixed Rate 30 Year Mortgage (%)\")+\n          ggtitle(\"Fixed Rate 30 Year Morgage Change by the Year: {round(frame_time)}\")+\n          labs(color=\"Week of the Year\")+\n          theme(legend.position = \"bottom\",\n                axis.text.x =element_text(angle = 90, hjust = 1))\n    \nanimate(p,nframes=48, fps=1)"
  },
  {
    "objectID": "tidytuesday/2019/Week_6/index.html#fixed-rate-15-years-from-1991-to-2018",
    "href": "tidytuesday/2019/Week_6/index.html#fixed-rate-15-years-from-1991-to-2018",
    "title": "Week 6 : Mortgage, Recession and States",
    "section": "Fixed Rate 15 Years from 1991 to 2018",
    "text": "Fixed Rate 15 Years from 1991 to 2018\nFrom 1991 only we have Fixed Rate for 15 Years and in the beginning we can see the percentage slightly above 8. and over the years it is decreasing while some fluctuations occur. This fluctuations happen in the years of 2000, 2006, 2007 and 2018, where they brake pattern of decreasing.\nIn the year 2018 it reaches slightly less than 4% in the first 20 or so weeks, but the last 20 weeks the percentage is above 4%.\n\np&lt;-ggplot(subset(mortgage,year(date)&gt;=1991),\n          aes(x=factor(year(date)),y=fixed_rate_15_yr,color=week(date)))+\n          geom_jitter()+transition_time(year(date))+ease_aes(\"linear\")+\n          shadow_mark()+xlab(\"Year\")+ylab(\"Fixed Rate 15 Year Mortgage (%)\")+\n          ggtitle(\"Fixed Rate 15 Year Morgage Change by the Year: {round(frame_time)}\")+\n          labs(color=\"Week of the Year\")+\n          theme(legend.position = \"bottom\",\n                axis.text.x =element_text(angle = 90, hjust = 1))\n    \nanimate(p,nframes=28, fps=1)"
  },
  {
    "objectID": "tidytuesday/2019/Week_6/index.html#fees-and-points-of-30-years-from-1971-to-2018",
    "href": "tidytuesday/2019/Week_6/index.html#fees-and-points-of-30-years-from-1971-to-2018",
    "title": "Week 6 : Mortgage, Recession and States",
    "section": "Fees and Points of 30 Years from 1971 to 2018",
    "text": "Fees and Points of 30 Years from 1971 to 2018\nHighest peek occurs in 1983 which is 2.7 and it decreases over the years gradually. While in the year 1971 the points were close to 1. The gradual decrease is not in effect between the years 1995 and 1996 and its clear in the plot. Yet, we can see no other anomaly in the next few years after 1996, while in 2007 it reaches its lowest point of slightly less than 0.3 (Could be related to the Great recession)\nAnyway by year 2018 after this 2007 recession the points have increased but has not reached 1 and is always oscillating between 0.4 and 0.6 in the years of 2015 to 2018.\n\np1&lt;-ggplot(mortgage,aes(x=factor(year(date)),y=factor(fees_and_pts_30_yr),color=week(date)))+\n       geom_jitter()+ theme(legend.position = \"bottom\",\n                            axis.text.x =element_text(angle = 90, hjust = 1))+\n       xlab(\"Year\")+ylab(\"Fees and Percentage points of the Loan Amount\")+\n       labs(color=\"Week of the Year\")+\n       ggtitle(\"Fess and Percentage points (30 Years) of the Loan Amount \\n \n                by the Year : {round(frame_time)}\")+\n       transition_time(year(date))+ease_aes(\"linear\")+\n       shadow_mark()\n\nanimate(p1,nframes=48, fps=1)"
  },
  {
    "objectID": "tidytuesday/2019/Week_6/index.html#fees-and-points-of-15-years-from-1991-to-2018",
    "href": "tidytuesday/2019/Week_6/index.html#fees-and-points-of-15-years-from-1991-to-2018",
    "title": "Week 6 : Mortgage, Recession and States",
    "section": "Fees and Points of 15 Years from 1991 to 2018",
    "text": "Fees and Points of 15 Years from 1991 to 2018\nIn 1991 the points are close to 1.9 and it wavers in between 1.6 and 1.8 until 1997. There is a significant drop from 1997 to 1998 where the points end up averaged around 1 and over the years it slowly decreases until year 2007. Where the lowest point of 0.3 occurs.\nAfter this new low it struggles to maintain any steady increase and rather holds below 0.8 over the next few years until 2018.\n\np&lt;-ggplot(subset(mortgage,year(date)&gt;=1991),\n          aes(x=factor(year(date)),y=factor(fees_and_pts_15_yr),color=week(date)))+\n       geom_jitter()+ theme(legend.position = \"bottom\",\n                            axis.text.x =element_text(angle = 90, hjust = 1))+\n       xlab(\"Year\")+ylab(\"Fees and Percentage points of the Loan Amount\")+\n       labs(color=\"Week of the Year\")+\n       ggtitle(\"Fees and Percentage points (15 Year)of the Loan Amount by the Year : {round(frame_time)}\")+\n       transition_time(year(date))+ease_aes(\"linear\")+\n       shadow_mark()\n\nanimate(p,nframes=28, fps=1)"
  },
  {
    "objectID": "tidytuesday/2019/Week_6/index.html#new-england-region",
    "href": "tidytuesday/2019/Week_6/index.html#new-england-region",
    "title": "Week 6 : Mortgage, Recession and States",
    "section": "New England Region",
    "text": "New England Region\nClear visibility of 2007 recession where US Avg and Price Index declining until 2010 and then improving over the next few years. All states begin very closely but end up very differently in 2018 and in troubled times.\n\np&lt;-ggplot(subset(state_hpi,state==\"CT\"|state==\"ME\"|state==\"MA\"|\n                           state==\"NH\"| state==\"RI\"|state==\"VT\"),\n       aes(x=us_avg,y=price_index,color=state))+\n       geom_point()+xlab(\"US Average\")+ylab(\"Price Index\")+\n       ggtitle(\"Price Index vs Us Avg change over Year: {round(frame_time)}\")+\n       shadow_mark()+\n       transition_time(year)+ease_aes(\"linear\")\nanimate(p,nframes = 44,fps=1)"
  },
  {
    "objectID": "tidytuesday/2019/Week_6/index.html#mideast-region",
    "href": "tidytuesday/2019/Week_6/index.html#mideast-region",
    "title": "Week 6 : Mortgage, Recession and States",
    "section": "Mideast Region",
    "text": "Mideast Region\nAfter the 2007 recession there is clear difference among DC and other states and the gap cannot be ignored at all.\n\np&lt;-ggplot(subset(state_hpi,state==\"DE\"|state==\"DC\"|state==\"MD\"|\n                           state==\"NJ\"| state==\"NY\"|state==\"PA\"),\n       aes(x=us_avg,y=price_index,color=state))+\n       geom_point()+xlab(\"US Average\")+ylab(\"Price Index\")+\n       ggtitle(\"Price Index vs Us Avg change over Year: {round(frame_time)}\")+\n       shadow_mark()+\n       transition_time(year)+ease_aes(\"linear\")\nanimate(p,nframes = 44,fps=1)"
  },
  {
    "objectID": "tidytuesday/2019/Week_6/index.html#great-lakes-region",
    "href": "tidytuesday/2019/Week_6/index.html#great-lakes-region",
    "title": "Week 6 : Mortgage, Recession and States",
    "section": "Great Lakes Region",
    "text": "Great Lakes Region\nAfter year 2000 there is clear difference among the 5 states and it becomes more complex with the 2007 recession and recovery periods. But this is not the case in year 2018 because all five states are now closely intact with the increase with both variables.\n\np&lt;-ggplot(subset(state_hpi,state==\"IL\"|state==\"OH\"|state==\"WI\"|\n                           state==\"IN\"| state==\"MI\"),\n       aes(x=us_avg,y=price_index,color=state))+\n       geom_point()+xlab(\"US Average\")+ylab(\"Price Index\")+\n       ggtitle(\"Price Index vs Us Avg change over Year: {round(frame_time)}\")+\n       shadow_mark()+\n       transition_time(year)+ease_aes(\"linear\")\nanimate(p,nframes = 44,fps=1)"
  },
  {
    "objectID": "tidytuesday/2019/Week_6/index.html#plains-region",
    "href": "tidytuesday/2019/Week_6/index.html#plains-region",
    "title": "Week 6 : Mortgage, Recession and States",
    "section": "Plains Region",
    "text": "Plains Region\nBefore the 2007 recession all states behaved very similarly, but this is not the case after year 2011 where North Dakota has a higher Price index and US Average than other states which is clearly seen in the plot.\n\np&lt;-ggplot(subset(state_hpi,state==\"IO\"|state==\"MN\"|state==\"NE\"|\n                           state==\"KS\"| state==\"MS\"|state==\"ND\"|state==\"SD\"),\n       aes(x=us_avg,y=price_index,color=state))+\n       geom_point()+xlab(\"US Average\")+ylab(\"Price Index\")+\n       ggtitle(\"Price Index vs Us Avg change over Year: {round(frame_time)}\")+\n       shadow_mark()+\n       transition_time(year)+ease_aes(\"linear\")\nanimate(p,nframes = 44,fps=1)"
  },
  {
    "objectID": "tidytuesday/2019/Week_6/index.html#southeast-region",
    "href": "tidytuesday/2019/Week_6/index.html#southeast-region",
    "title": "Week 6 : Mortgage, Recession and States",
    "section": "Southeast Region",
    "text": "Southeast Region\nSoutheast region has alot of states therefore it would be time consuming to compare. Clearly the 2007 recession has a toll on both variables, but not as the effect from year 2000.\n\np&lt;-ggplot(subset(state_hpi,state==\"AL\"|state==\"FL\"|state==\"KY\"|\n                           state==\"AR\"|state==\"GA\"|state==\"MS\"|\n                           state==\"LA\"|state==\"NC\"|state==\"SC\"|\n                           state==\"TN\"|state==\"VA\"|state==\"WV\"),\n       aes(x=us_avg,y=price_index,color=state))+\n       geom_point()+xlab(\"US Average\")+ylab(\"Price Index\")+\n       ggtitle(\"Price Index vs Us Avg change over Year: {round(frame_time)}\")+\n       shadow_mark()+\n       transition_time(year)+ease_aes(\"linear\")\nanimate(p,nframes = 44,fps=1)"
  },
  {
    "objectID": "tidytuesday/2019/Week_6/index.html#southwest-region",
    "href": "tidytuesday/2019/Week_6/index.html#southwest-region",
    "title": "Week 6 : Mortgage, Recession and States",
    "section": "Southwest Region",
    "text": "Southwest Region\nBefore the 2007 recession and after also we can see the clear changes. Before that in year 2000 also we can see rapid changes which lead up-to the recession. The damage done by the recession have not been recovered in some states even by 2018 according to the gap in Price index.\n\np&lt;-ggplot(subset(state_hpi,state==\"AZ\"|state==\"OK\"|\n                           state==\"TX\"|state==\"NM\"),\n       aes(x=us_avg,y=price_index,color=state))+\n       geom_point()+xlab(\"US Average\")+ylab(\"Price Index\")+\n       ggtitle(\"Price Index vs Us Avg change over Year: {round(frame_time)}\")+\n       shadow_mark()+\n       transition_time(year)+ease_aes(\"linear\")\nanimate(p,nframes = 44,fps=1)"
  },
  {
    "objectID": "tidytuesday/2019/Week_6/index.html#rocky-mountain-region",
    "href": "tidytuesday/2019/Week_6/index.html#rocky-mountain-region",
    "title": "Week 6 : Mortgage, Recession and States",
    "section": "Rocky Mountain Region",
    "text": "Rocky Mountain Region\nChanges after 2000 are very different for the 5 states in this region and after the 2007 recession also we can see the rapid set back in Us avg and price index. But this is not the case after 2013 even though it has already made significant amount of divide between the state of MO and other states, which is clearly seen at the end of year 2018.\n\np&lt;-ggplot(subset(state_hpi,state==\"CO\"|state==\"MO\"|state==\"WY\"|\n                           state==\"ID\"| state==\"UT\"),\n       aes(x=us_avg,y=price_index,color=state))+\n       geom_point()+xlab(\"US Average\")+ylab(\"Price Index\")+\n       ggtitle(\"Price Index vs Us Avg change over Year: {round(frame_time)}\")+\n       shadow_mark()+\n       transition_time(year)+ease_aes(\"linear\")\nanimate(p,nframes = 44,fps=1)"
  },
  {
    "objectID": "tidytuesday/2019/Week_6/index.html#far-west-region",
    "href": "tidytuesday/2019/Week_6/index.html#far-west-region",
    "title": "Week 6 : Mortgage, Recession and States",
    "section": "Far West Region",
    "text": "Far West Region\nEarly 1990 has a sudden raise and it quickly settles down close to year 1998. Where by 2000 all six states share the same price index value, but this changes over time with clear difference among two groups. Each group containing 3 states, but this progress entirely changes by the 2007 recession and its recovery. Because clearly after 2013 there is no more 2 groups, it is now 3 groups. Where state of Hawaii has the highest pricing index and lowest goes to Alaska, this is by the end of year 2018.\n\np&lt;-ggplot(subset(state_hpi,state==\"AL\"|state==\"NV\"|state==\"OR\"|\n                           state==\"CA\"| state==\"HI\"|state==\"WA\"),\n       aes(x=us_avg,y=price_index,color=state))+\n       geom_point()+xlab(\"US Average\")+ylab(\"Price Index\")+\n       ggtitle(\"Price Index vs Us Avg change over Year: {round(frame_time)}\")+\n       shadow_mark()+\n       transition_time(year)+ease_aes(\"linear\")\nanimate(p,nframes = 44,fps=1)\n\n\n\n\nIt might look that I have not done enough justice for the changes which occurred before the year 2000, and I do agree with you. But if I do add them into my consideration this article would be very long. Hopefully, the animated plots clearly indicate the strong changes which occurred in the pre-y2k era.\nTHANK YOU"
  },
  {
    "objectID": "tidytuesday/2019/Week_43/index.html",
    "href": "tidytuesday/2019/Week_43/index.html",
    "title": "Week 43: Horror Movies",
    "section": "",
    "text": "Data is for Week 43 under the title Horror Movies. Have not done any plotting for TidyTuesday for some time or to be exact more than two months. So here I am trying out d3 visualisation in R.\nHelpful packages for this post are ‘d3rain’ and ‘d3treeR’.\n# load the packages\nlibrary(d3rain)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(splitstackshape)\nlibrary(d3treeR)\nlibrary(treemap)\n\n# load the data\nhorror_movies &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-10-22/horror_movies.csv\")"
  },
  {
    "objectID": "tidytuesday/2019/Week_43/index.html#organize-data",
    "href": "tidytuesday/2019/Week_43/index.html#organize-data",
    "title": "Week 43: Horror Movies",
    "section": "Organize Data",
    "text": "Organize Data\nGenuine look indicates that data is fine. Well if you look closer there are some changes that need to be done to make the most of it. I have simpler dreams such as drawing two plots which would make sense.\n\nRain plot where Horror movie distribution occurs by each month for days.(Easier to look at the plot than try to grasp from what I am telling)\nTree map which would indicate how the Genres of the movies are assigned. It is my intention to understand if there is a hierarchy in this genre naming as some movies have multiple assignments.\n\nTo achieve above set plots some changes in the two columns ‘genres’ and ‘release_date’ are necessary; precisely we have to break one column into multiples based on a pattern for former and based on a condition for latter.\n\nRainDataFormat &lt;- cSplit(horror_movies,'release_date','-',drop=TRUE)\n\nRainDataFormat &lt;- RainDataFormat %&gt;% \n                    rename(Day = release_date_1) %&gt;%\n                    rename(Month = release_date_2) %&gt;%\n                    rename(Year = release_date_3) %&gt;%\n                    select(Year,Month,Day,title)\n\nRainDataFormat$Year[is.na(RainDataFormat$Year)==TRUE] &lt;-\n    stringr::str_remove(RainDataFormat$Day[is.na(RainDataFormat$Year)==TRUE],\"20\")\n\nclass(RainDataFormat$Year) &lt;- \"numeric\"\nclass(RainDataFormat$Day) &lt;- \"numeric\"\n\nRainDataFormat$Day[RainDataFormat$Day&gt;31] &lt;- NA\n\n\nTreeDataFormat &lt;- cSplit(horror_movies,'genres','|',drop=TRUE)\n\nTreeDataFormat &lt;- TreeDataFormat %&gt;%\n                    rename(First_Choice = genres_1) %&gt;%\n                    rename(Second_Choice = genres_2) %&gt;%\n                    rename(Third_Choice = genres_3)\n\nTreeDataFormat &lt;- TreeDataFormat %&gt;%\n                  group_by(First_Choice,Second_Choice,Third_Choice) %&gt;%\n                  mutate(\"Counts\" = n()) %&gt;%\n                  select(First_Choice,Second_Choice,Third_Choice,Counts) %&gt;%\n                  unique()"
  },
  {
    "objectID": "tidytuesday/2019/Week_43/index.html#rain-graph",
    "href": "tidytuesday/2019/Week_43/index.html#rain-graph",
    "title": "Week 43: Horror Movies",
    "section": "Rain Graph",
    "text": "Rain Graph\nCompared how horror movies have increased compared to year 2017 from year 2012.\n\nRainDataFormat &lt;- RainDataFormat[complete.cases(RainDataFormat)]\nlevels(RainDataFormat$Month) &lt;- c(\"Jan\",\"Feb\",\"Mar\",\"Apr\",\n                                  \"May\",\"Jun\",\"Jul\",\"Aug\",\n                                  \"Sep\",\"Oct\",\"Nov\",\"Dec\")\n\nRainDataFormat[Year==12] %&gt;%\n    d3rain(Day,Month,toolTip = Day,title = \"Days vs Months for Year 2012\") %&gt;%\n    drip_settings(dripSequence = 'iterate',\n                  ease = 'linear',\n                  jitterWidth = 25,\n                  dripSpeed = 500,\n                  dripFill = 'steelblue') %&gt;% \n    chart_settings(fontFamily = 'times',\n                   yAxisTickLocation = 'left')\n\nRainDataFormat[Year==17] %&gt;%\n    d3rain(Day,Month,toolTip = Day,title = \"Days vs Months for Year 2017\") %&gt;%\n    drip_settings(dripSequence = 'iterate',\n                  ease = 'linear',\n                  jitterWidth = 25,\n                  dripSpeed = 500,\n                  dripFill = 'red') %&gt;% \n    chart_settings(fontFamily = 'times',\n                   yAxisTickLocation = 'left')"
  },
  {
    "objectID": "tidytuesday/2019/Week_43/index.html#tree-map",
    "href": "tidytuesday/2019/Week_43/index.html#tree-map",
    "title": "Week 43: Horror Movies",
    "section": "Tree map",
    "text": "Tree map\nSome movies have more than three genres assigned to them. So here is the tree map to plot them in choice order for genre.\n\nTreeData &lt;- treemap(\n                    TreeDataFormat,\n                    index=c(\"First_Choice\", \"Second_Choice\",'Third_Choice'),\n                    vSize=\"Counts\",\n                    vColor=\"Counts\",\n                    type=\"value\"\n                    )\n\nd3tree3(TreeData,rootname = \"Choices\")\n\nTHANK YOU"
  },
  {
    "objectID": "tidytuesday/2019/Week_34/index.html",
    "href": "tidytuesday/2019/Week_34/index.html",
    "title": "Week 34: Nuclear Explosions",
    "section": "",
    "text": "GitHub Code\n{{% tweet \"1163767794636328960\" %}}\nnuclear_explosions &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-08-20/nuclear_explosions.csv\")\n\n# load packages\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(gganimate)\nlibrary(tvthemes)"
  },
  {
    "objectID": "tidytuesday/2019/Week_34/index.html#all-points",
    "href": "tidytuesday/2019/Week_34/index.html#all-points",
    "title": "Week 34: Nuclear Explosions",
    "section": "All points",
    "text": "All points\n\nggplot(nuclear_explosions,aes(magnitude_surface,magnitude_body))+\n      geom_point(color=blues9[7])+\n      xlab(\"Magnitude Surface\")+ylab(\"Magnitude Body\")+\n      ggtitle(\"Magnitude Surface Changing with Magnitude Body\")+labs(caption=\"TidyTuesday 34\")+\n      theme_simpsons()"
  },
  {
    "objectID": "tidytuesday/2019/Week_34/index.html#points-where-magnitude-surface-and-body-are-more-than-0-measurement",
    "href": "tidytuesday/2019/Week_34/index.html#points-where-magnitude-surface-and-body-are-more-than-0-measurement",
    "title": "Week 34: Nuclear Explosions",
    "section": "Points where Magnitude surface and body are more than 0 measurement",
    "text": "Points where Magnitude surface and body are more than 0 measurement\n\nnuclear_explosions %&gt;%\n  filter(magnitude_surface &gt;0) %&gt;%\n  filter(magnitude_body &gt;0) %&gt;%\nggplot(.,aes(magnitude_surface,magnitude_body))+\n      geom_point(color=blues9[7])+\n      xlab(\"Magnitude Surface\")+ylab(\"Magnitude Body\")+\n      ggtitle(\"Magnitude Surface Changing with Magnitude Body\")+labs(caption=\"TidyTuesday 34\")+\n      theme_simpsons()"
  },
  {
    "objectID": "tidytuesday/2019/Week_34/index.html#magnitude-surface-and-body-but-based-on-country",
    "href": "tidytuesday/2019/Week_34/index.html#magnitude-surface-and-body-but-based-on-country",
    "title": "Week 34: Nuclear Explosions",
    "section": "Magnitude Surface and Body but Based on Country",
    "text": "Magnitude Surface and Body but Based on Country\n\nnuclear_explosions %&gt;%\n  filter(magnitude_surface &gt;0) %&gt;%\n  filter(magnitude_body &gt;0) %&gt;%\nggplot(.,aes(magnitude_surface,magnitude_body,color=country))+\n      geom_point()+\n      xlab(\"Magnitude Surface\")+ylab(\"Magnitude Body\")+\n      ggtitle(\"Magnitude Surface Changing with Magnitude Body\",\n              subtitle = \"By Country\")+labs(caption=\"TidyTuesday 34\")+\n      theme_simpsons()+scale_color_simpsons()"
  },
  {
    "objectID": "tidytuesday/2019/Week_34/index.html#magnitude-surface-and-body-but-based-on-country-with-animation",
    "href": "tidytuesday/2019/Week_34/index.html#magnitude-surface-and-body-but-based-on-country-with-animation",
    "title": "Week 34: Nuclear Explosions",
    "section": "Magnitude Surface and Body but Based on Country with animation",
    "text": "Magnitude Surface and Body but Based on Country with animation\n\nnuclear_explosions %&gt;%\n  filter(magnitude_surface &gt;0) %&gt;%\n  filter(magnitude_body &gt;0) %&gt;%\nggplot(.,aes(magnitude_surface,magnitude_body,color=country))+\n      geom_point()+\n      xlab(\"Magnitude Surface\")+ylab(\"Magnitude Body\")+\n      ggtitle(\"Magnitude Surface Changing with Magnitude Body\",\n              subtitle = \"By Country\")+labs(caption=\"TidyTuesday 34\")+\n      transition_states(country)+theme_simpsons()+scale_color_simpsons()"
  },
  {
    "objectID": "tidytuesday/2019/Week_34/index.html#all-points-1",
    "href": "tidytuesday/2019/Week_34/index.html#all-points-1",
    "title": "Week 34: Nuclear Explosions",
    "section": "All points",
    "text": "All points\n\nggplot(nuclear_explosions,aes(yield_lower,yield_upper))+\n      geom_point(color=blues9[7])+\n      xlab(\"Yield Lower\")+ylab(\"Yield Upper\")+\n      ggtitle(\"Yield Lower Changing with Yield Upper\")+labs(caption=\"TidyTuesday 34\")+\n      theme_simpsons()"
  },
  {
    "objectID": "tidytuesday/2019/Week_34/index.html#points-where-yield-upper-and-lower-are-more-than-0-measurement",
    "href": "tidytuesday/2019/Week_34/index.html#points-where-yield-upper-and-lower-are-more-than-0-measurement",
    "title": "Week 34: Nuclear Explosions",
    "section": "Points where Yield Upper and Lower are more than 0 measurement",
    "text": "Points where Yield Upper and Lower are more than 0 measurement\n\nnuclear_explosions %&gt;%\n  filter(yield_lower &gt;0) %&gt;%\n  filter(yield_upper &gt;0) %&gt;%\nggplot(.,aes(yield_lower,yield_upper))+\n      geom_point(color=blues9[7])+\n      xlab(\"Yield Lower\")+ylab(\"Yield Upper\")+\n      ggtitle(\"Yield Lower Changing with Yield Upper\")+labs(caption=\"TidyTuesday 34\")+\n      theme_simpsons()"
  },
  {
    "objectID": "tidytuesday/2019/Week_34/index.html#yield-upper-and-lower-but-based-on-country",
    "href": "tidytuesday/2019/Week_34/index.html#yield-upper-and-lower-but-based-on-country",
    "title": "Week 34: Nuclear Explosions",
    "section": "Yield Upper and Lower but Based on Country",
    "text": "Yield Upper and Lower but Based on Country\n\nnuclear_explosions %&gt;%\n  filter(yield_lower &gt;0) %&gt;%\n  filter(yield_upper &gt;0) %&gt;%\nggplot(.,aes(yield_lower,yield_upper,color=country))+\n      geom_point()+\n      xlab(\"Yield Lower\")+ylab(\"Yield Upper\")+\n      ggtitle(\"Yield Lower Changing with Yield Upper\",\n              subtitle = \"By Country\")+labs(caption=\"TidyTuesday 34\")+\n      theme_simpsons()+scale_color_simpsons()"
  },
  {
    "objectID": "tidytuesday/2019/Week_34/index.html#yield-upper-and-lower-but-based-on-country-with-animation",
    "href": "tidytuesday/2019/Week_34/index.html#yield-upper-and-lower-but-based-on-country-with-animation",
    "title": "Week 34: Nuclear Explosions",
    "section": "Yield Upper and Lower but Based on Country with animation",
    "text": "Yield Upper and Lower but Based on Country with animation\n\nnuclear_explosions %&gt;%\n  filter(yield_lower &gt;0) %&gt;%\n  filter(yield_upper &gt;0) %&gt;%\nggplot(.,aes(yield_lower,yield_upper,color=country))+\n      geom_point()+\n      xlab(\"Yield Lower\")+ylab(\"Yield Upper\")+\n      ggtitle(\"Yield Lower Changing with Yield Upper\",\n              subtitle = \"By Country\")+labs(caption=\"TidyTuesday 34\")+\n       transition_states(country)+theme_simpsons()+scale_color_simpsons()"
  },
  {
    "objectID": "tidytuesday/2019/Week_34/index.html#animated",
    "href": "tidytuesday/2019/Week_34/index.html#animated",
    "title": "Week 34: Nuclear Explosions",
    "section": "Animated",
    "text": "Animated\n\nnuclear_explosions %&gt;%\n   group_by(year,country) %&gt;%\n   summarise(counting=n()) %&gt;%\nggplot(.,aes(country,counting,label=counting))+geom_col(fill=blues9[1])+\n   xlab(\"Country\")+ylab(\"Counts\")+geom_text(vjust=-1)+\n   ggtitle(\"Nuclear Explosions for Countries\",\n          subtitle = \"Year :{closest_state}\")+labs(caption=\"TidyTuesday 34\")+\n   transition_states(year)+\n   scale_fill_simpsons()+theme_simpsons()"
  },
  {
    "objectID": "tidytuesday/2019/Week_34/index.html#stack-graph",
    "href": "tidytuesday/2019/Week_34/index.html#stack-graph",
    "title": "Week 34: Nuclear Explosions",
    "section": "Stack graph",
    "text": "Stack graph\n\nnuclear_explosions %&gt;%\n   group_by(year,country) %&gt;%\n   summarise(counting=n()) %&gt;%\nggplot(.,aes(year,counting,label=counting,fill=country))+\n   geom_col(position = \"stack\")+\n   xlab(\"Year\")+ylab(\"Counts\")+geom_text(position = \"stack\",vjust=1,size=2)+\n   ggtitle(\"Nuclear Explosions by Year\",subtitle = \"for Countries\")+\n   labs(caption=\"TidyTuesday 34\")+\n   theme_simpsons()+scale_fill_simpsons()"
  },
  {
    "objectID": "tidytuesday/2019/Week_34/index.html#animated-1",
    "href": "tidytuesday/2019/Week_34/index.html#animated-1",
    "title": "Week 34: Nuclear Explosions",
    "section": "Animated",
    "text": "Animated\n\nggplot(nuclear_explosions,aes(type,purpose,color=country))+\n  geom_jitter(alpha=0.7)+xlab(\"Type\")+ylab(\"Purpose\")+\n  ggtitle(\"Purpose vs Type for Nuclear Missiles\",subtitle = \"By Country\")+\n  labs(caption=\"TidyTuesday 34\")+ transition_states(country)+\n  theme_simpsons()+scale_color_simpsons()+\n  theme(axis.text.x = element_text(angle=60,hjust=1))"
  },
  {
    "objectID": "tidytuesday/2019/Week_34/index.html#animated-2",
    "href": "tidytuesday/2019/Week_34/index.html#animated-2",
    "title": "Week 34: Nuclear Explosions",
    "section": "Animated",
    "text": "Animated\n\nnuclear_explosions %&gt;%\n  group_by(year,type,country) %&gt;%\n  summarise(counting=n()) %&gt;%\n  select(year,country,type,counting) %&gt;%\n  subset(type==\"SHAFT\"|type==\"TUNNEL\"|type==\"ATMOSPH\"|type==\"SHAFT/GR\"|type==\"AIRDROP\") %&gt;%\nggplot(.,aes(year,counting,fill=type,label=counting))+\n  geom_col()+geom_text(position = \"stack\",vjust=1,size=2)+\n  transition_states(country)+theme_simpsons()+\n  enter_fade() + exit_shrink() + ease_aes('sine-in-out')+\n  scale_fill_simpsons()+xlab(\"Year\")+ylab(\"Counts\")+\n  ggtitle(\"Yearly change by Countries for Types\",\n          subtitle = \"Country:{closest_state}\")+\n  labs(caption=\"TidyTuesday 34\")\n\n\n\n\nTHANK YOU"
  },
  {
    "objectID": "tidytuesday/2019/Week_3/index.html",
    "href": "tidytuesday/2019/Week_3/index.html",
    "title": "Week 3: Space Agencies and Launches",
    "section": "",
    "text": "# load the packages\nlibrary(readr)\nlibrary(tidyverse)\nlibrary(ggalt)\nlibrary(magrittr)\nlibrary(dplyr)\nlibrary(ggthemr)\nlibrary(gganimate)\n\n# Load the Agency data\nagencies&lt;-read_csv(\"agencies.csv\")\n\n# Load the Launches data\nlaunches&lt;-read_csv(\"launches.csv\")\n\nattach(agencies)\nattach(launches)\n\n# load a theme\nggthemr(\"flat dark\")"
  },
  {
    "objectID": "tidytuesday/2019/Week_3/index.html#agency-vs-count",
    "href": "tidytuesday/2019/Week_3/index.html#agency-vs-count",
    "title": "Week 3: Space Agencies and Launches",
    "section": "Agency vs Count",
    "text": "Agency vs Count\nMost amount of launches are from Rakentiye Voiska Strategicheskogo Naznacheniye (RVSN) and it is 1528 and next is Upravleniye Nachalnika Kosmicheskikh Sredstv (UNKS) with 904. Out of the Top 10 places (considering the most launches) it is clear that class D agencies has the most amount (of 5),while 3 from class C agencies and the rest with class B. NASA is in third place with 469 launches and it is a class C agency. My favorite agency Space X (SPX) has launched 65 times and it is a class B agency.\n\nggplot(agencies,aes(x=fct_inorder(agency),y=count,\n                    color=class,fill=class))+\n       geom_bar(stat=\"identity\",width=0.75)+coord_flip()+\n       geom_text(label=agencies$count, hjust=-0.15)+\n       xlab(\"Space Agency\")+ylab(\"Frequency\")+\n       ggtitle(\"Space Agency vs Frequency By Class\")\n\n\n\n\nSimilarly, for the same bar plot if we change color according to agency type we have different insight. Top 10 agencies is 90% filled with state ownership and 10% is with private ownership. It should be noted that overall there are only two start-ups and close to 10 have private ownership, rest is state owned. The highest amount of launches for a private ownership is from Arian Space(AE) and for start-up its Space X (SPX). Respectively, their counts are 258 and 65.\n\nggplot(agencies,aes(x=fct_inorder(agency),y=count,\n                    color=agency_type,fill=agency_type))+\n       geom_bar(stat=\"identity\",width=0.75)+coord_flip()+\n       geom_text(aes(label=count), hjust=-0.15)+\n       xlab(\"Space Agency\")+ylab(\"Frequency\")+\n      labs(color=\"Agency Type\",fill=\"Agency Type\")+\n      ggtitle(\"Space Agency vs Frequency By Agency Type\")"
  },
  {
    "objectID": "tidytuesday/2019/Week_3/index.html#type-vs-count",
    "href": "tidytuesday/2019/Week_3/index.html#type-vs-count",
    "title": "Week 3: Space Agencies and Launches",
    "section": "Type vs Count",
    "text": "Type vs Count\nType of agencies is very complex because an agency can play multiple roles. Highest amount of count is for O/LA type with 3227 and second place is for LA type with 821 counts. There are 145 agencies with the highest combination of types and this category is O/LA/LV/PL/E/S with 145 counts.\n\nagencies[,c('type','count')] %&gt;% group_by(type) %&gt;%\n  summarise_each(funs(sum)) %&gt;% arrange(count) %&gt;%\nggplot(.,aes(fct_inorder(type),count))+\n  geom_bar(stat = \"identity\")+\n  geom_text(aes(label=count),hjust=-0.15)+coord_flip()+\n  xlab(\"Type\")+ylab(\"Frequency\")+\n  ggtitle(\"Type vs Frequency\")"
  },
  {
    "objectID": "tidytuesday/2019/Week_3/index.html#class-vs-count",
    "href": "tidytuesday/2019/Week_3/index.html#class-vs-count",
    "title": "Week 3: Space Agencies and Launches",
    "section": "Class vs Count",
    "text": "Class vs Count\nClass C and B has similar amounts of count which is close to 1100 and most launches are from D class agencies with the count of 3584.\n\nagencies[,c('class','count')] %&gt;% group_by(class) %&gt;%\n  summarise_each(funs(sum)) %&gt;% arrange(count) %&gt;%\nggplot(.,aes(fct_inorder(class),count))+\n  geom_bar(stat = \"identity\")+\n  geom_text(aes(label=count),vjust=-0.15)+\n  xlab(\"Class\")+ylab(\"Frequency\")+\n  ggtitle(\"Class vs Frequency\")"
  },
  {
    "objectID": "tidytuesday/2019/Week_3/index.html#agency-type-vs-count",
    "href": "tidytuesday/2019/Week_3/index.html#agency-type-vs-count",
    "title": "Week 3: Space Agencies and Launches",
    "section": "Agency Type vs Count",
    "text": "Agency Type vs Count\nIn perspective of agency type there are 4765 state owned launches, but only 67 launches from start-ups.\n\nagencies[,c('agency_type','count')] %&gt;% group_by(agency_type) %&gt;% \n  summarise_each(funs(sum)) %&gt;% arrange(count) %&gt;%\nggplot(.,aes(fct_inorder(agency_type),count))+\n  geom_bar(stat = \"identity\")+\n  geom_text(aes(label=count),vjust=-0.15)+\n  xlab(\"Agency Type\")+ylab(\"Frequency\")+\n  ggtitle(\"Agency Type vs Frequency\")"
  },
  {
    "objectID": "tidytuesday/2019/Week_3/index.html#state-code-vs-count",
    "href": "tidytuesday/2019/Week_3/index.html#state-code-vs-count",
    "title": "Week 3: Space Agencies and Launches",
    "section": "State Code vs Count",
    "text": "State Code vs Count\nClose to 2500 missions were launched by Soviet Union and 1709 were done by Unite States.\n\nagencies[,c('state_code','count')] %&gt;% group_by(state_code) %&gt;%\n  summarise_each(funs(sum)) %&gt;% arrange(count) %&gt;%\nggplot(.,aes(fct_inorder(state_code),count))+\n  geom_bar(stat = \"identity\")+ coord_flip()+\n  geom_text(aes(label=count),hjust=-0.15)+\n  xlab(\"State Code\")+ylab(\"Frequency\")+\n  ggtitle(\"State Code vs Count\")"
  },
  {
    "objectID": "tidytuesday/2019/Week_3/index.html#location-vs-count",
    "href": "tidytuesday/2019/Week_3/index.html#location-vs-count",
    "title": "Week 3: Space Agencies and Launches",
    "section": "Location vs Count",
    "text": "Location vs Count\nMore than 1500 launches are from Mosvka? and exactly 1204 launches from Moskva. Further, 469 launches from Washington D.C.\n\nagencies[,c('location','count')] %&gt;% group_by(location) %&gt;%\n  summarise_each(funs(sum)) %&gt;% arrange(count) %&gt;%\nggplot(.,aes(fct_inorder(location),count))+\n  geom_bar(stat = \"identity\")+ coord_flip()+\n  geom_text(aes(label=count),hjust=-0.15)+\n  xlab(\"Location\")+ylab(\"Frequency\")+\n  ggtitle(\"Location vs Count\")"
  },
  {
    "objectID": "tidytuesday/2019/Week_3/index.html#start-year-and-end-year-vs-agency",
    "href": "tidytuesday/2019/Week_3/index.html#start-year-and-end-year-vs-agency",
    "title": "Week 3: Space Agencies and Launches",
    "section": "Start Year and End Year vs agency",
    "text": "Start Year and End Year vs agency\nBelow is a Dumbbell plot to see at the agencies which are no longer active. Before 1960 there was very small activity and they are all owned by the state. With the American and Russian Space race we have private sector also being part of this adventure, but most of them are ending their service around the first half of 1990. There is more activity after this regularly but they are short lived for these agencies. Royal Aircraft Establishment (RAE) has long life for space adventure which was begun around late 1915, and ends its service in around 1990.\n\nsubset(agencies, substr(tstart,1,4) != \"-\" & \n                 substr(tstop,1,4) != \"-\" & \n                 substr(tstop,1,4) != \"*\" ) %&gt;%\nggplot(aes(y=reorder(agency,as.numeric(substr(tstart,1,4))),\n           x=as.numeric(substr(tstart,1,4)),xend=as.numeric(substr(tstop,1,4)),\n           fill=agency_type,color=agency_type))+\n  geom_dumbbell(size_x = 2,size_xend = 2.75,size=1.25)+ \n  xlab(\"Year\")+ylab(\"Agency\")+ \n  scale_x_continuous(breaks=seq(1910,2020,5),labels=seq(1910,2020,5))+\n  labs(fill=\"Agency Type\",color=\"Agency Type\")+\n  theme(axis.text.x = element_text(angle = 90))+\n  ggtitle(\"Start Year and End Year vs Agency If We Know When\")\n\n\n\n\nIt should be effectively noted that “-” means still active and “*” means unknown in my perspective. Here we cannot consider the years as numeric because of the characters used. Agencies like NASA and Space X are still active according to my knowledge therefore I considered the above assumption for characters. Most of these agencies are state owned and after Space X there is Rocket Lab USA (RLABU). Most of these agencies were launched after 1980.\n\nsubset(agencies, substr(tstart,1,4) == \"-\" | \n                 substr(tstop,1,4) == \"-\" | \n                 substr(tstop,1,4) == \"*\" ) %&gt;%\nggplot(aes(y=reorder(agency,as.numeric(substr(tstart,1,4))),\n           x=substr(tstart,1,4),\n           xend=substr(tstop,1,4),\n           fill=agency_type,color=agency_type))+\n  geom_dumbbell(size_x = 2,size_xend = 3,size=1.25)+\n  xlab(\"Year\")+ylab(\"Agency\")+\n  labs(fill=\"Agency Type\",color=\"Agency Type\")+\n  theme(axis.text.x = element_text(angle = 90))+\n  ggtitle(\"Start Year and End Year vs Agency If Do Not Know When\")"
  },
  {
    "objectID": "tidytuesday/2019/Week_3/index.html#success-or-failure-of-these-missions-vs-category-variables",
    "href": "tidytuesday/2019/Week_3/index.html#success-or-failure-of-these-missions-vs-category-variables",
    "title": "Week 3: Space Agencies and Launches",
    "section": "Success or Failure of these missions vs Category Variables",
    "text": "Success or Failure of these missions vs Category Variables\nThere are few categorical variables which could be associated with the success or failure of these missions.\nSuccess or Failure vs Launch Year\nLess mistakes over the year with technologies improving and in between 1960 to 1990 we can see alot of launches always above 100 per year. This enthusiasm no longer exists until 2005. After 2005 there is positive increase in launches and failures also less.\n\nggplot(launches,aes(x=factor(launch_year),fill=category))+\n  geom_bar()+\n  theme(axis.text.x = element_text(angle = 90))+\n  xlab(\"Years\")+ylab(\"Frequency\")+\n  ggtitle(\"Years vs Frequency\")+\n  scale_y_continuous(labels=seq(0,150,10),breaks=seq(0,150,10))\n\n\n\n\nSuccess or Failure vs Agency Type\nState owned agencies has more failures than private and start-ups because it would be costly. More than 4750 launches are from state owned agencies but in them more than 500 launches are failures. Even though private owned agencies has a history from 1990 they have less than 1000 launches.\n\nggplot(launches,aes(x=fct_infreq(factor(agency_type)),fill=category))+\n  geom_bar()+\n  xlab(\"Agency Type\")+ylab(\"Frequency\")+\n  ggtitle(\"Agency Type vs Frequency\")+\n  scale_y_continuous(labels=seq(0,5000,250),breaks=seq(0,5000,250))\n\n\n\n\nSuccess or Failure vs State Code\nSoviet Union (SU) and United States (US) has the most dominant appearance in this field. More than 2400 launches from SU and for US it is more than 1700 launches. Failures also considerably higher for SU and US.\n\nggplot(launches,aes(x=fct_infreq(factor(state_code)),fill=category))+\n  geom_bar()+\n  theme(axis.text.x = element_text(angle = 90))+\n  xlab(\"State Code\")+ylab(\"Frequency\")+\n  ggtitle(\"State Code vs Frequency\")+\n  scale_y_continuous(labels=seq(0,2500,100),breaks=seq(0,2500,100))"
  },
  {
    "objectID": "tidytuesday/2019/Week_3/index.html#state-code-vs-category-over-time-for-success-and-failure",
    "href": "tidytuesday/2019/Week_3/index.html#state-code-vs-category-over-time-for-success-and-failure",
    "title": "Week 3: Space Agencies and Launches",
    "section": "State Code vs Category Over time for Success and Failure",
    "text": "State Code vs Category Over time for Success and Failure\nAnimated jitter plot here explains how over the years these launches occur based on States and Success(O) or Failure(F).\n\np&lt;-ggplot(launches,aes(y=category,x=state_code,color=agency_type))+\n       geom_jitter()+\n       labs(title = \"States vs Success or Failure by : {round(frame_time,0)}\",\n            x=\"State Code\",y= \"Success or Failure\")+\n       transition_time(launch_year)+ease_aes('linear')+\n       labs(color=\"Agency Type\")\n\nanimate(p,fps=2,duration = 60)\n\n\n\n\nTHANK YOU"
  },
  {
    "objectID": "tidytuesday/2019/Week_28/index.html",
    "href": "tidytuesday/2019/Week_28/index.html",
    "title": "Week 28: FIFA Womens World Cup",
    "section": "",
    "text": "wwc_outcomes &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-07-09/wwc_outcomes.csv\")\nsquads &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-07-09/squads.csv\")\ncodes &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-07-09/codes.csv\")\n\nlibrary(ggplot2)\nlibrary(gganimate)\nlibrary(dplyr)\nlibrary(tvthemes)\nGitHub Code\n{{% tweet \"1148866198370750464\" %}}"
  },
  {
    "objectID": "tidytuesday/2019/Week_28/index.html#captain-ship-vs-goals",
    "href": "tidytuesday/2019/Week_28/index.html#captain-ship-vs-goals",
    "title": "Week 28: FIFA Womens World Cup",
    "section": "Captain-ship vs Goals",
    "text": "Captain-ship vs Goals\n\np1&lt;-ggplot(squads,aes(caps,goals,color=pos,label1=player,label2=country,group=1))+geom_point()+\n    ylab(\"No of Goals Scored\")+xlab(\"No of Matches as Captains\")+\n    ggtitle(\"Relationship Between Goals Scoring and Captain-ship\",\n            subtitle = \"This is plotly so click on the points on legend\")+\n    theme_parksAndRec()+labs(color=\"Position\")\np1\n\n\n\n#plotly::plotly_build(p1)"
  },
  {
    "objectID": "tidytuesday/2019/Week_28/index.html#age-vs-goals",
    "href": "tidytuesday/2019/Week_28/index.html#age-vs-goals",
    "title": "Week 28: FIFA Womens World Cup",
    "section": "Age vs Goals",
    "text": "Age vs Goals\n\np1&lt;-ggplot(squads,aes(age,goals,color=pos,label1=player,label2=country,group=1))+geom_point()+\n    xlab(\"Age of the Player\")+ylab(\"No of Goals Scored\")+\n    ggtitle(\"Relationship Between Goals Scoring and Age\",\n            subtitle = \"This is plotly so click on the points on legend\")+\n    theme_parksAndRec()+labs(color=\"Position\")\np1\n\n\n\n#plotly::plotly_build(p1)"
  },
  {
    "objectID": "tidytuesday/2019/Week_28/index.html#country-and-positions",
    "href": "tidytuesday/2019/Week_28/index.html#country-and-positions",
    "title": "Week 28: FIFA Womens World Cup",
    "section": "Country and Positions",
    "text": "Country and Positions\n\nsquads %&gt;%\n  group_by(country,pos) %&gt;%\n  count() %&gt;%\n  ggplot(.,aes(country,n,fill=pos,label=n))+geom_col()+\n         theme_parksAndRec()+labs(fill=\"Position\")+\n         xlab(\"Country\")+ylab(\"Count of Players\")+geom_text(position = \"stack\",vjust=1)+\n         ggtitle(\"Positions and their counts according to Country\")+\n         theme(axis.text.x = element_text(angle = 90,vjust=0))"
  },
  {
    "objectID": "tidytuesday/2019/Week_28/index.html#age-and-positions",
    "href": "tidytuesday/2019/Week_28/index.html#age-and-positions",
    "title": "Week 28: FIFA Womens World Cup",
    "section": "Age and Positions",
    "text": "Age and Positions\n\nsquads %&gt;%\n  group_by(age,pos) %&gt;%\n  count() %&gt;%\n  ggplot(.,aes(factor(age),n,fill=pos,label=n))+geom_col()+\n         xlab(\"Age\")+ylab(\"Count of Players\")+geom_text(position = \"stack\",vjust=1,size=3.5)+\n         ggtitle(\"Positions and their counts according to Age\")+\n         theme_parksAndRec()+labs(fill=\"Position\")"
  },
  {
    "objectID": "tidytuesday/2019/Week_28/index.html#country-and-age-distributions",
    "href": "tidytuesday/2019/Week_28/index.html#country-and-age-distributions",
    "title": "Week 28: FIFA Womens World Cup",
    "section": "Country and Age Distributions",
    "text": "Country and Age Distributions\n\nggplot(squads,aes(age,country))+\n       ggridges::geom_density_ridges(scale=1.45,jittered_points=TRUE,fill=\"green\",alpha=0.8)+\n       theme_parksAndRec()+\n       scale_x_continuous(breaks=seq(10,45,5),labels=seq(10,45,5))+\n       scale_y_discrete(expand = c(0, 1))+\n       xlab(\"Age\")+ylab(\"Country\")+\n       ggtitle(\"Country vs Age Distribution\")"
  },
  {
    "objectID": "tidytuesday/2019/Week_28/index.html#country-and-goal-experience",
    "href": "tidytuesday/2019/Week_28/index.html#country-and-goal-experience",
    "title": "Week 28: FIFA Womens World Cup",
    "section": "Country and Goal Experience",
    "text": "Country and Goal Experience\n\nsquads %&gt;%\n  group_by(country=factor(country)) %&gt;%\n  na.omit() %&gt;%\n  summarise(goal_count=sum(goals)) %&gt;%\nggplot(.,aes(x=country,y=goal_count,label=goal_count))+geom_col(fill=\"lightgreen\")+\n       theme_parksAndRec_light()+geom_text(vjust=-0.5,color=blues9[9],size=5)+\n       xlab(\"Country\")+ylab(\"No of Goals Scored by All Players\")+\n       ggtitle(\"No of Goals Scored with Country as in Experience\")+\n       theme(axis.text.x = element_text(angle = 90,vjust=0))"
  },
  {
    "objectID": "tidytuesday/2019/Week_28/index.html#top-15-highest-goal-scorers-participated-in-the-fifa-world-cup",
    "href": "tidytuesday/2019/Week_28/index.html#top-15-highest-goal-scorers-participated-in-the-fifa-world-cup",
    "title": "Week 28: FIFA Womens World Cup",
    "section": "Top 15 Highest Goal Scorers participated in the FIFA World Cup",
    "text": "Top 15 Highest Goal Scorers participated in the FIFA World Cup\n\nsquads %&gt;%\n  top_n(15,goals) %&gt;%\n  select(country,player,goals) %&gt;% \nggplot(.,aes(stringr::str_wrap(player,10),goals,fill=country,label=goals))+geom_col()+\n       xlab(\"Player\")+ylab(\"No of Goals\")+theme_parksAndRec_light()+\n       labs(fill=\"Country\")+geom_text(vjust=-0.5,size=7,color=\"green\")+\n       theme(axis.text.x = element_text(angle = 90,vjust=0))+\n       ggtitle(\"Top 15 Players who have played most number of Goals So Far\")"
  },
  {
    "objectID": "tidytuesday/2019/Week_28/index.html#year-vs-countries-of-participation",
    "href": "tidytuesday/2019/Week_28/index.html#year-vs-countries-of-participation",
    "title": "Week 28: FIFA Womens World Cup",
    "section": "Year vs Countries of participation",
    "text": "Year vs Countries of participation\n\nwwc_outcomes %&gt;%\n  group_by(year,team) %&gt;%\n  count(team) %&gt;%\n  select(year,team) %&gt;%\n  ungroup() %&gt;%\n  count(year) %&gt;%\nggplot(.,aes(factor(year),n,label=n))+geom_col(fill=blues9[5])+\n      geom_text(vjust=1,size=9,color=blues9[3])+theme_brooklyn99()+\n      xlab(\"Year\")+ylab(\"No of Countries Participated\")+\n      ggtitle(\"No of Countries participated by Year\")\n\n\n\n\n\nwwc_outcomes %&gt;%\n  group_by(year,team) %&gt;%\n  count(team) %&gt;% \nggplot(.,aes(stringr::str_wrap(team,12),n,label=n))+\n      geom_col(fill=blues9[5])+\n      geom_text(vjust=1,size=6,color=blues9[3])+theme_brooklyn99()+\n      xlab(\"Country\")+ylab(\"No of Matches\")+\n      transition_states(year)+ease_aes(\"linear\")+\n      theme(axis.text.x = element_text(angle = 90,vjust=0))+\n      ggtitle(\"No of Matches played by a country changing over the Tournament\",\n              subtitle = \"Year:{closest_state}\")"
  },
  {
    "objectID": "tidytuesday/2019/Week_28/index.html#year-vs-rounds-of-matches",
    "href": "tidytuesday/2019/Week_28/index.html#year-vs-rounds-of-matches",
    "title": "Week 28: FIFA Womens World Cup",
    "section": "Year vs Rounds of Matches",
    "text": "Year vs Rounds of Matches\n\nwwc_outcomes %&gt;%\n  group_by(year,round) %&gt;%\n  count(round) %&gt;%\n  select(year,round) %&gt;%\n  ungroup() %&gt;%\n  count(year) %&gt;%\nggplot(.,aes(factor(year),n,label=n))+geom_col(fill=blues9[5])+\n      geom_text(vjust=1,size=9,color=blues9[3])+theme_brooklyn99()+\n      xlab(\"Year\")+ylab(\"No of Rounds in the Tournament\")+\n      ggtitle(\"No of Rounds by Year\")  \n\n\n\n\n\nwwc_outcomes %&gt;%\n  group_by(year,round) %&gt;%\n  count(round) %&gt;%\n  mutate(matches=n/2) %&gt;%\nggplot(.,aes(stringr::str_wrap(round,12),matches,label=matches))+\n      geom_col(fill=blues9[5])+\n      geom_text(vjust=1,size=6,color=blues9[3])+theme_brooklyn99()+\n      xlab(\"Rounds of the Tournament\")+ylab(\"No of Matches\")+\n      transition_states(year)+ease_aes(\"linear\")+\n      ggtitle(\"No of Matches changing over the Tournament\",\n              subtitle = \"Year:{closest_state}\")"
  },
  {
    "objectID": "tidytuesday/2019/Week_28/index.html#years-vs-goals",
    "href": "tidytuesday/2019/Week_28/index.html#years-vs-goals",
    "title": "Week 28: FIFA Womens World Cup",
    "section": "Years vs Goals",
    "text": "Years vs Goals\n\nwwc_outcomes %&gt;%\n  group_by(year) %&gt;%\n  count(score) %&gt;%\nggplot(.,aes(factor(score),n,label=n))+geom_col(fill=blues9[5])+\n      geom_text(vjust=1,size=6,color=blues9[3])+theme_brooklyn99()+\n      transition_states(year)+ease_aes(\"linear\")+\n      xlab(\"Goals Scored\")+ylab(\"No of Situations\")+\n      ggtitle(\"No of Goals Scored by Year with Situations\",\n              subtitle = \"Year :{closest_state}\")  \n\n\n\n\nTHANK YOU"
  },
  {
    "objectID": "tidytuesday/2019/Week_26/index.html",
    "href": "tidytuesday/2019/Week_26/index.html",
    "title": "Week 26: Unidentified Flying Objects",
    "section": "",
    "text": "ufo_sightings &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-06-25/ufo_sightings.csv\")\n\nlibrary(tidyverse)\nlibrary(gganimate)\nlibrary(ggthemes)\nlibrary(lubridate)\nlibrary(ggforce)\nlibrary(ggthemr)\nGitHub\n{{% tweet \"1143566481801850880\" %}}"
  },
  {
    "objectID": "tidytuesday/2019/Week_26/index.html#yearly-count-changes",
    "href": "tidytuesday/2019/Week_26/index.html#yearly-count-changes",
    "title": "Week 26: Unidentified Flying Objects",
    "section": "Yearly count changes",
    "text": "Yearly count changes\n\nufo_sightings$date_time &lt;- mdy_hm(ufo_sightings$date_time)\n\nufo_sightings %&gt;%\n  mutate(year=year(date_time)) %&gt;%\n  count(year) %&gt;%\n  ggplot(.,aes(x=as.factor(year),y=n))+geom_col(fill=blues9[4])+\n         theme_minimal()+\n         theme(axis.text.x = element_text(angle = 90))+\n         geom_text(aes(label=n),hjust=-0.1,angle=90,color=blues9[9])+\n         xlab(\"Years\")+ylab(\"Frequency\")+\n         ggtitle(\"Yearly count changes\")"
  },
  {
    "objectID": "tidytuesday/2019/Week_26/index.html#monthly-count-changes",
    "href": "tidytuesday/2019/Week_26/index.html#monthly-count-changes",
    "title": "Week 26: Unidentified Flying Objects",
    "section": "Monthly count changes",
    "text": "Monthly count changes\n\nufo_sightings %&gt;%\n  mutate(month=month(date_time)) %&gt;%\n  count(month) %&gt;%\n  ggplot(.,aes(x=as.factor(month),y=n))+geom_col(fill=blues9[4])+\n         theme_minimal()+\n         geom_text(aes(label=n),vjust=-0.1,color=blues9[9])+\n         xlab(\"Months\")+ylab(\"Frequency\")+\n         ggtitle(\"Monthly count changes\")"
  },
  {
    "objectID": "tidytuesday/2019/Week_26/index.html#hourly-count-changes",
    "href": "tidytuesday/2019/Week_26/index.html#hourly-count-changes",
    "title": "Week 26: Unidentified Flying Objects",
    "section": "Hourly count changes",
    "text": "Hourly count changes\n\nufo_sightings %&gt;%\n  mutate(hour=hour(date_time)) %&gt;%\n  count(hour) %&gt;%\n  ggplot(.,aes(x=as.factor(hour),y=n))+geom_col(fill=blues9[4])+\n         theme_minimal()+\n         geom_text(aes(label=n),vjust=-0.1,color=blues9[9])+\n         xlab(\"Hours\")+ylab(\"Frequency\")+\n         ggtitle(\"Hourly count changes\")"
  },
  {
    "objectID": "tidytuesday/2019/Week_26/index.html#yearly-count-changes-for-countries",
    "href": "tidytuesday/2019/Week_26/index.html#yearly-count-changes-for-countries",
    "title": "Week 26: Unidentified Flying Objects",
    "section": "Yearly Count changes for Countries",
    "text": "Yearly Count changes for Countries\n\nufo_sightings %&gt;%\n   mutate(year=year(date_time)) %&gt;%\n   mutate(country=recode_factor(country,\n                                \"au\"=\"Australia\",\n                                \"ca\"=\"Canada\",\n                                \"gb\"=\"Great Britain\",\n                                \"us\"=\"USA\")) %&gt;%\n   group_by(year,country) %&gt;%\n   remove_missing() %&gt;%\n   count(country) %&gt;%\n   ggplot(.,aes(x=as.factor(year),y=n))+geom_col(fill=blues9[8])+\n         theme_minimal()+\n         facet_wrap(~country,scales = \"free_y\",ncol = 1)+\n         theme(axis.text.x = element_text(angle = 90))+\n         xlab(\"Year\")+ylab(\"Frequency\")+\n         ggtitle(\"Yearly count changes for Nations\")"
  },
  {
    "objectID": "tidytuesday/2019/Week_26/index.html#monthly-count-changes-for-countries",
    "href": "tidytuesday/2019/Week_26/index.html#monthly-count-changes-for-countries",
    "title": "Week 26: Unidentified Flying Objects",
    "section": "Monthly Count changes for Countries",
    "text": "Monthly Count changes for Countries\n\nufo_sightings %&gt;%\n   mutate(month=month(date_time)) %&gt;%\n   mutate(country=recode_factor(country,\n                                \"au\"=\"Australia\",\n                                \"ca\"=\"Canada\",\n                                \"gb\"=\"Great Britain\",\n                                \"us\"=\"USA\")) %&gt;%\n   group_by(month,country) %&gt;%\n   remove_missing() %&gt;%\n   count(country) %&gt;%\n   ggplot(.,aes(x=as.factor(month),y=n))+geom_col(fill=blues9[8])+\n         theme_minimal()+\n         facet_wrap(~country,scales = \"free_y\",ncol = 1)+\n         geom_text(aes(label=n),vjust=0.95,color=\"white\")+\n         xlab(\"Months\")+ylab(\"Frequency\")+\n         ggtitle(\"Monthly count changes for Nations\")"
  },
  {
    "objectID": "tidytuesday/2019/Week_26/index.html#hourly-count-changes-for-countries",
    "href": "tidytuesday/2019/Week_26/index.html#hourly-count-changes-for-countries",
    "title": "Week 26: Unidentified Flying Objects",
    "section": "Hourly Count changes for Countries",
    "text": "Hourly Count changes for Countries\n\nufo_sightings %&gt;%\n   mutate(hour=hour(date_time)) %&gt;%\n   mutate(country=recode_factor(country,\n                                \"au\"=\"Australia\",\n                                \"ca\"=\"Canada\",\n                                \"gb\"=\"Great Britain\",\n                                \"us\"=\"USA\")) %&gt;%\n   group_by(hour,country) %&gt;%\n   remove_missing() %&gt;%\n   count(country) %&gt;%\n   ggplot(.,aes(x=as.factor(hour),y=n))+geom_col(fill=blues9[8])+\n         theme_minimal()+\n         facet_wrap(~country,scales = \"free_y\",ncol = 1)+\n         geom_text(aes(label=n),vjust=0.95,color=\"white\")+\n         xlab(\"Hours\")+ylab(\"Frequency\")+\n         ggtitle(\"Hours count changes for Nations\")"
  },
  {
    "objectID": "tidytuesday/2019/Week_26/index.html#decade-wise-ufo-shape-changes",
    "href": "tidytuesday/2019/Week_26/index.html#decade-wise-ufo-shape-changes",
    "title": "Week 26: Unidentified Flying Objects",
    "section": "Decade wise UFO shape changes",
    "text": "Decade wise UFO shape changes\n\nufo_sightings_new&lt;-ufo_sightings\n\nufo_sightings_new$year&lt;-year(ufo_sightings_new$date_time)\n\nufo_sightings_new$year&lt;-cut(ufo_sightings_new$year,\n                            breaks=c(1910,1919,1929,1939,\n                                     1949,1959,1969,1979,\n                                     1989,1999,2009,2015),\n                            labels=c(\"1910s\",\"1920s\",\"1930s\",\"1940s\",\n                                     \"1950s\",\"1960s\",\"1970s\",\"1980s\",\n                                     \"1990s\",\"2000s\",\"2010s\"))\n\nufo_sightings_new %&gt;%\n  group_by(year,ufo_shape) %&gt;%\n  count(ufo_shape) %&gt;%\n  remove_missing() %&gt;%\n  ggplot(.,aes(x=str_wrap(ufo_shape,10),y=n))+geom_col(fill=blues9[8])+\n         theme_minimal()+\n         theme(axis.text.x = element_text(angle = 90))+\n         geom_text(aes(label=n),hjust=-0.1,angle=90)+\n         scale_y_continuous(expand = c(0,1000))+\n         transition_states(year)+ease_aes(\"linear\")+\n         xlab(\"Shape\")+ylab(\"Frequency\")+\n         ggtitle(\"UFO shapes over the Years\",\n                 subtitle = \"Year:{closest_state}\")"
  },
  {
    "objectID": "tidytuesday/2019/Week_24/index.html",
    "href": "tidytuesday/2019/Week_24/index.html",
    "title": "Week 24: Meteorite Data",
    "section": "",
    "text": "This week has data for Meteors from 8th century to year 2013(well 2101 is an error).\nSo I have used the packages tidyverse,gganimate,maps and ggthemes to make this more interesting. There are three plots which I have generated, two of them are inspired by others.\n\nmeteorites &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-06-11/meteorites.csv\")\n\nlibrary(tidyverse)\nlibrary(gganimate)\nlibrary(maps)\nlibrary(ggthemes)\n\nMeteorites over the Year\nBased on the below tweet I was inspired to creata a similar plot which is animated by years.\n\n\n{{% tweet \"1138187182559170560\" %}}\n\n\nFrames change is based on year, and meteors which have missing values are dropped.\n\nmeteorites_rm&lt;-remove_missing(meteorites)\n\nworld &lt;- ggplot() + theme_map()+\n         borders(\"world\", colour = \"#353535\", fill = \"#74e04a\") \n\nplot1&lt;-world+geom_point(data=meteorites_rm,x=meteorites_rm$long,y=meteorites_rm$lat,\n                        color=\"#e20e06\")+\n             transition_states(meteorites_rm$year,transition_length = 1,state_length = 1)+\n             theme(plot.title = element_text(color = \"black\", \n                                           size = 15, \n                                           face = \"bold\"),\n                   panel.background = element_rect(fill = \"#5e8bed\")) +\n             ggtitle(\"Meteors Falling from the sky by Name Type\",subtitle = \"Year:{closest_state}\")\n\nanimate(plot1,nframes=253,fps=1,detail = 1)\n\n\n\n\nMeteorites and Classes\nA Bar plot which changes for the top 5 classes based on their counts after the year 1999. This plot is based on the blog post published in Rbloggers. This type is a very popular animated bar plot.\nLink for the Blog post\n\ntest&lt;-meteorites%&gt;%\n          subset(year &gt;= 2000 & year &lt; 2101) %&gt;%\n          group_by(year,class) %&gt;%\n          count(class) %&gt;%\n          group_by(year)%&gt;%     \n          mutate(rank=rank(-n),\n                 value_lbl=paste0(\"\",n)) %&gt;%\n          group_by(class) %&gt;%\n          filter(rank &lt;=5) %&gt;%\n          ungroup()\n\nplot3&lt;-ggplot(test, aes(rank, group = class, \n              fill = as.factor(class), color = as.factor(class))) +\n       geom_tile(aes(y = n/2,height = n,width = 0.9), alpha = 0.8, color = NA) +\n       geom_text(aes(y = 0, label = paste(class, \" \")), vjust = 0.2, hjust = 1) +\n       geom_text(aes(y=n,label = value_lbl, hjust=0)) +\n       coord_flip(clip = \"off\", expand = FALSE) +\n       scale_y_continuous(labels = scales::comma) +\n       scale_x_reverse() +\n       guides(color = FALSE, fill = FALSE) +\n       theme(axis.line=element_blank(),\n             axis.text.x=element_blank(),\n             axis.text.y=element_blank(),\n             axis.ticks=element_blank(),\n             axis.title.x=element_blank(),\n             axis.title.y=element_blank(),\n             legend.position=\"none\",\n             panel.background=element_blank(),\n             panel.border=element_blank(),\n             panel.grid.major=element_blank(),\n             panel.grid.minor=element_blank(),\n             panel.grid.major.x = element_line( size=.1, color=\"grey\" ),\n             panel.grid.minor.x = element_line( size=.1, color=\"grey\" ),\n             plot.title=element_text(size=25, hjust=0.5, face=\"bold\", colour=\"grey\", vjust=-1),\n             plot.subtitle=element_text(size=18, hjust=0.5, face=\"italic\", color=\"grey\"),\n             plot.caption =element_text(size=8, hjust=0.5, face=\"italic\", color=\"grey\"),\n             plot.background=element_blank(),\n             plot.margin = margin(2,2, 2, 4, \"cm\")) +\n       transition_states(year, transition_length = 3, state_length = 2) +\n  view_follow(fixed_x = TRUE)  +\n  labs(title = 'Meteorites Classes by Year: {closest_state}',  \n       subtitle  =  \"Top 5 Classes\") \n\n# For GIF\nanimate(plot3, nframes=200, fps = 20) \n\n\n\n\nBelow is the same plot but not animated, therefore we can understand patiently how the top 5 classes counts change after year 1999.\n\ntest %&gt;%\n     ggplot(.,aes(x=class,y=n,label=rank,fill=class))+geom_col()+\n     facet_wrap(~factor(year))+geom_text(vjust=-0.5)+\n     geom_text(aes(label=n),vjust=1)+\n     ggtitle(\"Meteors class count from 2000 to 2013\",subtitle = \"Top 5 Ranks\")+\n     scale_y_continuous(expand = c(0,75))+theme_dark()+\n     theme(axis.text.x = element_text(angle = 45))\n\n\n\n\nSo I posted a tweet about the Meteor data for week 24 of #TidyTuesday.\n\n\n{{% tweet \"1138476351453913090\" %}}\n\n\nGitHub Code\nTHANK YOU"
  },
  {
    "objectID": "tidytuesday/2019/Week_2/index.html",
    "href": "tidytuesday/2019/Week_2/index.html",
    "title": "Week 2: IMDB TV Shows Data",
    "section": "",
    "text": "# loading the packages\nlibrary(tidyverse)\nlibrary(summarytools)\nlibrary(magrittr)\nlibrary(readr)\nlibrary(lubridate)\nlibrary(gganimate)\nlibrary(stringr)\n\n# load the dataset\nRatings &lt;- read_csv(\"IMDb_Economist_tv_ratings.csv\", \n                    col_types = cols(date = col_date(format = \"%Y-%m-%d\")))\nRatings data-set is from the IMDB site. I just found out that IMDb is active from 1990, that is a very long time and new information to me.\n{{% tweet \"1082693109304168448\" %}}\nTV shows from 1990 to 2018 with their ratings, genres, sharing and aired dates is in this data-set. I wanted a cool function to summarize the data-set at once, therefore browse through the internet and found the package summarytools. There are quite a few functions in the mix, yet I choose dfSummary.\nBelow is the code of using that function on the Ratings data-set.\nData Frame Summary  \nRatings  \nDimensions: 2266 x 7  \nDuplicates: 1  \n\n+----+--------------+------------------------------+----------------------+---------------------+----------+---------+\n| No | Variable     | Stats / Values               | Freqs (% of Valid)   | Graph               | Valid    | Missing |\n+====+==============+==============================+======================+=====================+==========+=========+\n| 1  | titleId      | 1. tt0098844                 |   20 ( 0.9%)         |                     | 2266     | 0       |\n|    | [character]  | 2. tt0203259                 |   20 ( 0.9%)         |                     | (100.0%) | (0.0%)  |\n|    |              | 3. tt0118401                 |   19 ( 0.8%)         |                     |          |         |\n|    |              | 4. tt0108757                 |   15 ( 0.7%)         |                     |          |         |\n|    |              | 5. tt0247082                 |   15 ( 0.7%)         |                     |          |         |\n|    |              | 6. tt0413573                 |   15 ( 0.7%)         |                     |          |         |\n|    |              | 7. tt0452046                 |   14 ( 0.6%)         |                     |          |         |\n|    |              | 8. tt0118375                 |   13 ( 0.6%)         |                     |          |         |\n|    |              | 9. tt0460681                 |   13 ( 0.6%)         |                     |          |         |\n|    |              | 10. tt0460627                |   12 ( 0.5%)         |                     |          |         |\n|    |              | [ 866 others ]               | 2110 (93.1%)         | IIIIIIIIIIIIIIIIII  |          |         |\n+----+--------------+------------------------------+----------------------+---------------------+----------+---------+\n| 2  | seasonNumber | Mean (sd) : 3.3 (3.4)        | 27 distinct values   | :                   | 2266     | 0       |\n|    | [numeric]    | min &lt; med &lt; max:             |                      | :                   | (100.0%) | (0.0%)  |\n|    |              | 1 &lt; 2 &lt; 44                   |                      | :                   |          |         |\n|    |              | IQR (CV) : 3 (1.1)           |                      | :                   |          |         |\n|    |              |                              |                      | : .                 |          |         |\n+----+--------------+------------------------------+----------------------+---------------------+----------+---------+\n| 3  | title        | 1. Law & Order               |   20 ( 0.9%)         |                     | 2266     | 0       |\n|    | [character]  | 2. Law & Order: Special Vict |   20 ( 0.9%)         |                     | (100.0%) | (0.0%)  |\n|    |              | 3. Midsomer Murders          |   19 ( 0.8%)         |                     |          |         |\n|    |              | 4. CSI: Crime Scene Investig |   15 ( 0.7%)         |                     |          |         |\n|    |              | 5. ER                        |   15 ( 0.7%)         |                     |          |         |\n|    |              | 6. Grey's Anatomy            |   15 ( 0.7%)         |                     |          |         |\n|    |              | 7. Criminal Minds            |   14 ( 0.6%)         |                     |          |         |\n|    |              | 8. King of the Hill          |   13 ( 0.6%)         |                     |          |         |\n|    |              | 9. Supernatural              |   13 ( 0.6%)         |                     |          |         |\n|    |              | 10. Bones                    |   12 ( 0.5%)         |                     |          |         |\n|    |              | [ 858 others ]               | 2110 (93.1%)         | IIIIIIIIIIIIIIIIII  |          |         |\n+----+--------------+------------------------------+----------------------+---------------------+----------+---------+\n| 4  | date         | min : 1990-01-03             | 1808 distinct values |                   : | 2266     | 0       |\n|    | [Date]       | med : 2012-12-07             |                      |                 . : | (100.0%) | (0.0%)  |\n|    |              | max : 2018-10-10             |                      |               . : : |          |         |\n|    |              | range : 28y 9m 7d            |                      |           . : : : : |          |         |\n|    |              |                              |                      | . . . . : : : : : : |          |         |\n+----+--------------+------------------------------+----------------------+---------------------+----------+---------+\n| 5  | av_rating    | Mean (sd) : 8.1 (0.7)        | 1997 distinct values |               :     | 2266     | 0       |\n|    | [numeric]    | min &lt; med &lt; max:             |                      |               : :   | (100.0%) | (0.0%)  |\n|    |              | 2.7 &lt; 8.1 &lt; 9.7              |                      |               : :   |          |         |\n|    |              | IQR (CV) : 0.8 (0.1)         |                      |             . : :   |          |         |\n|    |              |                              |                      |             : : : . |          |         |\n+----+--------------+------------------------------+----------------------+---------------------+----------+---------+\n| 6  | share        | Mean (sd) : 1.3 (3.4)        | 454 distinct values  | :                   | 2266     | 0       |\n|    | [numeric]    | min &lt; med &lt; max:             |                      | :                   | (100.0%) | (0.0%)  |\n|    |              | 0 &lt; 0.3 &lt; 55.6               |                      | :                   |          |         |\n|    |              | IQR (CV) : 1 (2.6)           |                      | :                   |          |         |\n|    |              |                              |                      | :                   |          |         |\n+----+--------------+------------------------------+----------------------+---------------------+----------+---------+\n| 7  | genres       | 1. Crime,Drama,Mystery       | 369 (16.3%)          | III                 | 2266     | 0       |\n|    | [character]  | 2. Comedy,Drama              | 174 ( 7.7%)          | I                   | (100.0%) | (0.0%)  |\n|    |              | 3. Drama                     | 168 ( 7.4%)          | I                   |          |         |\n|    |              | 4. Action,Crime,Drama        | 146 ( 6.4%)          | I                   |          |         |\n|    |              | 5. Action,Adventure,Drama    | 112 ( 4.9%)          |                     |          |         |\n|    |              | 6. Crime,Drama               | 107 ( 4.7%)          |                     |          |         |\n|    |              | 7. Drama,Romance             |  86 ( 3.8%)          |                     |          |         |\n|    |              | 8. Comedy,Crime,Drama        |  80 ( 3.5%)          |                     |          |         |\n|    |              | 9. Comedy,Drama,Romance      |  76 ( 3.4%)          |                     |          |         |\n|    |              | 10. Crime,Drama,Thriller     |  63 ( 2.8%)          |                     |          |         |\n|    |              | [ 87 others ]                | 885 (39.1%)          | IIIIIII             |          |         |\n+----+--------------+------------------------------+----------------------+---------------------+----------+---------+\nBasic summary of seasons indicate 27 distinct values and obviously 1 is the minimum value, but the maximum value is 44. Another odd thing is median for being 2, further the average is 3.26. Which means most of the TV shows have only up-to few seasons. I would guess not more than 5.\nSummary of date indicates the earliest TV show from 1990 and latest from 2018. So the year difference is 28 years, but there are only 1808 distinct values. Even though the data-set contains 2266 observations. Some TV shows might have to start on the same day, that is the only plausible conclusion.\n“av_rating” (I presume Audio/Video Rating) is in the scale from 1 to 10, where people had influence. The least rating value is 2.7 and the most rating value is 9.68, but the median is 8.11. Also the mean is 8.06. We can say most of these TV shows are excellent to watch. There is another numeric variable called share and the value ranges from 0 to 55.65 where the average is 1.28."
  },
  {
    "objectID": "tidytuesday/2019/Week_2/index.html#genre-and-season",
    "href": "tidytuesday/2019/Week_2/index.html#genre-and-season",
    "title": "Week 2: IMDB TV Shows Data",
    "section": "Genre and Season",
    "text": "Genre and Season\nGenre and Season are two categorical variables which should be compared to find out if over time do people like the same genre type. If we look at the bar plot it is clear “Crime,Drama,Mystery” type has seasons from 1 to 20. Mostly in all genres there is clear sign of TV shows with seasons up-to three or four. Some of them make it to season ten or eleven, for example genres like “Drama”, “Drama,Thriller”, “Animation,Comedy,Drama” and “Adventure,Drama,Family”.\nOddly in “Drama,Romance” and “Crime,Drama” there are TV shows which has seasons above 35 but very few. It becomes more weird where for the same genre types the seasons in-between 25 and 34 are missing. Clearly in the legend also until season 20 there is continuity, but this does not carry on for higher seasons.\n\nggplot(Ratings,aes(x=fct_infreq(factor(genres)),fill=factor(seasonNumber)))+\n  geom_bar()+ coord_flip()+labs(fill=\"Season\")+\n  xlab(\"Genre\")+ylab(\"Frequency\")+\n  ggtitle(\"Genre and Seasons\")+\n  scale_y_continuous(breaks=seq(0,375,25),labels=seq(0,375,25))"
  },
  {
    "objectID": "tidytuesday/2019/Week_2/index.html#genre-and-year",
    "href": "tidytuesday/2019/Week_2/index.html#genre-and-year",
    "title": "Week 2: IMDB TV Shows Data",
    "section": "Genre and Year",
    "text": "Genre and Year\nBar plot indicates that after 2014 around 90% of these genre type TV shows have been done. In the top ten category according to the counts of TV shows clearly all of these genres have been active since 1990 to now. Some of them were started in mid 1990s which include the genre types “Action,Crime,Drama”, “Crime,Drama” and “Comedy,Crime,Drama”.\nTypes such as “Drama,Romance,Sport” and “Adventure,Drama,Romance” were in active in the mid 2000s but no longer. Genres such as “Drama,History”, “Drama,Horror,Thriller” and “Action,Drama” are a few of them which were popular after 2012.\n\nggplot(Ratings,aes(x=fct_infreq(factor(genres)),fill=factor(year(date))))+\n  geom_bar()+ coord_flip()+labs(fill=\"Year\")+\n  xlab(\"Genre\")+ylab(\"Frequency\")+\n  ggtitle(\"Genre Over the Year\")+\n  scale_y_continuous(breaks=seq(0,375,25),labels=seq(0,375,25))"
  },
  {
    "objectID": "tidytuesday/2019/Week_2/index.html#genre-and-month",
    "href": "tidytuesday/2019/Week_2/index.html#genre-and-month",
    "title": "Week 2: IMDB TV Shows Data",
    "section": "Genre and Month",
    "text": "Genre and Month\nMonth of airing might have an influence on the TV shows. Bar plot indicates that most of the TV shows are aired in the first quarter or last quarter of the year. Which means shows aired in the fall (September or October) or aired after winter break (January).\n\nggplot(Ratings,aes(x=fct_infreq(factor(genres)),fill=factor(month(date))))+\n  geom_bar()+ coord_flip()+ labs(fill=\"Month\")+\n  xlab(\"Genre\")+ylab(\"Frequency\")+\n  ggtitle(\"Genre Over the Months\")+\n  scale_y_continuous(breaks=seq(0,375,25),labels=seq(0,375,25))"
  },
  {
    "objectID": "tidytuesday/2019/Week_2/index.html#season-and-year",
    "href": "tidytuesday/2019/Week_2/index.html#season-and-year",
    "title": "Week 2: IMDB TV Shows Data",
    "section": "Season and Year",
    "text": "Season and Year\nClearly there is an increase in TV shows aired over the years. All time low occurs in 1992 but all time high occurs in 2017. From 1990 to 2010 the TV shows aired have increased from 25 to 100. By the end of year 2017 the number of shows aired has reached more than 250, but its drops to slightly above 175 the next year. The all time low of less than 12 seasons occurs in 1990.\nIn the years 1990,1996,2005,2007,2010,2011 and 2015 there are TV shows which has season above 30, but it should be reminded that according to the legend after season 20 there is no continuity.\nIf we focus closely until 2005 most of the TV shows have seasons up-to 10 , but after 2005 there are TV shows which aired season until 20. This shows the popularity of certain shows over three decades.\n\nggplot(Ratings,aes(x=factor(year(date)),fill=factor(seasonNumber)))+\n  geom_bar()+ coord_flip()+labs(fill=\"Season\")+\n  xlab(\"Year\")+ylab(\"Frequency\")+\n  ggtitle(\"Seasons over the Years\")+\n  scale_y_continuous(breaks=seq(0,230,10),labels=seq(0,230,10))"
  },
  {
    "objectID": "tidytuesday/2019/Week_2/index.html#season-and-month",
    "href": "tidytuesday/2019/Week_2/index.html#season-and-month",
    "title": "Week 2: IMDB TV Shows Data",
    "section": "Season and Month",
    "text": "Season and Month\nHighest amount of more than 500 shows were aired in January and lowest amount of slightly less than 100 was aired in June. Second place goes to February with shows close to 200 being aired this is not even the half of what aired in the previous month. In the months of January, February and September most of the seasons were aired, while in the other months the seasons aired are from the range of 1 to 10. Where very few of them ever reached the double digits or above season 8.\n\nggplot(Ratings,aes(x=factor(month(date)),fill=factor(seasonNumber)))+\n  geom_bar()+coord_flip()+labs(fill=\"Season\")+\n  xlab(\"Month\")+ylab(\"Frequency\")+\n  ggtitle(\"Seasons over the months\")+\n  scale_y_continuous(breaks=seq(0,550,25),labels=seq(0,550,25))"
  },
  {
    "objectID": "tidytuesday/2019/Week_2/index.html#crime-drama-mystery",
    "href": "tidytuesday/2019/Week_2/index.html#crime-drama-mystery",
    "title": "Week 2: IMDB TV Shows Data",
    "section": "Crime Drama Mystery",
    "text": "Crime Drama Mystery\nLaw and Order of 20 seasons has been over for more than 5 years and it began airing in 1990. Law and Order Special Victims Unit started airing in 1999 even now its still being aired. There are also odd shows like which has not aired continuously and skipped an year or two. Among them Columbo, Agatha Christie’s Marple and II commissario Montalbano are specially noted.\nThere are lot of shows which only aired one or two seasons only and then stopped. They also can be noted from the bar plot. In the legend there are colors to indicate all the years from 1990 to 2018.\n\nsubset(Ratings,genres==\"Crime,Drama,Mystery\") %&gt;%\n  ggplot(aes(x=fct_infreq(title),fill=factor(year(date)),color=factor(year(date))))+\n    geom_bar()+ coord_flip()+labs(color=\"Year\",fill=\"Year\")+\n    xlab(\"TV shows\")+ylab(\"Frequency\")+\n    ggtitle(\"'Crime,Drama,Mystery' Genre type over the Years\")+\n    scale_y_continuous(breaks=0:20,labels=0:20)"
  },
  {
    "objectID": "tidytuesday/2019/Week_2/index.html#comedy-drama",
    "href": "tidytuesday/2019/Week_2/index.html#comedy-drama",
    "title": "Week 2: IMDB TV Shows Data",
    "section": "Comedy Drama",
    "text": "Comedy Drama\nSimilarly as above we can interpret the bar plot as well. But here the highest amount of seasons any TV show has reached is 9. Only the TV shows “Scrubs” and “Shameless” have reached that milestone and both of them begin in different decades. “Scrubs” began in early 2000s, but “Shameless” was aired after 2010.\nIn the years 1998 and 1999 there were no TV shows aired under the genre “Comedy,Drama”.\n\nsubset(Ratings,genres==\"Comedy,Drama\") %&gt;%\n    ggplot(aes(x=fct_infreq(title),fill=factor(year(date)),color=factor(year(date))))+\n    geom_bar()+ coord_flip()+labs(color=\"Year\",fill=\"Year\")+\n    xlab(\"TV shows\")+ylab(\"Frequency\")+\n    ggtitle(\"'Comedy,Drama' Genre type over the Years\")+\n    scale_y_continuous(breaks=0:9,labels=0:9)"
  },
  {
    "objectID": "tidytuesday/2019/Week_2/index.html#drama",
    "href": "tidytuesday/2019/Week_2/index.html#drama",
    "title": "Week 2: IMDB TV Shows Data",
    "section": "Drama",
    "text": "Drama\nMost of the TV shows in this genre type are limited to one season an a few more with 2 or 3 seasons. Even though most of them were aired after 2015. TV shows “Mad Men”, “Skins” and “The West Wing” has aired for 7 seasons.\nSome of these shows were limited series like “The News Room”. The only very early TV show is “Rebel Highway” which was aired in 1994 and in year 2004 “Summerland” was aired, where both of them were limited to one season.\nAccording to the legend the years 1992,1993,1995 to 1998 were years free of “Drama” genre TV shows.\n\nsubset(Ratings,genres==\"Drama\") %&gt;%\n ggplot(aes(x=fct_infreq(title),fill=factor(year(date)),color=factor(year(date))))+\n    geom_bar()+ coord_flip()+labs(color=\"Year\",fill=\"Year\")+\n    xlab(\"TV shows\")+ylab(\"Frequency\")+\n    ggtitle(\"'Drama' Genre type over the Years\")+\n    scale_y_continuous(breaks=0:7,labels=0:7)"
  },
  {
    "objectID": "tidytuesday/2019/Week_14/index.html",
    "href": "tidytuesday/2019/Week_14/index.html",
    "title": "Week 14 : Seattle Bikes",
    "section": "",
    "text": "library(tidyverse)\nlibrary(dplyr)\nlibrary(gganimate)\nlibrary(ggthemr)\nlibrary(splitstackshape)\nlibrary(lubridate)\nlibrary(readr)\n\nbike_traffic &lt;- read_csv(\"bike_traffic.csv\")\n\n#bike_traffic &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-04-02/bike_traffic.csv\")\nblogdown::shortcode(\"tweet\",\"1113036696214495232\")\n\n{{% tweet \"1113036696214495232\" %}}\nGitHub Code"
  },
  {
    "objectID": "tidytuesday/2019/Week_14/index.html#monthly-average-bike-count-for-everyday-with-am-or-pm",
    "href": "tidytuesday/2019/Week_14/index.html#monthly-average-bike-count-for-everyday-with-am-or-pm",
    "title": "Week 14 : Seattle Bikes",
    "section": "Monthly Average Bike Count for Everyday with AM or PM",
    "text": "Monthly Average Bike Count for Everyday with AM or PM\n\np&lt;-bike_traffic %&gt;%\n      cSplit(\"date\",sep=\" \") %&gt;%\n      rename(\"DMY\"=\"date_1\") %&gt;%\n      rename(\"HMS\"=\"date_2\") %&gt;%\n      rename(\"MorE\"=\"date_3\") %&gt;%\n      mutate(DMY=mdy(DMY)) %&gt;%\n      mutate(Year=year(DMY)) %&gt;%\n      mutate(Month=month(DMY)) %&gt;%\n      mutate(Day=day(DMY)) %&gt;%\n      group_by(MorE,Year,Month,Day) %&gt;%\n      summarise(Average=mean(bike_count,na.rm = TRUE)) %&gt;%\nggplot(.,aes(y=Average,x=factor(Month),color=MorE))+\n      geom_jitter()+transition_time(Year)+\n      ease_aes(\"linear\")+\n      scale_y_continuous(labels=seq(0,175,5),breaks=seq(0,175,5))+\n      xlab(\"Month\")+ylab(\"Average Bike Count\")+\n      ggtitle(\"Average Bike Count changing with Time\",\n              subtitle = \"Year : {frame_time}\")\n  \nanimate(p,nframes=7,fps=1)"
  },
  {
    "objectID": "tidytuesday/2019/Week_14/index.html#day-by-average-bike-count-for-everyday-with-am-or-pm",
    "href": "tidytuesday/2019/Week_14/index.html#day-by-average-bike-count-for-everyday-with-am-or-pm",
    "title": "Week 14 : Seattle Bikes",
    "section": "Day by Average Bike Count for Everyday with AM or PM",
    "text": "Day by Average Bike Count for Everyday with AM or PM\n\np&lt;-bike_traffic %&gt;%\n      cSplit(\"date\",sep=\" \") %&gt;%\n      rename(\"DMY\"=\"date_1\") %&gt;%\n      rename(\"HMS\"=\"date_2\") %&gt;%\n      rename(\"MorE\"=\"date_3\") %&gt;%\n      mutate(DMY=mdy(DMY)) %&gt;%\n      mutate(HMS=hms(HMS)) %&gt;%\n      mutate(Day=day(DMY)) %&gt;%\n      mutate(Year=year(DMY)) %&gt;%\n      mutate(Month=month(DMY)) %&gt;%\n      group_by(MorE,Year,Month,Day) %&gt;%\n      summarise(Average=mean(bike_count,na.rm = TRUE)) %&gt;%\nggplot(.,aes(y=Average,x=factor(Day),color=MorE))+\n      geom_jitter()+transition_time(Year)+\n      ease_aes(\"linear\")+coord_flip()+\n      scale_y_continuous(labels=seq(0,170,10),breaks=seq(0,170,10))+\n      xlab(\"Day\")+ylab(\"Average Bike Count\")+\n      ggtitle(\"Average Bike Count changing with Time\",\n              subtitle = \"Year : {frame_time}\")\n  \nanimate(p,nframes=7,fps=1)"
  },
  {
    "objectID": "tidytuesday/2019/Week_14/index.html#hourly-average-bike-count-for-every-month-with-am-or-pm",
    "href": "tidytuesday/2019/Week_14/index.html#hourly-average-bike-count-for-every-month-with-am-or-pm",
    "title": "Week 14 : Seattle Bikes",
    "section": "Hourly Average Bike Count for Every Month with AM or PM",
    "text": "Hourly Average Bike Count for Every Month with AM or PM\n\np&lt;-bike_traffic %&gt;%\n      cSplit(\"date\",sep=\" \") %&gt;%\n      rename(\"DMY\"=\"date_1\") %&gt;%\n      rename(\"HMS\"=\"date_2\") %&gt;%\n      rename(\"MorE\"=\"date_3\") %&gt;%\n      mutate(DMY=mdy(DMY)) %&gt;%\n      mutate(HMS=hms(HMS)) %&gt;%\n      mutate(Hour=hour(HMS)) %&gt;%\n      mutate(Year=year(DMY)) %&gt;%\n      mutate(Month=month(DMY)) %&gt;%\n      group_by(MorE,Year,Month,Hour) %&gt;%\n      summarise(Average=mean(bike_count,na.rm = TRUE)) %&gt;%\nggplot(.,aes(y=Average,x=factor(Hour),color=MorE))+\n      geom_jitter()+transition_time(Year)+\n      ease_aes(\"linear\")+\n      xlab(\"Hour\")+ylab(\"Average Bike Count\")+\n      scale_y_continuous(labels=seq(0,55,5),breaks=seq(0,55,5))+\n      ggtitle(\"Average Bike Count changing with Time\",\n              subtitle = \"Year : {frame_time}\")\n  \nanimate(p,nframes=7,fps=1)"
  },
  {
    "objectID": "tidytuesday/2019/Week_14/index.html#monthly-pedestrian-count-for-everyday-with-am-or-pm-when-true",
    "href": "tidytuesday/2019/Week_14/index.html#monthly-pedestrian-count-for-everyday-with-am-or-pm-when-true",
    "title": "Week 14 : Seattle Bikes",
    "section": "Monthly Pedestrian Count for Everyday with AM or PM when TRUE",
    "text": "Monthly Pedestrian Count for Everyday with AM or PM when TRUE\n\np&lt;-bike_traffic %&gt;%\n      cSplit(\"date\",sep=\" \") %&gt;%\n      rename(\"DMY\"=\"date_1\") %&gt;%\n      rename(\"HMS\"=\"date_2\") %&gt;%\n      rename(\"MorE\"=\"date_3\") %&gt;%\n      mutate(DMY=mdy(DMY)) %&gt;%\n      mutate(Year=year(DMY)) %&gt;%\n      mutate(Month=month(DMY)) %&gt;%\n      mutate(Day=day(DMY)) %&gt;%\n      group_by(MorE,Year,Month,Day) %&gt;%\n      subset(ped_count==TRUE) %&gt;%\n      summarise(Average=summary.factor(ped_count)) %&gt;%\nggplot(.,aes(y=Average,x=factor(Month),color=MorE))+\n      geom_jitter()+transition_time(Year)+\n      ease_aes(\"linear\")+\n      xlab(\"Month\")+ylab(\"Pedestrian Count\")+\n      scale_y_continuous(labels=seq(0,22.5,2.5),breaks=seq(0,22.5,2.5))+\n      ggtitle(\"Pedestrian Count changing with Time\",\n              subtitle = \"Year : {frame_time}\")\n  \nanimate(p,nframes=7,fps=1)"
  },
  {
    "objectID": "tidytuesday/2019/Week_14/index.html#monthly-pedestrian-count-for-everyday-with-am-or-pm-when-false",
    "href": "tidytuesday/2019/Week_14/index.html#monthly-pedestrian-count-for-everyday-with-am-or-pm-when-false",
    "title": "Week 14 : Seattle Bikes",
    "section": "Monthly Pedestrian Count for Everyday with AM or PM when FALSE",
    "text": "Monthly Pedestrian Count for Everyday with AM or PM when FALSE\n\np&lt;-bike_traffic %&gt;%\n      cSplit(\"date\",sep=\" \") %&gt;%\n      rename(\"DMY\"=\"date_1\") %&gt;%\n      rename(\"HMS\"=\"date_2\") %&gt;%\n      rename(\"MorE\"=\"date_3\") %&gt;%\n      mutate(DMY=mdy(DMY)) %&gt;%\n      mutate(Year=year(DMY)) %&gt;%\n      mutate(Month=month(DMY)) %&gt;%\n      mutate(Day=day(DMY)) %&gt;%\n      group_by(MorE,Year,Month,Day) %&gt;%\n      subset(ped_count==FALSE) %&gt;%\n      summarise(Average=summary.factor(ped_count)) %&gt;%\nggplot(.,aes(y=Average,x=factor(Month),color=MorE))+\n      geom_jitter()+transition_time(Year)+\n      ease_aes(\"linear\")+\n      xlab(\"Month\")+ylab(\"Pedestrian Count\")+\n      scale_y_continuous(labels=seq(0,75,5),breaks=seq(0,75,5))+\n      ggtitle(\"Pedestrian Count changing with Time\",\n              subtitle = \"Year : {frame_time}\")\n  \nanimate(p,nframes=7,fps=1)"
  },
  {
    "objectID": "tidytuesday/2019/Week_14/index.html#day-by-pedestrian-count-for-everyday-with-am-or-pm-when-true",
    "href": "tidytuesday/2019/Week_14/index.html#day-by-pedestrian-count-for-everyday-with-am-or-pm-when-true",
    "title": "Week 14 : Seattle Bikes",
    "section": "Day by Pedestrian Count for Everyday with AM or PM when TRUE",
    "text": "Day by Pedestrian Count for Everyday with AM or PM when TRUE\n\np&lt;-bike_traffic %&gt;%\n      cSplit(\"date\",sep=\" \") %&gt;%\n      rename(\"DMY\"=\"date_1\") %&gt;%\n      rename(\"HMS\"=\"date_2\") %&gt;%\n      rename(\"MorE\"=\"date_3\") %&gt;%\n      mutate(DMY=mdy(DMY)) %&gt;%\n      mutate(HMS=hms(HMS)) %&gt;%\n      mutate(Day=day(DMY)) %&gt;%\n      mutate(Year=year(DMY)) %&gt;%\n      mutate(Month=month(DMY)) %&gt;%\n      group_by(MorE,Year,Month,Day) %&gt;%\n      subset(ped_count==TRUE) %&gt;%\n      summarise(Average=summary.factor(ped_count)) %&gt;%\nggplot(.,aes(y=Average,x=factor(Day),color=MorE))+\n      geom_jitter()+transition_time(Year)+\n      ease_aes(\"linear\")+coord_flip()+\n      xlab(\"Day\")+ylab(\"Pedestrian Count\")+\n      scale_y_continuous(labels=seq(0,22.5,2.5),breaks=seq(0,22.5,2.5))+\n      ggtitle(\"Pedestrian Count changing with Time\",\n              subtitle = \"Year : {frame_time}\")\n  \nanimate(p,nframes=7,fps=1)"
  },
  {
    "objectID": "tidytuesday/2019/Week_14/index.html#day-by-pedestrian-count-for-everyday-with-am-or-pm-when-false",
    "href": "tidytuesday/2019/Week_14/index.html#day-by-pedestrian-count-for-everyday-with-am-or-pm-when-false",
    "title": "Week 14 : Seattle Bikes",
    "section": "Day by Pedestrian Count for Everyday with AM or PM when FALSE",
    "text": "Day by Pedestrian Count for Everyday with AM or PM when FALSE\n\np&lt;-bike_traffic %&gt;%\n      cSplit(\"date\",sep=\" \") %&gt;%\n      rename(\"DMY\"=\"date_1\") %&gt;%\n      rename(\"HMS\"=\"date_2\") %&gt;%\n      rename(\"MorE\"=\"date_3\") %&gt;%\n      mutate(DMY=mdy(DMY)) %&gt;%\n      mutate(HMS=hms(HMS)) %&gt;%\n      mutate(Day=day(DMY)) %&gt;%\n      mutate(Year=year(DMY)) %&gt;%\n      mutate(Month=month(DMY)) %&gt;%\n      group_by(MorE,Year,Month,Day) %&gt;%\n      subset(ped_count==FALSE) %&gt;%\n      summarise(Average=summary.factor(ped_count)) %&gt;%\nggplot(.,aes(y=Average,x=factor(Day),color=MorE))+\n      geom_jitter()+transition_time(Year)+\n      ease_aes(\"linear\")+coord_flip()+\n      xlab(\"Day\")+ylab(\"Pedestrian Count\")+\n      scale_y_continuous(labels=seq(0,70,5),breaks=seq(0,70,5))+\n      ggtitle(\"Pedestrian Count changing with Time\",\n              subtitle = \"Year : {frame_time}\")\n  \nanimate(p,nframes=7,fps=1)"
  },
  {
    "objectID": "tidytuesday/2019/Week_14/index.html#hourly-pedestrian-count-for-every-month-with-am-or-pm-when-true",
    "href": "tidytuesday/2019/Week_14/index.html#hourly-pedestrian-count-for-every-month-with-am-or-pm-when-true",
    "title": "Week 14 : Seattle Bikes",
    "section": "Hourly Pedestrian Count for Every Month with AM or PM when TRUE",
    "text": "Hourly Pedestrian Count for Every Month with AM or PM when TRUE\n\np&lt;-bike_traffic %&gt;%\n      cSplit(\"date\",sep=\" \") %&gt;%\n      rename(\"DMY\"=\"date_1\") %&gt;%\n      rename(\"HMS\"=\"date_2\") %&gt;%\n      rename(\"MorE\"=\"date_3\") %&gt;%\n      mutate(DMY=mdy(DMY)) %&gt;%\n      mutate(HMS=hms(HMS)) %&gt;%\n      mutate(Hour=hour(HMS)) %&gt;%\n      mutate(Year=year(DMY)) %&gt;%\n      mutate(Month=month(DMY)) %&gt;%\n      group_by(MorE,Year,Month,Hour) %&gt;%\n      subset(ped_count==TRUE) %&gt;%\n      summarise(Average=summary.factor(ped_count)) %&gt;%\nggplot(.,aes(y=Average,x=factor(Hour),color=MorE))+\n      geom_jitter()+transition_time(Year)+\n      ease_aes(\"linear\")+\n      xlab(\"Hour\")+ylab(\"Pedestrian Count\")+\n      scale_y_continuous(labels=seq(0,60,5),breaks=seq(0,60,5))+\n      ggtitle(\"Pedestrian Count changing with Time\",\n              subtitle = \"Year : {frame_time}\")\n  \nanimate(p,nframes=7,fps=1)"
  },
  {
    "objectID": "tidytuesday/2019/Week_14/index.html#hourly-pedestrian-count-for-every-month-with-am-or-pm-when-false",
    "href": "tidytuesday/2019/Week_14/index.html#hourly-pedestrian-count-for-every-month-with-am-or-pm-when-false",
    "title": "Week 14 : Seattle Bikes",
    "section": "Hourly Pedestrian Count for Every Month with AM or PM when FALSE",
    "text": "Hourly Pedestrian Count for Every Month with AM or PM when FALSE\n\np&lt;-bike_traffic %&gt;%\n      cSplit(\"date\",sep=\" \") %&gt;%\n      rename(\"DMY\"=\"date_1\") %&gt;%\n      rename(\"HMS\"=\"date_2\") %&gt;%\n      rename(\"MorE\"=\"date_3\") %&gt;%\n      mutate(DMY=mdy(DMY)) %&gt;%\n      mutate(HMS=hms(HMS)) %&gt;%\n      mutate(Hour=hour(HMS)) %&gt;%\n      mutate(Year=year(DMY)) %&gt;%\n      mutate(Month=month(DMY)) %&gt;%\n      group_by(MorE,Year,Month,Hour) %&gt;%\n      subset(ped_count==TRUE) %&gt;%\n      summarise(Average=summary.factor(ped_count)) %&gt;%\nggplot(.,aes(y=Average,x=factor(Hour),color=MorE))+\n      geom_jitter()+transition_time(Year)+\n      ease_aes(\"linear\")+\n      xlab(\"Hour\")+ylab(\"Pedestrian Count\")+\n      scale_y_continuous(labels=seq(0,60,5),breaks=seq(0,60,5))+\n      ggtitle(\"Pedestrian Count changing with Time\",\n              subtitle = \"Year : {frame_time}\")\n  \nanimate(p,nframes=7,fps=1)"
  },
  {
    "objectID": "tidytuesday/2019/Week_11/index.html",
    "href": "tidytuesday/2019/Week_11/index.html",
    "title": "Week 11 : Board Games",
    "section": "",
    "text": "# load the packages\nlibrary(readr)\nlibrary(tidyverse)\nlibrary(tidylog)\nlibrary(gganimate)\nlibrary(splitstackshape)\nlibrary(ggthemr)\n\n# load the theme\nggthemr(\"chalk\")\n\n# load the data\nboard_games &lt;- read_csv(\"board_games.csv\")\nBoard Games data-set contains a lot of variables yet i will be focusing on only a few of them here. They are category, year_published, average_rating, users_rated, max_players, max_playtime, min_players and min_playtime. Mostly the plots will identify patterns how over the years variables change with the help of jitter/point plots. Slightly above 10,000 observations are in this data-set\n{{% tweet \"1105389530008317952\" %}}\nGitHub Code"
  },
  {
    "objectID": "tidytuesday/2019/Week_11/index.html#general",
    "href": "tidytuesday/2019/Week_11/index.html#general",
    "title": "Week 11 : Board Games",
    "section": "General",
    "text": "General\n\np&lt;-ggplot(board_games,aes(min_players,min_playtime))+\n          geom_jitter()+transition_time(year_published)+\n          ease_aes(\"linear\")+\n          xlab(\"Minimum No of Players\")+\n          ylab(\"Minimum Playing Time\")+\n          scale_x_continuous(breaks=seq(0,9),labels = seq(0,9))+\n          scale_y_continuous(breaks = seq(0,60000,5000),labels=seq(0,60000,5000))+\n          ggtitle(\"Minimum No of Players vs Minimum Playing Time\",\n                  subtitle =\"Year :{frame_time}\" )\n\nanimate(p,nframes=67,fps=1)"
  },
  {
    "objectID": "tidytuesday/2019/Week_11/index.html#scrutinized",
    "href": "tidytuesday/2019/Week_11/index.html#scrutinized",
    "title": "Week 11 : Board Games",
    "section": "Scrutinized",
    "text": "Scrutinized\n\np&lt;-subset(board_games, min_playtime &lt; 5000) %&gt;%\n   ggplot(.,aes(min_players,min_playtime))+\n          geom_jitter()+transition_time(year_published)+\n          ease_aes(\"linear\")+\n          xlab(\"Minimum No of Players\")+\n          ylab(\"Minimum Playing Time\")+\n          scale_x_continuous(breaks=seq(0,9),labels = seq(0,9))+\n          scale_y_continuous(breaks = seq(0,5000,500),labels=seq(0,5000,500))+\n          ggtitle(\"Minimum No of Players vs Minimum Playing Time\",\n                  subtitle =\"Year :{frame_time}\" )\n\nanimate(p,nframes=67,fps=1)"
  },
  {
    "objectID": "tidytuesday/2019/Week_11/index.html#more-scrutinized",
    "href": "tidytuesday/2019/Week_11/index.html#more-scrutinized",
    "title": "Week 11 : Board Games",
    "section": "More Scrutinized",
    "text": "More Scrutinized\n\np&lt;-subset(board_games, min_playtime &lt; 500) %&gt;%\n   ggplot(.,aes(min_players,min_playtime))+\n          geom_jitter()+transition_time(year_published)+\n          ease_aes(\"linear\")+\n          xlab(\"Minimum No of Players\")+\n          ylab(\"Minimum Playing Time\")+\n          scale_x_continuous(breaks=seq(0,9),labels = seq(0,9))+\n          scale_y_continuous(breaks = seq(0,500,50),labels=seq(0,500,50))+\n          ggtitle(\"Minimum No of Players vs Minimum Playing Time\",\n                  subtitle =\"Year :{frame_time}\" )\n\nanimate(p,nframes=67,fps=1)"
  },
  {
    "objectID": "tidytuesday/2019/Week_11/index.html#general-1",
    "href": "tidytuesday/2019/Week_11/index.html#general-1",
    "title": "Week 11 : Board Games",
    "section": "General",
    "text": "General\n\np&lt;-ggplot(board_games,aes(max_players,max_playtime))+\n          geom_jitter()+transition_time(year_published)+\n          ease_aes(\"linear\")+\n          xlab(\"Maximum No of Players\")+\n          ylab(\"Maximum Playing Time\")+\n          scale_x_continuous(breaks=seq(0,1000,50),labels = seq(0,1000,50))+\n          scale_y_continuous(breaks = seq(0,60000,5000),labels=seq(0,60000,5000))+\n          ggtitle(\"Maximum No of Players vs Maximum Playing Time\",\n                  subtitle =\"Year :{frame_time}\" )\n\nanimate(p,nframes=67,fps=1)"
  },
  {
    "objectID": "tidytuesday/2019/Week_11/index.html#scrutinized-1",
    "href": "tidytuesday/2019/Week_11/index.html#scrutinized-1",
    "title": "Week 11 : Board Games",
    "section": "Scrutinized",
    "text": "Scrutinized\n\np&lt;-subset(board_games,max_players&lt; 125 & max_playtime &lt; 10000) %&gt;%\n   ggplot(.,aes(max_players,max_playtime))+\n          geom_jitter()+transition_time(year_published)+\n          ease_aes(\"linear\")+\n          xlab(\"Maximum No of Players\")+\n          ylab(\"Maximum Playing Time\")+\n          scale_x_continuous(breaks=seq(0,100,10),labels = seq(0,100,10))+\n          scale_y_continuous(breaks = seq(0,6000,500),labels=seq(0,6000,500))+\n          ggtitle(\"Maximum No of Players vs Maximum Playing Time\",\n                  subtitle =\"Year :{frame_time}\" )\n\nanimate(p,nframes=67,fps=1)"
  },
  {
    "objectID": "tidytuesday/2019/Week_11/index.html#more-scrutinized-1",
    "href": "tidytuesday/2019/Week_11/index.html#more-scrutinized-1",
    "title": "Week 11 : Board Games",
    "section": "More Scrutinized",
    "text": "More Scrutinized\n\np&lt;-subset(board_games,max_players&lt; 15 & max_playtime &lt; 1000) %&gt;%\n   ggplot(.,aes(max_players,max_playtime))+\n          geom_jitter()+transition_time(year_published)+\n          ease_aes(\"linear\")+\n          xlab(\"Maximum No of Players\")+\n          ylab(\"Maximum Playing Time\")+\n          scale_x_continuous(breaks=seq(0,15),labels = seq(0,15))+\n          scale_y_continuous(breaks = seq(0,1000,50),labels=seq(0,1000,50))+\n          ggtitle(\"Maximum No of Players vs Maximum Playing Time\",\n                  subtitle =\"Year :{frame_time}\" )\n\nanimate(p,nframes=67,fps=1)"
  },
  {
    "objectID": "tidytuesday/2019/Week_11/index.html#general-2",
    "href": "tidytuesday/2019/Week_11/index.html#general-2",
    "title": "Week 11 : Board Games",
    "section": "General",
    "text": "General\n\np&lt;-ggplot(board_games,aes(max_players,average_rating))+\n          geom_jitter()+transition_time(year_published)+\n          ease_aes(\"linear\")+\n          xlab(\"Maximum No of Players\")+\n          ylab(\"Average Rating\")+\n          scale_x_continuous(breaks=seq(0,1000,50),labels = seq(0,1000,50))+\n          scale_y_continuous(breaks = seq(0,10,.5),labels=seq(0,10,.5))+\n          ggtitle(\"Maximum No of Players vs Average Rating\",\n                  subtitle =\"Year :{frame_time}\" )\n\nanimate(p,nframes=67,fps=1)"
  },
  {
    "objectID": "tidytuesday/2019/Week_11/index.html#scrutinized-2",
    "href": "tidytuesday/2019/Week_11/index.html#scrutinized-2",
    "title": "Week 11 : Board Games",
    "section": "Scrutinized",
    "text": "Scrutinized\n\np&lt;-subset(board_games,max_players &lt;75 ) %&gt;%\n   ggplot(.,aes(max_players,average_rating))+\n          geom_jitter()+transition_time(year_published)+\n          ease_aes(\"linear\")+\n          xlab(\"Maximum No of Players\")+\n          ylab(\"Average Rating\")+\n          scale_x_continuous(breaks=seq(0,75,5),labels = seq(0,75,5))+\n          scale_y_continuous(breaks = seq(0,10,.5),labels=seq(0,10,.5))+\n          ggtitle(\"Maximum No of Players vs Average Rating\",\n                  subtitle =\"Year :{frame_time}\" )\n\nanimate(p,nframes=67,fps=1)"
  },
  {
    "objectID": "tidytuesday/2019/Week_11/index.html#more-scrutinized-2",
    "href": "tidytuesday/2019/Week_11/index.html#more-scrutinized-2",
    "title": "Week 11 : Board Games",
    "section": "More Scrutinized",
    "text": "More Scrutinized\n\np&lt;-subset(board_games,max_players &lt;15 ) %&gt;%\n   ggplot(.,aes(max_players,average_rating))+\n          geom_jitter()+transition_time(year_published)+\n          ease_aes(\"linear\")+\n          xlab(\"Maximum No of Players\")+\n          ylab(\"Average Rating\")+\n          scale_x_continuous(breaks=seq(0,15),labels = seq(0,15))+\n          scale_y_continuous(breaks = seq(0,10,.5),labels=seq(0,10,.5))+\n          ggtitle(\"Maximum No of Players vs Average Rating\",\n                  subtitle =\"Year :{frame_time}\" )\n\nanimate(p,nframes=67,fps=1)"
  },
  {
    "objectID": "tidytuesday/2019/Week_11/index.html#animated",
    "href": "tidytuesday/2019/Week_11/index.html#animated",
    "title": "Week 11 : Board Games",
    "section": "Animated",
    "text": "Animated\n\np&lt;-ggplot(board_games,aes(average_rating,users_rated))+\n          geom_point()+transition_time(year_published)+\n          ease_aes(\"linear\")+\n          ylab(\"Users Rated\")+\n          xlab(\"Average Rating\")+\n          scale_x_continuous(breaks=seq(0,10,.5),labels = seq(0,10,.5))+\n          scale_y_continuous(breaks = seq(0,70000,2500),labels=seq(0,70000,2500))+\n          ggtitle(\"Average Rating vs Users Rated\",\n                  subtitle =\"Year :{frame_time}\" )\n\nanimate(p,nframes=67,fps=1)"
  },
  {
    "objectID": "tidytuesday/2019/Week_11/index.html#not-animated",
    "href": "tidytuesday/2019/Week_11/index.html#not-animated",
    "title": "Week 11 : Board Games",
    "section": "Not Animated",
    "text": "Not Animated\n\nggplot(board_games,aes(average_rating,users_rated,color=year_published))+\n          geom_point(alpha=0.85)+\n          labs(color=\"Year\")+\n          ylab(\"Users Rated\")+\n          xlab(\"Average Rating\")+\n          scale_x_continuous(breaks=seq(0,10,.5),labels = seq(0,10,.5))+\n          scale_y_continuous(breaks = seq(0,70000,2500),labels=seq(0,70000,2500))+\n          ggtitle(\"Average Rating vs Users Rated\",\n                  subtitle =\"Year : 1950 to 2016\" )"
  },
  {
    "objectID": "tidytuesday/2019/Week_11/index.html#general-and-category-1",
    "href": "tidytuesday/2019/Week_11/index.html#general-and-category-1",
    "title": "Week 11 : Board Games",
    "section": "General and Category 1",
    "text": "General and Category 1\n\np&lt;-board_games %&gt;%\n   select(year_published,category,average_rating) %&gt;%\n   cSplit(\"category\",sep=\",\") %&gt;%\n   gather(Groups,category,\"category_01\",\"category_02\",\"category_03\",\"category_04\") %&gt;%\n   select(year_published,Groups,category,average_rating) %&gt;%\n   subset(Groups==\"category_01\" & average_rating &gt; 7)%&gt;%\n   ggplot(.,aes(category,average_rating))+geom_jitter()+\n   coord_flip()+xlab(\"Category\")+ylab(\"Average Rating\")+\n   transition_time(year_published)+ease_aes(\"linear\")+\n   ggtitle(\"Average Rating for First Category\",\n           subtitle = \"Year: {floor(frame_time)}\")\n\nanimate(p,fps=1,nframes=54)"
  },
  {
    "objectID": "tidytuesday/2019/Week_11/index.html#general-and-category-2",
    "href": "tidytuesday/2019/Week_11/index.html#general-and-category-2",
    "title": "Week 11 : Board Games",
    "section": "General and Category 2",
    "text": "General and Category 2\n\np&lt;-board_games %&gt;%\n   select(year_published,category,average_rating) %&gt;%\n   cSplit(\"category\",sep=\",\") %&gt;%\n   gather(Groups,category,\"category_01\",\"category_02\",\"category_03\",\"category_04\") %&gt;%\n   select(year_published,Groups,category,average_rating) %&gt;%\n   subset(Groups==\"category_02\" & average_rating &gt;7)%&gt;%\n   ggplot(.,aes(category,average_rating))+geom_jitter()+\n   coord_flip()+xlab(\"Category\")+ylab(\"Average Rating\")+\n   transition_time(year_published)+ease_aes(\"linear\")+\n   ggtitle(\"Average Rating for Second Category\",\n           subtitle = \"Year: {floor(frame_time)}\")\n\nanimate(p,fps=1,nframes=54)\n\n\n\n\nTHANK YOU"
  },
  {
    "objectID": "tidytuesday/2019/Week_1/index.html",
    "href": "tidytuesday/2019/Week_1/index.html",
    "title": "2019 Week 1 : #TidyTuesday Tweets",
    "section": "",
    "text": "# load the necessary packages\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(kableExtra)\nlibrary(ggthemr)\n\n#load the ggthemr\nggthemr(\"flat dark\")\n\n# load the data set\ntidytuesday_tweets&lt;-readRDS(\"tidytuesday_tweets.rds\")"
  },
  {
    "objectID": "tidytuesday/2019/Week_1/index.html#earliest-tweet",
    "href": "tidytuesday/2019/Week_1/index.html#earliest-tweet",
    "title": "2019 Week 1 : #TidyTuesday Tweets",
    "section": "Earliest Tweet",
    "text": "Earliest Tweet\nThe first tweet is on April 2nd and it has 156 favorites and 64 retweets, where the tweet is from Thomas Mock and the next 3 tweets are also from him.\n\ntidytuesday_tweets[order(tidytuesday_tweets$created_at),c(3,4,13,14,71)] %&gt;%\n  head(5) %&gt;%\n  kable()  %&gt;%\n  kable_styling()\n\n\n\ncreated_at\nscreen_name\nfavorite_count\nretweet_count\nname\n\n\n\n2018-04-02 21:35:08\nthomas_mock\n156\n64\nThomas Mock\n\n\n2018-04-02 21:35:10\nthomas_mock\n6\n0\nThomas Mock\n\n\n2018-04-02 21:35:11\nthomas_mock\n4\n0\nThomas Mock\n\n\n2018-04-02 23:31:11\nthomas_mock\n2\n0\nThomas Mock\n\n\n2018-04-03 00:25:51\numairdurrani87\n6\n2\nUmair Durrani"
  },
  {
    "objectID": "tidytuesday/2019/Week_1/index.html#any-verified-profiles",
    "href": "tidytuesday/2019/Week_1/index.html#any-verified-profiles",
    "title": "2019 Week 1 : #TidyTuesday Tweets",
    "section": "Any Verified Profiles ?",
    "text": "Any Verified Profiles ?\nThere are only 3 verified profiles where Hadley Wickham has the highest amount of followers with 76469, where that tweet has 61 favorites but no retweets. Other profiles are Civis Analytics and grspur, but both of them have friends above 600 counts, but Hadley Wickham friends count is close to 290.\n\nsubset(tidytuesday_tweets[,c(4,13,14,76,77,82)],verified==TRUE) %&gt;%\n  kable() %&gt;%\n  kable_styling()\n\n\n\nscreen_name\nfavorite_count\nretweet_count\nfollowers_count\nfriends_count\nverified\n\n\n\nCivisAnalytics\n3\n1\n6880\n658\nTRUE\n\n\nhadleywickham\n61\n0\n76469\n288\nTRUE\n\n\ngrspur\n14\n6\n857\n623\nTRUE"
  },
  {
    "objectID": "tidytuesday/2019/Week_1/index.html#source-of-tweets",
    "href": "tidytuesday/2019/Week_1/index.html#source-of-tweets",
    "title": "2019 Week 1 : #TidyTuesday Tweets",
    "section": "Source of Tweets",
    "text": "Source of Tweets\nClose to 1050 tweets are done by the web client and other clients such as Android and Iphone have tweet counts of respectively 106 and 233. Other sources include oddly Instagram, Facebook, WordPress and LinkedIn, which I am naming because of their popularity.\n\nggplot(tidytuesday_tweets,aes(fct_infreq(source)))+\n  geom_bar()+coord_flip()+\n  geom_text(stat = \"count\",aes(label=..count..),hjust=-0.25)+\n  ylab(\"Frequency\")+xlab(\"Types of Sources\")+\n  ggtitle(\"Source of Tweets\")"
  },
  {
    "objectID": "tidytuesday/2019/Week_1/index.html#tweets-per-month",
    "href": "tidytuesday/2019/Week_1/index.html#tweets-per-month",
    "title": "2019 Week 1 : #TidyTuesday Tweets",
    "section": "Tweets Per Month",
    "text": "Tweets Per Month\nBeginning of #TidyTuesday we have 293 tweets on the month of April. Even though over the next months the number of tweets are decreasing this is not the case in October. Lowest number of tweets are recorded in September with 115 tweets.\n\nggplot(tidytuesday_tweets,\n       aes(x=month(tidytuesday_tweets$created_at)))+\n  geom_bar()+\n  geom_text(stat = \"count\",aes(label=..count..),vjust=-0.15)+\n  scale_x_continuous(breaks = seq(1,12),labels = seq(1,12))+\n  ylab(\"Frequency\")+ xlab(\"Months\")+\n  ggtitle(\"Tweet Counts By Month\")"
  },
  {
    "objectID": "tidytuesday/2019/Week_1/index.html#most-tweets-by-screen-name",
    "href": "tidytuesday/2019/Week_1/index.html#most-tweets-by-screen-name",
    "title": "2019 Week 1 : #TidyTuesday Tweets",
    "section": "Most Tweets By Screen Name",
    "text": "Most Tweets By Screen Name\nThere are 30 twitter users if we consider the accounts that have tweeted more than or equal to 10 tweets under the hashtag “TidyTuesday”. Thomas Mock has tweeted most which is 172 including retweets, and the second place goes to R4DScommunity with 92 tweets. All the other users have individually less than 40 tweets.\n\ntidytuesday_tweets %&gt;%\n  group_by(screen_name) %&gt;%\n  filter(n() &gt;= 10) %&gt;%\nggplot(aes(x=fct_infreq(screen_name)))+\n  geom_bar()+ coord_flip()+\n  geom_text(stat = \"count\",aes(label=..count..),hjust=-0.15)+\n  ylab(\"Frequency\")+ xlab(\"Screen Name\")+\n  ggtitle(\"Screen Name with Most Tweets\")\n\n\n\n\nMost Tweets By Screen Name and their Source\nFor the same plot if we consider the source for the tweets, it is clear that only seven sources were used. Mostly all of these users are using the web client, but some are using the iPhone as well. R4DS community does more tweeting through iPhone than TweetDeck. TweetDeck is a simple way of handling multiple twitter accounts at the same time. Tidyyourworld account only uses Android and WeAreRLadies uses only TweetDeck.\n\ntidytuesday_tweets %&gt;%\n  group_by(screen_name) %&gt;%\n  filter(n() &gt;= 10) %&gt;%\nggplot(aes(x=fct_infreq(screen_name),fill=source))+\n  geom_bar(position = \"stack\",stat=\"count\")+ \n  coord_flip()+\n  geom_text(stat = \"count\",aes(label=..count..),hjust=1,\n            position = position_stack())+\n  ylab(\"Frequency\")+ xlab(\"Screen Name\")+\n  ggtitle(\"Screen Name with Most Tweets and their Source\")\n\n\n\n\nMost Tweets By Screen Name with their Retweet Counts\nOf the Top 30 users with most amount of tweets the highest amount of retweets is to a tweet from WeAreRLadies and it is 95. There are more outliers from Thomas Mock. and the highest range is to the user drob. Most from this top 30 users have the range between 0 and 10.\n\ntidytuesday_tweets[,c(\"screen_name\",\"retweet_count\")] %&gt;%\n  group_by(screen_name) %&gt;%\n  filter(n() &gt;= 10) %&gt;%\nggplot(.,aes(x=fct_infreq(screen_name),y=retweet_count))+\n  geom_boxplot()+ coord_flip()+\n  scale_y_continuous(breaks = seq(0,100,5),labels = seq(0,100,5))+\n  ylab(\"Retweets\")+ xlab(\"Screen Name\")+\n  ggtitle(\"Screen Name with Most Tweets and their Retweets Count\")\n\n\n\n\nMost Tweets By Screen Name with their Favorite Counts\nSimilarly Thomas Mock has more outliers, and the highest range is to the user drob. Second place for outliers goes to R4DScommunity user. Close to 500 favorites are counted to a tweet by drob and second place is to a tweet by WeAreRladies with favorite counts slightly above 450.\n\ntidytuesday_tweets[,c(\"screen_name\",\"favorite_count\")] %&gt;%\n  group_by(screen_name) %&gt;%\n  filter(n() &gt;= 10) %&gt;%\nggplot(.,aes(x=fct_infreq(screen_name),y=favorite_count))+\n  geom_boxplot()+ coord_flip()+\n   scale_y_continuous(breaks = seq(0,500,25),labels = seq(0,500,25))+\n  ylab(\"Favourites Count\")+ xlab(\"Screen Name\")+\n  ggtitle(\"Screen Name with Most Tweets and their Favourties Count\")"
  },
  {
    "objectID": "tidytuesday/2019/Week_1/index.html#relationship-between-favorite-counts-vs-retweet-counts",
    "href": "tidytuesday/2019/Week_1/index.html#relationship-between-favorite-counts-vs-retweet-counts",
    "title": "2019 Week 1 : #TidyTuesday Tweets",
    "section": "Relationship between Favorite Counts vs Retweet Counts ?",
    "text": "Relationship between Favorite Counts vs Retweet Counts ?\nVery clear positive correlation. Y scale ranges from 0 to 500, where x scale range is from 0 to 100 and most of the data points are centered around the range of 0 to 12 retweets and 0 to 60 Favorites. A Few data data points are out of the above mentioned range.\n\nggplot(tidytuesday_tweets, \n       aes(x=retweet_count,y=favorite_count))+\n  geom_point()+geom_smooth()+\n  scale_x_continuous(breaks =seq(0,100,2) ,labels =seq(0,100,2))+\n  scale_y_continuous(breaks =seq(0,500,10),labels =seq(0,500,10))+\n  xlab(\"Retweets\")+ylab(\"Likes\")+\n  ggtitle(\"Retweets Versus Likes\")"
  },
  {
    "objectID": "tidytuesday/2019/Week_1/index.html#relationship-between-followers-count-vs-friends-count",
    "href": "tidytuesday/2019/Week_1/index.html#relationship-between-followers-count-vs-friends-count",
    "title": "2019 Week 1 : #TidyTuesday Tweets",
    "section": "Relationship between Followers Count vs Friends Count ?",
    "text": "Relationship between Followers Count vs Friends Count ?\nHere I have considered only twitter profiles which has followers count less than 5000 with friends count also less than 5000. The reason is to explain the relationship more clearly. Clearly most of the twitter profiles are having followers less than 2000 with friends also less than 2000. Clearly there are some profiles with Followers count above 1000 but friends count less than 1000. Even though there are few profiles with less than 1000 followers but more than 1000 Friends.\n\nggplot(subset(tidytuesday_tweets,\n       followers_count &lt; 5000 & friends_count &lt; 5000), \n       aes(x=followers_count,y=friends_count))+\n  geom_point()+geom_smooth()+\n  scale_x_continuous(breaks =seq(0,5000,250),labels =seq(0,5000,250))+\n  scale_y_continuous(breaks =seq(0,5000,250),labels =seq(0,5000,250))+\n  xlab(\"Followers Count\")+ylab(\"Friends Count\")+\n  ggtitle(\"Followers Count Versus Friends Count\")"
  },
  {
    "objectID": "tidytuesday/2018/Week_37/index.html",
    "href": "tidytuesday/2018/Week_37/index.html",
    "title": "Week 37: NYC Restaurants",
    "section": "",
    "text": "# load the packages\nlibrary(readr)\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(ggthemr)\nlibrary(lubridate)\nlibrary(stringr)\nlibrary(kableExtra)\n\n#using theme\nggthemr(\"fresh\")\n\n#load data\nNYC&lt;-read_csv(\"nyc_restaurants.csv\", \n    col_types = cols(inspection_date = \n                       col_date(format = \"%m/%d/%Y\")))\nattach(NYC)\nData set is completed with more than 300000 records and several important variables such as inspection date, violation code, critical flag, score and violation description. In this article I will mainly focus on Inspection Type, boro, Inspection year, cuisine type and Critical Flag.\n{{% tweet \"1073204074579968000\" %}}\nGitHub Code\nFirst see what type of inspections have occurred."
  },
  {
    "objectID": "tidytuesday/2018/Week_37/index.html#cycle-inspection",
    "href": "tidytuesday/2018/Week_37/index.html#cycle-inspection",
    "title": "Week 37: NYC Restaurants",
    "section": "Cycle Inspection",
    "text": "Cycle Inspection\nInitial Inspection is always high for all the years while assigning Critical. The years 2012, 2013 and 2014 has very low amount of counts while the succeeding years have increasing counts. Initial inspection over the years from 2015 is increasing for the gap between Critical and Not Critical. In 2015 the above gap is close to 4000 but by 2018 it has increased to 10000. If we consider Re-inspection the gap for Critical and Not Critical is close 1000 in year 2015 but by year 2018 it is 4000. Not applicable is increasing over the years for both initial inspection and re-inspection, even though the amounts are in hundreds.\n\n# subsetting data \n# Specific insepction type, critical flag and year with bar plot \nsubset(NYC,inspection_type==\"Cycle Inspection / Initial Inspection\" |\n           inspection_type==\"Cycle Inspection / Re-inspection\") %&gt;%\nggplot(.,mapping=aes(x=str_wrap(inspection_type,8),fill=critical_flag))+\n      geom_bar(position = \"dodge\",stat = \"count\")+\n      facet_wrap(~year(inspection_date)) +\n      xlab(\"Cycle Inspection\")+\n      ylab(\"Frequency\")+\n      ggtitle(\"Cycle Inspection over the years for Critical Flag\")+\n      labs(fill=\"Critical Flag\")+\n       geom_text(stat = \"count\",aes(label=..count..),\n                 position = position_dodge(width = 1), vjust = -0.05)"
  },
  {
    "objectID": "tidytuesday/2018/Week_37/index.html#pre-permit-operational",
    "href": "tidytuesday/2018/Week_37/index.html#pre-permit-operational",
    "title": "Week 37: NYC Restaurants",
    "section": "Pre-permit Operational",
    "text": "Pre-permit Operational\nSecond most considered type of inspection is Pre-permit Operational, where it begins from 2014. Even though in year 2014 the amounts for initial inspection and re-inspection are less than 20 in all three critical flag categories. Considering the gap between Critical and Not Critical for initial inspection over the years there is a clear increase. Where as in year 2015 the gap is slightly less than 400, next year it is close to 1000, but by year 2018 this gap is above than 2000.\nIn re-inspection for the year 2015 the gap is almost 150, yet with more inspections by 2018 the gap increases to 600. For, Not Applicable the counts do not have a clear increasing or decreasing pattern over the years.\n\n# subsetting data \n# Specific insepction type, critical flag and year with bar plot \nsubset(NYC,inspection_type==\"Pre-permit (Operational) / Initial Inspection\" |\n           inspection_type==\"Pre-permit (Operational) / Re-inspection\") %&gt;%\nggplot(.,aes(x=str_wrap(inspection_type,8),fill=critical_flag))+\n       geom_bar(position = \"dodge\",stat = \"count\")+\n      facet_wrap(~year(inspection_date)) +\n      xlab(\"Pre-permit (Operational)\")+\n      ylab(\"Frequency\")+\n      ggtitle(\"Pre-permit (Operational) over the years for Critical Flag\")+\n      labs(fill=\"Critical Flag\")+\n      facet_wrap(~year(inspection_date)) +\n       geom_text(stat = \"count\",aes(label=..count..),\n                 position = position_dodge(width = 1), vjust = -0.05)"
  },
  {
    "objectID": "tidytuesday/2018/Week_37/index.html#administrative-miscellaneous",
    "href": "tidytuesday/2018/Week_37/index.html#administrative-miscellaneous",
    "title": "Week 37: NYC Restaurants",
    "section": "Administrative Miscellaneous",
    "text": "Administrative Miscellaneous\nVery odd bar plot here than previous two plots for inspection types. There is no Critical type restaurants in the Administrative Miscellaneous inspection type. The counts begin from year 2014 but the amounts are less than 10. Clearly for initial inspections over the years from 2015 to 2018 the count for Not Applicable are increasing, but this is not the case for Not Critical.\nIn year 2015 the amount for the type Not Critical for initial inspection was 528, but in years 2016 and 2018 the amounts increased respectively to 1038 and 1486. Even though the amount decreased to 1015 in year 2017. The same pattern of increase and decrease behavior occurs for Re-inspection type as well.\n\n# subsetting data \n# Specific insepction type, critical flag and year with bar plot \nsubset(NYC,inspection_type==\"Administrative Miscellaneous / Initial Inspection\" |\n           inspection_type==\"Administrative Miscellaneous / Re-inspection\") %&gt;%\nggplot(.,aes(x=str_wrap(inspection_type,8),fill=critical_flag))+\n     geom_bar(position = \"dodge\",stat = \"count\")+\n   facet_wrap(~year(inspection_date)) +\n      xlab(\"Administrative Miscellaneous\")+\n      ylab(\"Frequency\")+\n      ggtitle(\"Administrative Miscellaneous over the years for Critical Flag\")+\n      labs(fill=\"Critical Flag\")+\n      facet_wrap(~year(inspection_date)) +\n       geom_text(stat = \"count\",aes(label=..count..),\n                 position = position_dodge(width = 1), vjust = -0.05)"
  },
  {
    "objectID": "tidytuesday/2018/Week_37/index.html#dunkin-donuts",
    "href": "tidytuesday/2018/Week_37/index.html#dunkin-donuts",
    "title": "Week 37: NYC Restaurants",
    "section": "Dunkin Donuts",
    "text": "Dunkin Donuts\nQueens has close to 1000 observations of Dunkin Donuts and lowest amount goes to Staten Island with 308. Other three boros have counts in between 550 and 710.\n\n# subsetting Dunkin Donuts with boro\nsubset(NYC, dba==\"DUNKIN' DONUTS\") %&gt;%\n  ggplot(.,aes(x=boro))+\n  geom_bar(position = \"dodge\",stat = \"count\")+\n  geom_text(stat = \"count\",aes(label=..count..), vjust = -0.05)+\n  ggtitle(\"How many Dunkin Donuts in a Boro\")+\n  xlab(\"Boro\")+ylab(\"Frequency\")\n\n\n\n\nThere are only 5 cuisine types and prominent ones are American, Donuts and drinks(Cafe/Coffee/Tea). Year 2014 is very low in amounts even for cuisine Donuts, but this is not the case over the next few years and the scores are mostly centered between 5 to 15. Bagels/Pretzels and Jewish/ Kosher have least amount of data where none of scores are above 25. It is safe to to say we have more Critical type data and they are mostly close to the score of 10.\n\n# Dunkin Donuts and scores with critical flag\nsubset(NYC, dba==\"DUNKIN' DONUTS\") %&gt;%\nggplot(.,mapping=aes(y=score,color=critical_flag,x=factor(year(inspection_date))))+\n      geom_jitter(alpha=0.3)+labs(color=\"Critical Flag\")+    \n      ggtitle(\"Dunkin Donuts score changes with Critical Flag for Cuisines\")+\n      xlab(\"Critical Flag\")+ylab(\"Score\")+\n      scale_y_continuous(breaks = seq(0,60,5),labels =seq(0,60,5))+\n      facet_wrap(~cuisine_description)"
  },
  {
    "objectID": "tidytuesday/2018/Week_37/index.html#subway",
    "href": "tidytuesday/2018/Week_37/index.html#subway",
    "title": "Week 37: NYC Restaurants",
    "section": "Subway",
    "text": "Subway\n912 points from Manhattan with the highest count and lowest count goes to Staten Island with 141 counts. Bronx and Brooklyn has counts in between 320 and 365, but Queens boro has an amount of 683.\n\n# subsetting Subway with boro\nsubset(NYC, dba==\"SUBWAY\") %&gt;% \n  ggplot(.,aes(x=boro))+\n  geom_bar(position = \"dodge\",stat = \"count\")+\n  geom_text(stat = \"count\",aes(label=..count..), vjust = -0.05)+\n  ggtitle(\"How many Subways in Boro\")+\n  xlab(\"Boro\")+ylab(\"Frequency\")\n\n\n\n\nSubway has only 5 cuisines which are American, Other, Sandwiches, Sandwiches/Salads/Mixed Buffet and Soups & Sandwiches. In these five categories only Sandwiches has significant amount of data points. Oddly, we can see the year 1900 in the x axis and which means error.\nIn Sandwiches category there are more points which are centered in between 5 to 15 in scores, while most of them are Not Critical. Oddly in 2018 there are Critical data points with scores above 55 in Sandwiches cuisine type.\nFor the Other category there are only 4 observations which are in 2018 and 75% of them are Not Critical. Considering American Cuisines there are points in all 4 years and most of them are less than 25% and Not Critical.\n\n# Subway and scores with critical flag\nsubset(NYC, dba==\"SUBWAY\") %&gt;%\nggplot(.,mapping=aes(y=score,color=critical_flag,x=factor(year(inspection_date))))+\n      geom_jitter(alpha=0.3)+ labs(color=\"Critical Flag\")+    \n      ggtitle(\"Subway score changes with Critical Flag for Cuisines\")+\n      xlab(\"Critical Flag\")+ylab(\"Score\")+\n      scale_y_continuous(breaks = seq(0,80,5),labels =seq(0,80,5))+\n      facet_wrap(~cuisine_description)"
  },
  {
    "objectID": "tidytuesday/2018/Week_37/index.html#mcdonalds",
    "href": "tidytuesday/2018/Week_37/index.html#mcdonalds",
    "title": "Week 37: NYC Restaurants",
    "section": "McDonalds",
    "text": "McDonalds\nClose to 550 data points are from Manhattan boro, but Staten Island boro has points close to 75. Second place of 487 counts goes to Queens boro. Other two boros, which are Bronx and Brooklyn have counts in the range of 320 and 360.\n\n# subsetting McDonalds with boro\nsubset(NYC, dba==\"MCDONALD'S\") %&gt;% \n  ggplot(.,aes(x=boro))+\n  geom_bar(position = \"dodge\",stat = \"count\")+\n  geom_text(stat = \"count\",aes(label=..count..), vjust = -0.05)+\n  ggtitle(\"How many McDonalds in Boro\")+\n  xlab(\"Boro\")+ylab(\"Frequency\")\n\n\n\n\nOnly the cuisines Other and Sandwiches/Salads/Mixed Buffet has data points in the years 2017 and 2018 and these points have an amount less than 15 in both cases. Considering the other two cuisines which are American and Hamburgers, there are more points in the latter than in the former. It should be noted that Hamburgers cuisine has mostly points centered close to the score range of 5 to 10, and these points are mostly Not Critical.\nIn American cuisine for the year 2016 there are 5 points which have scores close to 70, even though in any other year this has not occurred. To be more precise, except those 5 points all the others have scores less than 45 for American Cuisine.\n\n# McDonalds and scores with critical flag\nsubset(NYC, dba==\"MCDONALD'S\") %&gt;%\nggplot(.,mapping=aes(y=score,color=critical_flag,x=factor(year(inspection_date))))+\n      geom_jitter(alpha=0.3)+      \n      labs(color=\"Critical Flag\")+    \n      ggtitle(\"McDonalds score changes with Critical Flag for Cuisines\")+\n      xlab(\"Critical Flag\")+ylab(\"Score\")+\n      scale_y_continuous(breaks = seq(0,80,5),labels =seq(0,80,5))+\n      facet_wrap(~cuisine_description)"
  },
  {
    "objectID": "tidytuesday/2018/Week_37/index.html#starbucks",
    "href": "tidytuesday/2018/Week_37/index.html#starbucks",
    "title": "Week 37: NYC Restaurants",
    "section": "Starbucks",
    "text": "Starbucks\nThere are more than 1100 Starbucks in Manhattan only while other boros have less than 220 and lowest amount goes to Bronx with 31. Second lowest goes to Staten Island with 41.\n\n#subsetting Starbucks with boro\nsubset(NYC, dba==\"STARBUCKS\") %&gt;% \n  ggplot(.,aes(x=boro))+\n  geom_bar(position = \"dodge\",stat = \"count\")+\n  geom_text(stat = \"count\",aes(label=..count..), vjust = -0.05)+\n  ggtitle(\"How many Starbucks in Boro\")+\n  xlab(\"Boro\")+ylab(\"Frequency\")\n\n\n\n\nStarbucks is mainly focused on Cafe/Coffee/Tea rather than cuisines such as Pizza, Sandwiches, American and other. Clearly there are negligible amount of data points in those four categories, except American cuisine.\nIf we focus on Drinks(Coffee/Cafe/Tea), evidently most of them are Not Critical and they are centered around the score of 0 to 10. Even though scores higher than 15 do occur they are spread out widely. This is a common occurrence for all four years, which is from 2015 to 2018.\n\n# Starbucks and scores with critical flag\nsubset(NYC, dba==\"STARBUCKS\") %&gt;%\nggplot(.,mapping=aes(y=score,color=critical_flag,x=factor(year(inspection_date))))+\n      geom_jitter(alpha=0.3)+\n      labs(color=\"Critical Flag\")+    \n      ggtitle(\"Starbucks score changes with Critical Flag for Cuisines\")+\n      xlab(\"Critical Flag\")+ylab(\"Score\")+\n      scale_y_continuous(breaks = seq(0,80,5),labels =seq(0,80,5))+\n      facet_wrap(~cuisine_description)"
  },
  {
    "objectID": "tidytuesday/2018/Week_37/index.html#kennedy-fried-chicken",
    "href": "tidytuesday/2018/Week_37/index.html#kennedy-fried-chicken",
    "title": "Week 37: NYC Restaurants",
    "section": "Kennedy Fried Chicken",
    "text": "Kennedy Fried Chicken\nBronx the has most amount observations to Kennedy Fried Chicken’s with 626 while lowest count of 5 is from Staten Island. Other three boros have counts in the range of 75 to 190.\n\n#subsetting Kennedy Fried Chicken with boro\nsubset(NYC, dba==\"KENNEDY FRIED CHICKEN\") %&gt;% \n  ggplot(.,aes(x=boro))+\n  geom_bar(position = \"dodge\",stat = \"count\")+\n  geom_text(stat = \"count\",aes(label=..count..), vjust = -0.05)+\n  ggtitle(\"How many Kennedy Fried Chicken's in Boro\")+\n  xlab(\"Boro\")+ylab(\"Frequency\")\n\n\n\n\nThere are only four cuisines which are related to Kennedy Fried Chicken, and they are American, Chicken, Hamburgers and other. The categories Hamburgers and Other have very less amount of counts and the year 1900 is also mentioned. For Hamburgers cuisine only the year 2017 has significant amount of data points, where the year 2015 has only one and year 2018 has only two points.\nFurther, the scores for these points are always less than 20 and mostly Critical. Cuisines of Chicken has more data points than American but in both types there is no clear centering of data around a certain score.\n\n# Kennedy Fried Chicken and scores with critical flag\nsubset(NYC, dba==\"KENNEDY FRIED CHICKEN\") %&gt;%\nggplot(.,mapping=aes(y=score,color=critical_flag,x=factor(year(inspection_date))))+\n      geom_jitter(alpha=0.3)+     \n      labs(color=\"Critical Flag\")+    \n      ggtitle(\"Kennedy Fried Chicken score changes with Critical Flag\")+\n      xlab(\"Critical Flag\")+ylab(\"Score\")+\n      scale_y_continuous(breaks = seq(0,80,5),labels =seq(0,80,5))+\n      facet_wrap(~cuisine_description)"
  },
  {
    "objectID": "tidytuesday/2018/Week_35/index.html",
    "href": "tidytuesday/2018/Week_35/index.html",
    "title": "Week 35: Baltimore Bridges",
    "section": "",
    "text": "# loading the packages\nlibrary(readr)\nlibrary(tidyverse)\nlibrary(stringr)\nlibrary(ggthemr)\nlibrary(gganimate)\nlibrary(formattable)\n# load the theme flat dark\nggthemr(\"flat dark\")\n# reading the data\nbridges &lt;- read_csv(\"baltimore_bridges.csv\")\n#View(bridges)\n# naming the columns\nnames(bridges)&lt;-c(\"lat\",\"long\",\"County\",\"Carries\",\"Year Built\",\n                  \"Condition\",\"Average Daily Traffic\",\"Total Improvement\",\n                  \"Month\",\"Year\",\"Owner\",\"Responsibility\",\"Vehicles\")\nattach(bridges)"
  },
  {
    "objectID": "tidytuesday/2018/Week_35/index.html#counties-which-have-bridges-owned-by-state-highway-agency",
    "href": "tidytuesday/2018/Week_35/index.html#counties-which-have-bridges-owned-by-state-highway-agency",
    "title": "Week 35: Baltimore Bridges",
    "section": "Counties which have bridges owned by State Highway Agency",
    "text": "Counties which have bridges owned by State Highway Agency\nClose to 1000 bridges are owned by State Highway Agency, where most of them are in Baltimore County. High amount of bridges are in good condition, further more bridges are in Fair condition and only around 10 bridges in Poor condition.\nConsidering the Average Daily Traffic only one bridge in Poor condition has the amount of close to 110,000, while all the other poor condition bridges have Average Daily Traffic less than 30,000. Counties Anne Arundel and Hartford have no Poor condition bridges at all.\nMost of the bridges are from Baltimore County and around 20 bridges have count of more than 150,000 Average Daily Traffic for both Fair and Good conditions. Hartford and Carroll Counties have their Average Daily Traffic which does not exceed 80,000 at any condition of the bridge.\n\n# jitter plot to State Highway Agency\nggplot(subset(bridges,Owner==\"State Highway Agency\")\n       ,aes(color=Condition,y=`Average Daily Traffic`,x=str_wrap(County,7)))+\n  xlab(\"County\")+\n  ggtitle(\"Condition of Bridges owned by State Highway Agency \\nand their Average Daily Traffic\")+\n  scale_y_continuous(labels =seq(0,230000,10000) ,breaks = seq(0,230000,10000))+\n  transition_states(Condition,transition_length = 2,state_length = 3)+\n  enter_fade()+exit_shrink()+ease_aes(\"back-in\")+geom_jitter()"
  },
  {
    "objectID": "tidytuesday/2018/Week_35/index.html#counties-which-have-bridges-owned-by-county-highway-agency",
    "href": "tidytuesday/2018/Week_35/index.html#counties-which-have-bridges-owned-by-county-highway-agency",
    "title": "Week 35: Baltimore Bridges",
    "section": "Counties which have bridges owned by County Highway Agency",
    "text": "Counties which have bridges owned by County Highway Agency\nCounty Highway Agency owns the second most amount of bridges in this data-set. Therefore using jitter plots we are going to check how the condition of the bridges and counties are explained in the simplest manner.\nLess amount of poor condition bridges in all counties except Anne Roundel County. All bridges owned by County Highway Agency have a limited Average Daily Traffic less than 50,000. Clearly we have more Fair bridges than Good ones. In the Poor condition category only two have Average Daily Traffic more than 20,000, while other two have more than 10 bridges.\nMost of these bridges are in Baltimore County even it is in any one of three conditions. There are few bridges which have values more than 40,000 Average Daily Traffic and they are also in Baltimore County.\nThere are bridges which have Zero Average Daily Traffic. In all three Conditions only Hartford County has bridges which has Average Daily Traffic less than 10,000.\n\n# jitter plot to County Highway Agency\nggplot(subset(bridges,Owner==\"County Highway Agency\")\n       ,aes(color=Condition,y=`Average Daily Traffic`,x=str_wrap(County,7)))+\n  xlab(\"County\")+ geom_jitter()+\n  ggtitle(\"Condition of Bridges owned by County Highway Agency \\nand their Average Daily Traffic\")+\n  scale_y_continuous(labels =seq(0,40000,5000) ,breaks = seq(0,40000,5000))+\n  transition_states(Condition,transition_length = 2,state_length = 3)+\n  enter_fade()+exit_shrink()+ease_aes(\"back-in\")"
  },
  {
    "objectID": "tidytuesday/2018/Week_35/index.html#counties-which-have-bridges-owned-by-state-toll-authority",
    "href": "tidytuesday/2018/Week_35/index.html#counties-which-have-bridges-owned-by-state-toll-authority",
    "title": "Week 35: Baltimore Bridges",
    "section": "Counties which have bridges owned by State Toll Authority",
    "text": "Counties which have bridges owned by State Toll Authority\nThere is only one bridge which is in Poor condition and it is in Baltimore County, while counties Howard and Anne Arundel have no Good condition bridges. Further there is only 3 Fair condition bridges in Howard County while they have values for Average Daily Traffic less than 10,000.\nThe highest Average Daily Traffic is close to 170,000 which are only 4 and are in Good and Fair conditions. Further, Anne Arundel County has only one Good bridge and in Hartford it is six bridges. Only few of the bridges have Average Daily Traffic close to zero.\n\n# jitter plot to State tolll authority\nggplot(subset(bridges,Owner==\"State Toll Authority\")\n       ,aes(color=Condition,y=`Average Daily Traffic`,x=str_wrap(County,7)))+\n  xlab(\"County\")+geom_jitter()+\n  ggtitle(\"Condition of Bridges owned by State Toll Authority \\nand their Average Daily Traffic\")+\n  scale_y_continuous(labels =seq(0,170000,10000) ,breaks = seq(0,170000,10000))+\n  transition_states(Condition,transition_length = 2,state_length = 3)+\n  enter_fade()+exit_shrink()+ease_aes(\"back-in\")"
  },
  {
    "objectID": "tidytuesday/2018/Week_35/index.html#most-amount-of-bridges-built-based-on-year",
    "href": "tidytuesday/2018/Week_35/index.html#most-amount-of-bridges-built-based-on-year",
    "title": "Week 35: Baltimore Bridges",
    "section": "Most amount of bridges Built based on Year",
    "text": "Most amount of bridges Built based on Year\nYears 1957, 1970, 1975, 1991, 1963 and 1961 have the top 6 spots for building more than 50 bridges in those years. If we consider the conditions of Fair and Good only the year 1991 is suitable to be mentioned, while all other years has at-least one Poor condition bridge. Further There are more Poor condition bridges in 1961 than in 1957. While all Poor condition bridges has Average Daily Traffic less than 50,000.\nFinally, there are only few bridges which have Average Daily Traffic above 100,000 and only 3 are in Good condition. There are Bridges which can have Average Daily Traffic close to zero in all 6 years and all conditions.\n\n# jitter plot to years based on Most amount of bridges built\nggplot(subset(bridges,`Year Built`==\"1957\" | `Year Built`==\"1970\" | \n                `Year Built`==\"1975\" | `Year Built`==\"1991\" |\n                `Year Built`==\"1963\" | `Year Built`==\"1961\")\n       ,aes(color=Condition,y=`Average Daily Traffic`,x=factor(`Year Built`)))+\n  xlab(\"Year Built\")+ylab(\"Average Daily Traffic\")+\n  ggtitle(\"Most amount of Bridges built based on Years \\nand their Conditions\")+\n  geom_jitter()+legend_bottom()+\n  transition_states(Condition,transition_length = 2,state_length = 3)+\n  enter_fade()+exit_shrink()+ease_aes(\"back-in\")"
  },
  {
    "objectID": "tidytuesday/2018/Week_35/index.html#average-traffic-less-than-or-equal-to-100000-for-counties-with-bridge-condition",
    "href": "tidytuesday/2018/Week_35/index.html#average-traffic-less-than-or-equal-to-100000-for-counties-with-bridge-condition",
    "title": "Week 35: Baltimore Bridges",
    "section": "Average Traffic Less than or equal to 100,000 for Counties with Bridge Condition",
    "text": "Average Traffic Less than or equal to 100,000 for Counties with Bridge Condition\nWhile obtaining summary for county variable there is one issue because there are two observations which say “Baltimore city” than “Baltimore City” and I don’t want to change them.\nIf we focus on Average Daily Traffic less than or equal to 100,000 based on County and Condition. It is clear that Poor condition bridges are part of this criteria and mostly Average Daily Traffic is less than 5000 for Counties Howard, Hartford and Carroll. While Baltimore County has highest amount up-to 75,000, but Baltimore County has highest amount close to 40,000 for Average Daily Traffic. Finally Anne Arundel County has only one Poor condition bridge which has Average Daily Traffic close to zero.\nWe can see that there are more Fair Condition bridges than Good ones. In Baltimore County most of the Fair condition bridges have Average Daily Traffic less than 15000. Similarly Carroll county and Hartford county also behave under such criteria. But for Good condition bridges this is not the case where there is no certain strong dense region as similar to Fair condition bridges.\nPreviously when we looked into county we did not see Baltimore City often as a factor, but here that is not the case.\n\n# jitter plot to average daily Traffic less than or equal 1000000\nggplot(subset(bridges,`Average Daily Traffic`&lt;=100000 & County!=\"Baltimore city\"),\n       aes(x=County,y=`Average Daily Traffic`,color=Condition))+\n  xlab(\"County\")+ylab(\"Averag Daily Traffic\")+\n  ggtitle(\"Average Daily Traffic Less than 100,000 \\nFor Counties\")+\n  scale_y_continuous(labels = seq(0,100000,5000),breaks = seq(0,100000,5000))+\n  theme(axis.text.x = element_text(angle = -90))+coord_flip()+ geom_jitter()+\n  transition_states(Condition,transition_length = 2,state_length = 3)+\n  enter_fade()+exit_shrink()+ease_aes(\"back-in\")"
  },
  {
    "objectID": "tidytuesday/2018/Week_35/index.html#average-traffic-more-than-100000-for-counties-with-bridge-condition",
    "href": "tidytuesday/2018/Week_35/index.html#average-traffic-more-than-100000-for-counties-with-bridge-condition",
    "title": "Week 35: Baltimore Bridges",
    "section": "Average Traffic More than 100,000 for Counties with Bridge Condition",
    "text": "Average Traffic More than 100,000 for Counties with Bridge Condition\nThis Jitter plot is completely different than previous one, because there are no clear dense regions for any counties and conditions of the bridge. There is only one Poor condition bridge in Baltimore County where the Average Daily Traffic is close to 115,000. In Fair condition bridges also Baltimore County holds the most, while they are slightly dense in the region of 175,000 to 190,000. while for Howard County similar density occurs between 190,000 to 205,000. Bridges in Good condition have more higher values in Baltimore County than Anne Arundel County.\n\n# jitter plot to average daily Traffic more than  1000000\nggplot(subset(bridges, `Average Daily Traffic` &gt; 100000 & County != \"Baltimore city\"),\n       aes(x=County,y=`Average Daily Traffic`,color=Condition))+\n  xlab(\"County\")+ylab(\"Average Daily Traffic\")+\n  ggtitle(\"Average Daily Traffic More than 100,000 \\nFor Counties\")+\n  scale_y_continuous(labels=seq(100000,230000,5000),breaks=seq(100000,230000,5000))+\n  coord_flip()+ theme(axis.text.x = element_text(angle = -90))+\n  geom_jitter()+transition_states(Condition,transition_length = 2,state_length = 3)+\n  enter_fade()+exit_shrink()+ease_aes(\"back-in\")"
  },
  {
    "objectID": "tidytuesday/2018/Week_35/index.html#improvement-and-bridge-conditions-with-counties",
    "href": "tidytuesday/2018/Week_35/index.html#improvement-and-bridge-conditions-with-counties",
    "title": "Week 35: Baltimore Bridges",
    "section": "Improvement and Bridge Conditions with Counties",
    "text": "Improvement and Bridge Conditions with Counties\nIn the variable of Total Improvement there are 1438 missing values, 42 values are zero and the rest are actual values. I am going to look at Total Improvement in two tables. First table will include where Bridges have Total Improvement higher than 9,999,000 USD and less than 30,000,000 USD. Second table is for Bridges which have Total Improvement higher than or equal to 30,000,000 USD.\nFurther to make these tables interesting I will be using the package formattable, and colors and tiles for numerical values. In the first table there are 7 bridges while only Anne Arundel County holds 3 and Baltimore City holds 4. One bridge is from 1953, and others are from the period of 1977 to 1983. Conditions of these bridges are mostly Fair and two bridges are in Good condition. Lowest Average Daily Traffic is 11760, while highest is 124193, where both bridges are in Fair Condition, and the amount spent on them for Total Improvement are respectively 18,163,000 USD and 16,264,000 USD. The bridge with Highest amount of Average Daily traffic is built in 1953.\n\n# removing unnecessary columns and setting restriction to \n# Total Improvement\nTop10&lt;-subset(bridges[,c(-1,-2,-9,-10,-11,-12,-13)], \n              `Total Improvement` &gt; 9999 & `Total Improvement` &lt; 30000)\n\n# setting colours\ncustomRed0 = \"#FF8080\"\ncustomRed = \"#7F0000\"\n\ncustomyellow0 = \"#FFFF80\"\ncustomyellow = \"#BFBF00\"\n\ncustomblue0 = \"#6060BF\"\ncustomblue =  \"#00007F\"\n\n# creating the table for above data set\nformattable(Top10,align=c(\"l\",\"l\",\"c\",\"c\",\"c\",\"c\"),\n            list(\n              County =formatter(\"span\",style= ~style(color=\"grey\")),\n            `Total Improvement`=color_tile(customblue0,customblue),\n            `Average Daily Traffic`=color_tile(customyellow0,customyellow),\n            `Year Built`=color_tile(customRed0,customRed)\n            ))\n\n\n\n\n\nCounty\n\n\nCarries\n\n\nYear Built\n\n\nCondition\n\n\nAverage Daily Traffic\n\n\nTotal Improvement\n\n\n\n\n\nAnne Arundel County\n\n\nMD 2\n\n\n1983\n\n\nFair\n\n\n53221\n\n\n13504\n\n\n\n\nAnne Arundel County\n\n\nUS 50\n\n\n1953\n\n\nFair\n\n\n124193\n\n\n16264\n\n\n\n\nAnne Arundel County\n\n\nUPPER LEVEL ROADWA\n\n\n1977\n\n\nFair\n\n\n11760\n\n\n18163\n\n\n\n\nBaltimore City \n\n\nIS 95 SB\n\n\n1977\n\n\nFair\n\n\n94765\n\n\n15785\n\n\n\n\nBaltimore City \n\n\nIS 95 VIADUCT SB\n\n\n1980\n\n\nGood\n\n\n63650\n\n\n16051\n\n\n\n\nBaltimore City \n\n\nIS 95 VIADUCT NB\n\n\n1980\n\n\nFair\n\n\n52850\n\n\n20484\n\n\n\n\nBaltimore City \n\n\nIS 95 SB\n\n\n1980\n\n\nGood\n\n\n55621\n\n\n20484\n\n\n\n\n\n\nWhen I did try to plot the top ten bridges with most Total improvement values there was one issue, which is the distance between first two values and the next 8 values. Therefore I divided the table into two.\nIn this second table We can see there are two bridges which are from Baltimore City and are built in 1980 and 1971, but the amount spent on Total Improvement is 300,000,000 USD each. But their Average Daily Traffic is respectively 56280 and 30600.\nWhile we have another bridge from Baltimore City and built in 1907, but Total Improvement amount is 35,026,000 USD. Here, the Average Daily Traffic is 3900.\n\n# removing unnecessary columns and setting restriction to \n# Total Improvement\nTop3&lt;-subset(bridges[,c(-1,-2,-9,-10,-11,-12,-13)], `Total Improvement` &gt;= 30000)\n\n# setting colours\ncustomRed0 = \"#FF8080\"\ncustomRed = \"#7F0000\"\n\ncustomyellow0 = \"#FFFF80\"\ncustomyellow = \"#BFBF00\"\n\ncustomblue0 = \"#6060BF\"\ncustomblue =  \"#00007F\"\n\n# creating the table for above data set\nformattable(Top3,align=c(\"l\",\"l\",\"c\",\"c\",\"c\",\"c\"),\n            list(\n              County =formatter(\"span\",style= ~style(color=\"black\")),\n            `Total Improvement`=color_tile(customblue0,customblue),\n            `Average Daily Traffic`=color_tile(customyellow0,customyellow),\n            `Year Built`=color_tile(customRed0,customRed)\n            ))\n\n\n\n\n\nCounty\n\n\nCarries\n\n\nYear Built\n\n\nCondition\n\n\nAverage Daily Traffic\n\n\nTotal Improvement\n\n\n\n\n\nBaltimore City\n\n\nUS 40 EDMONDSON AV\n\n\n1907\n\n\nPoor\n\n\n3900\n\n\n35026\n\n\n\n\nBaltimore City\n\n\nIS 95 VIADUCT SB\n\n\n1980\n\n\nFair\n\n\n56280\n\n\n300000\n\n\n\n\nBaltimore City\n\n\nEASTERN AVENUE\n\n\n1971\n\n\nFair\n\n\n30600\n\n\n300000"
  },
  {
    "objectID": "tidytuesday/2018/Week_33/index.html",
    "href": "tidytuesday/2018/Week_33/index.html",
    "title": "Week 33 : Malaria Deaths",
    "section": "",
    "text": "# load the packages\nlibrary(ggplot2)\nlibrary(ggrepel)\nlibrary(ggthemr)\nlibrary(magrittr)\nlibrary(stringr)\nlibrary(gridExtra)\nlibrary(readr)\nlibrary(gganimate)\n\n# load the theme flat\nggthemr(\"flat\")\n\n#load the data sets\nmalaria_deaths&lt;-read_csv(\"malaria_deaths.csv\")\nmalaria_deaths_age&lt;-read_csv(\"malaria_deaths_age.csv\")\nattach(malaria_deaths)\nattach(malaria_deaths_age)\n\n# disseminating data\nMalaria_deaths_Code_missing&lt;-malaria_deaths[!complete.cases(Code),]\n\nMD_CM_SDI&lt;-subset(Malaria_deaths_Code_missing,Entity == \"High-middle SDI\" | \n                  Entity == \"High SDI\" | Entity == \"Low SDI\" | \n                  Entity ==\"Low-middle SDI\" | Entity ==\"Middle SDI\")\n\nMD_CM_Sahara&lt;-subset(Malaria_deaths_Code_missing,Entity == \"Central Sub-Saharan Africa\" |\n                     Entity == \"Western Sub-Saharan Africa\" | Entity == \"Southern Sub-Saharan Africa\" |\n                     Entity ==\"Eastern Sub-Saharan Africa\" | Entity ==\"Sub-Saharan Africa\")\n\nMD_CM_EU&lt;-subset(Malaria_deaths_Code_missing,Entity == \"Western Europe\" | \n                 Entity == \"Eastern Europe\" | Entity == \"Central Europe\")\n\nMD_CM_GB&lt;-subset(Malaria_deaths_Code_missing,Entity == \"England\" | \n                 Entity == \"Northern Ireland\" | Entity == \"Scotland\" | Entity ==\"Wales\")\n\nMD_CM_LA&lt;-subset(Malaria_deaths_Code_missing,Entity == \"Andean Latin America\" | \n                 Entity == \"North America\" | Entity == \"Southern Latin America\" | \n                 Entity ==\"Tropical Latin America\" | Entity ==\"Central Latin America\"  )\n\nMD_CM_A&lt;-subset(Malaria_deaths_Code_missing,Entity == \"North Africa and Middle East\" | \n                Entity == \"Southeast Asia\" | Entity == \"Australasia\" | \n                Entity ==\"East Asia\" | Entity ==\"Central Asia\" | Entity ==\"Oceania\"| \n                Entity ==\"High-income Asia Pacific\"| Entity ==\"South Asia\" )\n\nMD_CM_C_LA&lt;-subset(Malaria_deaths_Code_missing,Entity == \"Caribbean\" | \n                   Entity == \"Latin America and Caribbean\")\n\n\n# disseminating data\n#Malaria_deaths_age_Code_missing&lt;-malaria_deaths_age[!complete.cases(code),]\n\n#MD_age_CM_SDI&lt;-subset(Malaria_deaths_age_Code_missing,entity == \"High-middle SDI\" | \n#                      entity == \"High SDI\" | entity == \"Low SDI\" |\n#                      entity ==\"Low-middle SDI\" | entity ==\"Middle SDI\")\n\n#MD_age_CM_Sahara&lt;-subset(Malaria_deaths_age_Code_missing,entity == \"Central Sub-Saharan Africa\" | \n#                         entity == \"Western Sub-Saharan Africa\" | entity == \"Southern Sub-Saharan Africa\" |\n#                         entity ==\"Eastern Sub-Saharan Africa\" | entity ==\"Sub-Saharan Africa\")\n\n#MD_age_CM_EU&lt;-subset(Malaria_deaths_age_Code_missing,entity == \"Western Europe\" | \n#                     entity == \"Eastern Europe\" | entity == \"Central Europe\")\n\n#MD_age_CM_GB&lt;-subset(Malaria_deaths_age_Code_missing,entity == \"England\" | \n#                     entity == \"Northern Ireland\" | entity == \"Scotland\" |\n#                     entity ==\"Wales\" )\n\n#MD_age_CM_LA&lt;-subset(Malaria_deaths_age_Code_missing,entity == \"Andean Latin America\" | \n#                     entity == \"North America\" | entity == \"Southern Latin America\" |\n#                     entity ==\"Tropical Latin America\" | entity ==\"Central Latin America\")\n                 \n#MD_age_CM_A&lt;-subset(Malaria_deaths_age_Code_missing,entity == \"North Africa and Middle East\"| \n#                    entity == \"Southeast Asia\" | entity == \"Australasia\" | entity ==\"East Asia\"| \n#                    entity ==\"Central Asia\" | entity ==\"Oceania\"| \n#                    entity ==\"High-income Asia Pacific\"| entity ==\"South Asia\" )\n\n#MD_age_CM_C_LA&lt;-subset(Malaria_deaths_age_Code_missing,entity == \"Caribbean\" | \n#                       entity == \"Latin America and Caribbean\")\n\nWhat I posted in #TidyTuesday on 13th November 2018.\nBelow is the code and simple analysis I posted in relative to the Malaria data. I only focused on Sri Lanka and Malaria deaths over years. I could not find anything else at that time, but over the days I realized it should be worth to see how death rate counts change with different regions which do not have Code variable assigned for the data sets malaria deaths and malaria deaths of age.\n\n\n{{% tweet \"1062339233967415296\" %}}\n\n\n\nPackages : ggplot2, ggrepel, ggthemr\nTidyTuesday : week 33\nData: malaria_deaths\n\nMalaria_deaths_plot: Plot shows the death rate per 100,000 people in Sri Lanka decreasing rapidly from 1996 to 2003 with a drop of 0.89 to 0.24, and by 2016 it reaches 0. While in 2015 this rate is 0.13. Highest death rate was in 1990 with 0.91.\n\n\n#data subset has been used\n#scales of x and y have been more scrutinized\n#labels have been added\n#x axis have been modified to accomodate the years \nMalaria_deaths_plot&lt;-ggplot(subset.data.frame(malaria_deaths,Code==\"LKA\"),\n                      aes(x=Year,\n                      y= `Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,\n                      label=round(`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,2)))+\n                      geom_point()+geom_line()+geom_text_repel()+\n                      ggtitle(\"Malaria Deaths for Both genders in a rate over the years in Sri Lanka\")+\n                      ylab(\"Deaths - Malaria - Sex: Both -\\n Age: Age-standardized (Rate) (per 100,000 people)\")+\n                      scale_y_continuous(breaks=seq(0.1,1,by=0.1) ,labels=seq(0.1,1,by=0.1))+\n                      scale_x_continuous(breaks=seq(1990,2016),labels =seq(1990,2016))+\n                      theme(axis.text.x = element_text(angle = 90))\n\n# print the plot\nprint(Malaria_deaths_plot)\n\n\n\n# save the plot\nggsave(Malaria_deaths_plot,width = 10,height = 10,dpi=300,\n       filename = \"Malaria_Deaths_Sri Lanka.png\")\n\n\nData: malaria_deaths_age\n\nMalaria_deaths_age_plot: There are five categories in concern,where age category 15-49 has the most counts of in the range of 45-50. Second category is Under 5 close to 35 counts, while third category is 50-69 in-between 20-25. It should be noted that this order is for the year 1990. At the end of year 2015 this is not the case, where the categories and counts are 15-49 (close to 10), 50-69 (less than 10), 70 or order(close to 5), under 5(less than 5) and finally 5-14 (close to 0).\n\n\n#data subset has been used\n#according to age group colors are assigned\n#scales of x and y have been more scrutinized\n#labels have been added\n#x axis have been modified to accomodate the years \nMalaria_deaths_age_plot&lt;-ggplot(subset.data.frame(malaria_deaths_age,code==\"LKA\"),\n                                aes(x=year,y=deaths,color=factor(age_group)))+\n                         geom_point()+geom_line()+\n                         ggtitle(\"Malaria Deaths by age category in Sri Lanka over the years\")+\n                         ylab(\"Deaths Count\")+\n                         scale_x_continuous(breaks=seq(1990,2016),labels =seq(1990,2016))+\n                         scale_y_continuous(breaks=seq(0,60,by=5) ,labels=seq(0,60,by=5))+\n                         theme(axis.text.x = element_text(angle = 90))+\n                         scale_color_discrete(name=\"Age Category\")\n\n# print the plot\nprint(Malaria_deaths_age_plot)\n\n\n\n# save the plot\nggsave(Malaria_deaths_age_plot,width=10,height = 10,dpi = 300,\n       filename = \"Malaria Deaths Sri Lanka by Age.png\")\n\nMalaria Death count rate can be observed by countries or specific regions given in the data set. There are 9 regions in concern and each of these regions have sub parts or collections of countries. Variables included in this data-set are\n\nEntity - Full name of a country or region it is referred.\nCode - 3 letter ISO code for countries, but for regions there are no values.\nYear - Year range from 1990 to 2016.\nDeaths - Malaria - Sex : Both - Age : Age - Standardized (Rate) (per 100,000 people) - Number of people dead for both sexes per 100,000 people in a standardized age because of Malaria.\n\nThe Description of the data-set. I will focus on each region separately, because even inside the same region sub parts can behave differently. The 9 regions are\n\nSDI Countries\nSaharan Region\nEuropean Countries\nGreat Britain or United Kingdom\nAmerican Region\nAsian Countries\nNorth Africa and Middle East\nOceania Region\nCaribbean and Latin American Countries\nSDI Countries with Malaria Death Count Rate\nInitially there are 5 sub parts in SDI countries, but we can factor them into three based on their death rate behavior. Some regions are having higher death rate close to 75, while others have lower death rate close to 0.0002.\nFirst group is the sub regions which are having higher death rates than other SDI regions, which are Low SDI and Low-middle SDI countries. Low SDI countries are behaving poorly from 1990 with a 73.78 death rate which reaches it highest peek of 75.88 in 2001 after this death rate gradually declines until 2016 and reaches 37.87.\nThis is not the case for Low middle SDI countries where in 1990 death rate is 19.11 and reaches its peek of 22.01 in 2003 with small fluctuations. After this it gradually and slowly declines until 2016 while finishing in a rate of 15.7.\n\nattach(MD_CM_SDI)\n# scatter plot for Low and Low middle SDI\nggplot(subset(MD_CM_SDI,Entity == \"Low SDI\" | Entity == \"Low-middle SDI\"),\n       aes(x=Year,\n       y=`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,\n       color=Entity,\n       label=round(`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,2)))+\n      ylab(\"Deaths - Malaria - Sex: Both \\n- Age: Age-standardized \\n(Rate) (per 100,000 people)\")+\n      ggtitle(\"Low-middle and Low SDI Countries\")+ geom_text_repel()+\n      geom_point()+geom_line()+scale_x_continuous(breaks=seq(1990,2016),labels =seq(1990,2016))+ \n      legend_bottom()+ theme(axis.text.x = element_text(angle = 90))+\n      scale_y_continuous(labels = seq(15,80,5),breaks =seq(15,80,5))\n\n\n\n\nSDI countries Middle and High-middle are performing better than the above two regions from 1990 itself. While Middle SDI countries have a very low death rate of close to 1 and periodically it decreases to 0.6 at 2016. To be exact in 1990 the death rate is 0.9274 and decrement occurs with small but unaffected fluctuations and reaches 0.6018 in 2016, which is the lowest point.\nFor High middle SDI countries the death rate in 1990 is 0.1168 and in 1996 it reaches the highest point of 0.1281. After 1996 there is clear decrease of death rate which reaches its minimum value of 0.0454 in 2013.\n\n# scatter plot for High middle and Middle\nggplot(subset(MD_CM_SDI,Entity == \"Middle SDI\"| Entity == \"High-middle SDI\"),\n       aes(x=Year,\n       y=`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,\n       color=Entity,\n       label=round(`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,4)))+\n      ylab(\"Deaths - Malaria - Sex: Both \\n- Age: Age-standardized \\n(Rate) (per 100,000 people)\")+\n      ggtitle(\"Middle and High-middle SDI Countries\")+ geom_text_repel()+\n      geom_point()+geom_line()+ legend_bottom()+\n      scale_x_continuous(breaks=seq(1990,2016),labels =seq(1990,2016))+ \n      theme(axis.text.x = element_text(angle = 90))+\n      scale_y_continuous(labels = seq(0,1,0.05),breaks =seq(0,1,0.05))\n\n\n\n\nHigh SDI countries are showing very strong decrease from 1990 it self. While in 1990 the death rate is 0.001387 and by polynomial decreasing by 2016 this reaches the least value of 0.000194.\n\n# scatter plot for High SDI\nggplot(subset(MD_CM_SDI, Entity ==\"High SDI\" ),\n       aes(x=Year,\n       y=`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,\n       label=round(`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,6)))+\n      ylab(\"Deaths - Malaria - Sex: Both \\n- Age: Age-standardized \\n(Rate) (per 100,000 people)\")+\n      ggtitle(\"High SDI Countries\")+ geom_text_repel()+ \n      legend_bottom()+ geom_point()+ geom_line()+ \n      scale_x_continuous(breaks=seq(1990,2016),labels =seq(1990,2016))+\n      theme(axis.text.x = element_text(angle = 90))+\n      scale_y_continuous(labels = seq(0,0.0014,0.0002),breaks =seq(0,0.0014,0.0002))\n\n\n\ndetach(MD_CM_SDI)\n\nIf we do plot all these 5 sub parts of SDI countries on one plot we would not have seen these differences and even if we do use facet grid this is not possible. Therefore, first I did plot all 5 regions together and then sub setted them based on their death rate change in values.\nIt is clear that High SDI values have the lowest death rate and Low, Low middle SDI have the highest death rates over the years 1990 to 2016.\nSaharan Region with Malaria Death Count Rate\nThere are 5 sub parts for Saharan region countries, which are Central Sub-Saharan, Eastern Sub-Saharan, Southern Sub-Saharan, Western Sub-Saharan and Sub-Saharan Africa countries. Here though it does not look like we need to separate these regions and plot them as groups. While plotting them as a whole they are clear and possible to interpret the differences in this line plot.\nCentral Sub-Saharan Africa and Western Sub-Saharan Africa countries behave similarly where they start with death rates respectively 108.67, 118.94. Further this death rate increases and reaches its highest peaks of 138.23 and 121.08 respectively for Western Sub-Saharan Africa and Central Sub-Saharan Africa countries for the year 2003. Finally they decrease into their lowest of 64.64 death rate for Central Sub-Saharan Africa and 87.54 death rate for Western Sub-Saharan Africa.\nSub-Saharan Africa Regions has a higher death rate performance through out the time line when it begins with 89.33 in 1990, and reaches its highest value of 96.28 in 2003. Further death rate decreases slowly up-to 51.93 in 2016, which is the least minimum value. Eastern Sub-Saharan Africa starts with 77.83 in 1990 and reaches its peak of 78.49 in 1991. In the next few years until 2000 there is small fluctuations. Finally there is a steep slope and reaches its lowest death rate value of 23.15 in 2016.\nDeath rate of Southern Sub-Saharan Africa is completely different than above four sub regions of Saharan Africa. In 1990 the death rate is 1.69 and it fluctuates until 2016, but it reaches the highest death rate of 2.57 in 2006. Further with this fluctuation in 2016 the death rate is 2.29 which is higher than the previous 7 years from 2016.\n\nattach(MD_CM_Sahara)\n# scatter plot for Sahara Region\nggplot(MD_CM_Sahara,\n        aes(x=Year,\n        y=`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,\n        color=Entity,\n        label=round(`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,2)))+\n      ylab(\"Deaths - Malaria - Sex: Both \\n- Age: Age-standardized \\n(Rate) (per 100,000 people)\")+\n      ggtitle(\"Saharan Africa Region\")+geom_point()+geom_line()+ legend_bottom()+\n      scale_x_continuous(breaks=seq(1990,2016),labels =seq(1990,2016))+ geom_text_repel()+\n      theme(axis.text.x = element_text(angle = 90))+\n      scale_y_continuous(labels = seq(0,140,10),breaks =seq(0,140,10))\n\n\n\ndetach(MD_CM_Sahara)\n\nConsidering 1990 and 2016 the biggest decrease occurs for the region of Eastern Sub-Saharan Africa, while the lowest decrease is for Western Sub-Saharan Africa. While in the year gap of 1990 and 2016 only Southern Sub-Saharan Africa has an increase in death rate from 1.69 to 2.29.\nEuropean Countries with Malaria Death Count Rate\nI believe that European countries did not have any malaria related deaths even before 1990, because of their weather patterns. That is simply assured by here in this plot, where there is no deaths for the sub regions of Europe. Which are Central Europe, Eastern Europe and Western Europe.\n\nattach(MD_CM_EU)\n# scatter plot for European Region\nggplot(MD_CM_EU,\n       aes(x=Year,\n           y=`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,\n           color=Entity,\n           label=`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`))+\n      ylab(\"Deaths - Malaria - Sex: Both \\n- Age: Age-standardized \\n(Rate) (per 100,000 people)\")+\n      ggtitle(\"European Region\")+ geom_point()+geom_line()+ legend_bottom()+\n      scale_x_continuous(breaks=seq(1990,2016),labels =seq(1990,2016))+ \n      theme(axis.text.x = element_text(angle = 90))+geom_text_repel()+\n      scale_y_continuous(labels = seq(0,140,10),breaks =seq(0,140,10))\n\n\n\ndetach(MD_CM_EU)\n\nGreat Britain or United Kingdom with Malaria Death Count Rate\nWe consider the collection of these countries as Great Britain or United Kingdom, which are England, Northern Ireland, Scotland and Wales. Here also weather pattern does have a high probability in causing a situation of no malaria related deaths. Further, these four countries are close to the European regions geographically therefore this assures us more that over the years from 1990 to 2016 the death rate is zero.\n\nattach(MD_CM_GB)\n# scatter plot for Great Britain or United Kingdom\nggplot(MD_CM_GB,\n       aes(x=Year,\n           y=`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,\n           color=Entity,\n           label=`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`))+\n      ylab(\"Deaths - Malaria - Sex: Both \\n- Age: Age-standardized \\n(Rate) (per 100,000 people)\")+\n      ggtitle(\"Great Britain or United Kingdom\")+ legend_bottom()+\n      geom_point()+geom_line()+ geom_text_repel()+\n      scale_x_continuous(breaks=seq(1990,2016),labels =seq(1990,2016))+ \n      theme(axis.text.x = element_text(angle = 90))+\n      scale_y_continuous(labels = seq(0,1,0.5),breaks =seq(0,1,0.5))\n\n\n\ndetach(MD_CM_GB)\n\nAmerican Region with Malaria Death Count Rate\nThere are 5 sub regions for American countries, yet we can divide them into two groups. One group will include Andean, Central and Tropical Latin American countries, and other group includes North America and Southern Latin American countries.\nIn the first group, Andean and Tropical Latin American countries begin with respectively 0.354, 0.351 death rates in 1990. This gradually decreases until 2016 for both regions, but Tropical Latin countries has a better decrement than Andean Latin American countries because they achieve death rate of respectively 0.038 and 0.048.\nIn this same time period of 1990 to 2016, Central Latin American begins with a death rate of 0.296 and reaches its lowest point of 0.053. In year 1994, Tropical Latin American region and Central Latin American region have the same death rate of 0.222. Further in the years 2004 and 2005 the death rates of Andean Latin American countries are 0.089, 0.071, but the same years Central Latin American region has death rates of 0.088, 0.080. Those are the two crucial changes which occur.\n\nattach(MD_CM_LA)\n# scatter plot for Andean, Central and Tropical Latin America\nggplot(subset(MD_CM_LA,Entity == \"Andean Latin America\"| Entity == \"Central Latin America\" | \n              Entity == \"Tropical Latin America\"),\n       aes(x=Year,\n       y=`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,\n       color=Entity,\n       label=round(`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,3)))+\n      ylab(\"Deaths - Malaria - Sex: Both \\n- Age: Age-standardized \\n(Rate) (per 100,000 people)\")+\n      ggtitle(\"Andean, Central and Tropical Latin American Region\")+\n      geom_point()+geom_line()+ legend_bottom()+ geom_text_repel()+\n      scale_x_continuous(breaks=seq(1990,2016),labels =seq(1990,2016))+ \n      theme(axis.text.x = element_text(angle = 90))+\n      scale_y_continuous(labels = seq(0,0.40,0.05),breaks =seq(0,0.40,0.05))\n\n\n\n\nThe second group has North American region and Southern Latin American region. Clearly North America with its cold weather condition and developed status does not hold any deaths for malaria from 1990 to 2016. It is even possible to consider before this time range also there was no deaths for malaria. Yet Southern Latin American has death rate of 0.0208 in 1990 but decreases gradually and reaches its least minimum point of 0.0035 in 2016.\n\n# scatter plot for North and Southern Latin America\nggplot(subset(MD_CM_LA,Entity== \"North America\"| Entity ==\"Southern Latin America\"),\n       aes(x=Year,\n       y=`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,\n       color=Entity,\n       label=round(`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,4)))+\n      ylab(\"Deaths - Malaria - Sex: Both \\n- Age: Age-standardized \\n(Rate) (per 100,000 people)\")+\n      ggtitle(\"North America and Southern Latin American Region\")+\n      geom_point()+geom_line()+ legend_bottom()+ geom_text_repel()+\n      scale_x_continuous(breaks=seq(1990,2016),labels =seq(1990,2016))+ \n      theme(axis.text.x = element_text(angle = 90))+\n      scale_y_continuous(labels = seq(0,0.021,0.001),breaks =seq(0,0.021,0.001))\n\n\n\ndetach(MD_CM_LA)\n\nComparing the American region only North American region is malaria free, while least amount progress has occurred in Central Latin American region. Tropical region has out performed Andean Latin American region. Next to North America, Southern Latin American has done more progress than other three regions.\nAsian Countries with Malaria Death Count Rate\nThere are 6 regions in the Asian region, which are South Asia, Southeast Asia, East Asia, High Income Asia Pacific, Australasia and Central Asia. Asia is a Large continent with wide variety of countries therefore we have more sub regions than any other continent here.\nSouth Asia and Southeast Asia are two regions which behave similarly where it begins in a higher rate in year 1990 and gradually decreasing until year 2016. South Asia has a death rate of 6.21 in 1990, but in 2016 it reaches to 3.61. For Southeast Asian region the death rate is 5.29 in year 1990 and reaches the death rate of 2.56 in year 2016.\n\nattach(MD_CM_A)\n# scatter plot for South Asia and Southeast Asia\nggplot(subset(MD_CM_A,Entity == \"South Asia\" | Entity == \"Southeast Asia\"),\n       aes(x=Year,\n       y=`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,\n       color=Entity,\n       label=round(`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,2)))+\n      ylab(\"Deaths - Malaria - Sex: Both \\n- Age: Age-standardized \\n(Rate) (per 100,000 people)\")+\n      ggtitle(\"South Asia and Southeast Asia Region\")+ \n      geom_point()+geom_line()+ legend_bottom()+ geom_text_repel()+\n      scale_x_continuous(breaks=seq(1990,2016),labels =seq(1990,2016))+ \n      theme(axis.text.x = element_text(angle = 90))+\n      scale_y_continuous(labels = seq(2.4,6.4,0.2),breaks =seq(2.4,6.4,0.2))\n\n\n\n\nEast Asia region begins with a death rate of 0.0269 in year 1990 and it decreases over the coming years which will reach the lowest death rate of 0.0121 in year 2015. Further in year 2016 it slightly increases to 0.0122.\n\n# scatter plot for East Asia\nggplot(subset(MD_CM_A, Entity == \"East Asia\"),\n       aes(x=Year,\n           y=`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,\n           label=round(`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,4)))+\n      ylab(\"Deaths - Malaria - Sex: Both \\n- Age: Age-standardized \\n(Rate) (per 100,000 people)\")+\n      ggtitle(\"East Asia Region\")+  \n      geom_point()+ geom_line()+ legend_bottom()+ geom_text_repel()+\n      scale_x_continuous(breaks=seq(1990,2016),labels =seq(1990,2016))+ \n      theme(axis.text.x = element_text(angle = 90))+\n      scale_y_continuous(labels = seq(0.01,0.03,0.002),breaks =seq(0.01,0.03,0.002))\n\n\n\n\nCentral Asia region begins with a a death rate of 0.0304 in year 1990 and increases very slowly until year 1998 towards the death rate of 0.0314. After this the death rate decreases gradually until year 2013 where it becomes 0.0057. Until year 2016 this becomes the standard death rate for malaria in Central Asia Region.\n\n# scatter plot for Central Asia\nggplot(subset(MD_CM_A,Entity == \"Central Asia\"),\n       aes(x=Year,\n       y=`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,\n       label=round(`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,4)))+\n      ylab(\"Deaths - Malaria - Sex: Both \\n- Age: Age-standardized \\n(Rate) (per 100,000 people)\")+\n      ggtitle(\"Central Asia Region\")+ geom_text_repel()+\n      geom_point()+geom_line()+ legend_bottom()+\n      scale_x_continuous(breaks=seq(1990,2016),labels =seq(1990,2016))+ \n      theme(axis.text.x = element_text(angle = 90))+\n      scale_y_continuous(labels = seq(0,0.05,0.005),breaks =seq(0,0.05,0.005))\n\n\n\n\nHigh Income Asia Pacific countries have a lower death rate than previous Asian region countries where in 1990 the death rate is 0.0073 but over the years it decreases gradually and reaches the lowest death rate value of 0.0008 in year 2016.\nAustralasia countries have no death rate over the year range of 1990 to 2016, which implicate that there is a possibility that before 1990 also there could have not been any deaths related to malaria\n\n# scatter plot for High Income Pacific and Australasia\nggplot(subset(MD_CM_A, Entity == \"Australasia\" | Entity == \"High-income Asia Pacific\" ),\n       aes(x=Year,\n       y=`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,color=Entity,\n       label=round(`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,4)))+\n      ylab(\"Deaths - Malaria - Sex: Both \\n- Age: Age-standardized \\n(Rate) (per 100,000 people)\")+\n      ggtitle(\"High Income Asia Pacific  and Australasia Region\")+\n      geom_point()+geom_line()+ legend_bottom()+ geom_text_repel()+\n      scale_x_continuous(breaks=seq(1990,2016),labels =seq(1990,2016))+ \n      theme(axis.text.x = element_text(angle = 90))+\n      scale_y_continuous(labels = seq(0.001,0.008,0.0005),breaks =seq(0.001,0.008,0.0005))\n\n\n\ndetach(MD_CM_A)\n\nAustralasia is the only region which does not have malaria over the years given in the data set. The lowest death rate occurs to High Income Asian Pacific region than other regions of Asia.\nNorth Africa and Middle East Region with Death Count Rate\nNorth Africa and Middle East region is the only odd region of all in this data set for malaria death rate. In year 1990 the death rate is 0.7968 but in the coming years it increases until 2005 and reaches 1.3444. Despite this increment after 2005 the death rate suddenly and rapidly drops until year 2011 and reaches its lowest point of 0.731. Again there is a steady but slow increase in death rate and reaches 0.8332 in year 2016.\n\nattach(MD_CM_A)\n# scatter plot to North Africa and Middle East\nggplot(subset(MD_CM_A, Entity == \"North Africa and Middle East\" ),\n       aes(x=Year,\n       y=`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,\n       label=round(`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,4)))+\n      ggtitle(\"North Africa and Middle East Region\")+\n      ylab(\"Deaths - Malaria - Sex: Both \\n- Age: Age-standardized \\n(Rate) (per 100,000 people)\")+\n      geom_point()+geom_line()+ legend_bottom()+ geom_text_repel()+\n      scale_x_continuous(breaks=seq(1990,2016),labels =seq(1990,2016))+ \n      theme(axis.text.x = element_text(angle = 90))+\n      scale_y_continuous(labels = seq(0.5,1.5,0.05),breaks =seq(0.5,1.5,0.05))\n\n\n\ndetach(MD_CM_A)\n\nComparing all the other regions in this data set only North Africa and Middle East Region was very special with its odd fluctuations. At the end comparing the death rates of year 1990 and 2016 there is an increase close to 0.04, but it is far less than what was in year 2005 and 1990.\nOceania Region with Death Count Rate\nOceania region begins the malaria death rate of 14.78 in 1990, even though it oscillates over the next few years and reaches a death rate of 12.5 in 1998 and in year 2000 it reaches a staggering highest peak of 53.18 death rate. After reaching the highest peak it decreases slowly in the next few years and reaches its lowest point of 8.71 in 2016.\n\nattach(MD_CM_A)\n# Scatter plot to Oceania\nggplot(subset(MD_CM_A,Entity == \"Oceania\"),\n       aes(x=Year,\n       y=`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,\n       label=round(`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,2)))+\n      ylab(\"Deaths - Malaria - Sex: Both \\n- Age: Age-standardized \\n(Rate) (per 100,000 people)\")+\n      ggtitle(\"Oceania Region\")+\n      geom_point()+geom_line()+ legend_bottom()+ geom_text_repel()+\n      scale_x_continuous(breaks=seq(1990,2016),labels =seq(1990,2016))+ \n      theme(axis.text.x = element_text(angle = 90))+scale_y_continuous(labels = seq(6,54,2),breaks =seq(6,54,2))\n\n\n\ndetach(MD_CM_A)\n\nOceania region behaves very differently than other regions. By the year 2015 lowest death rate of 8.64 occurs for Oceania region.\nCaribbean and Latin America and Caribbean Countries with Death Count Rate\nCaribbean region countries in the 1990 has a malaria death rate of 0.1265, but next year it is 0.1519. Further it decreases over the years with some fluctuations. By year 2016 the death rate reaches 0.058.\nLatin American and Caribbean region countries begin with the death rate of 0.3043 in 1990, but it exponentially decreases until 2016 and reaches 0.0468. In the course it out performs Caribbean countries after year 1999 where Latin American and Caribbean countries occupy a death rate of 0.1282, Caribbean countries occupy a death rate of 0.1274.\n\nattach(MD_CM_C_LA)\n# Scatter plot to Caribbean and Latin America\nggplot(MD_CM_C_LA,\n       aes(x=Year,\n       y=`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,color=Entity,\n       label=round(`Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)`,4)))+\n      ylab(\"Deaths - Malaria - Sex: Both \\n- Age: Age-standardized \\n(Rate) (per 100,000 people)\")+\n      ggtitle(\"Caribbean and Latin American and Caribbean Region\")+\n      geom_point()+ geom_line()+ legend_bottom()+ geom_text_repel()+\n      scale_x_continuous(breaks=seq(1990,2016),labels =seq(1990,2016))+ \n      theme(axis.text.x = element_text(angle = 90))+\n      scale_y_continuous(labels = seq(0.0,0.35,0.05),breaks =seq(0.0,0.35,0.05))\n\n\n\ndetach(MD_CM_C_LA)\n\nClearly Latin American and Caribbean countries perform better than Caribbean countries after year 1999 until year 2016. At the beginning of 1990 the death rate gap between both these regions are clearly high.\nConclusion\nI shall conclude my findings in point form\n\nDeath rate for malaria decreases over the years with small fluctuations over the years and reaches zero in 2016 for Sri Lanka. By 2016 all age categories reach a death count of zero, but in 1990 they are at different parts of the scale.\nHigh SDI countries have a low death rate. While Low and Low middle SDI countries have the highest death rates over the years from 1990 to 2016.\nIn the Saharan region lowest death rate is for Southern Sub-Saharan region and highest death rate is for Western Sub-Saharan region on the year 2016. This order is same for year 1990 as well in these regions. Highest death rate of all years and all sub regions occurs in year 2003 of 138.23 for Western Sub-Saharan Africa. While lowest death rate of all years and all sub regions is in year 1994 for Southern Sub-Saharan Africa with 1.55.\nEuropean and Great Britain related regions have no malaria related incidents over the years of 1990 to 2016, this could be because of their cold climate pattern and their developed status in the world.\nNorth America shows figures of no malaria related deaths over the years of 1990 to 2016. Southern Latin America has the lowest death rate of 0.0035 in 2016 and highest death rate is for Central Latin America. But in year 1990 highest death rate is for Andean Latin American region which is 0.354 and least death rate is for Southern Latin America with 0.0208.\nAustralasia has no death rate over the years of 1990 to 2016. Where in year 2016 lowest death rate occurs to High Income Asia Pacific of 0.0008 and highest in that year for Oceania region with a value of 8.71. In the year 1990 highest death rate is for Oceania region with 14.78 but lowest is for High Income Asia Pacific region with 0.0073. A special occurrence where comparing all the regions and years the highest death rate is for Oceania Region in year 2000.\nHighest death rate occurs in year 2005 for North Africa and Middle East region, but in 1990 the malaria death rate is 0.7968. Further in 2011 it hits the lowest value of 0.731 and finally moves up-to 0.8332 in year 2016.\nIn year 1990 Caribbean region has a lower death rate than Latin American and Caribbean region. But this gap gradually decreases over time and by year 2016 Caribbean region reaches a death rate of 0.058 while Latin America and Caribbean region occupies the rate of 0.0468.\nFurther Analysis\n\nThis article only focuses on the death rate but we can expand it to countries and compare them over continents.\nIt is possible to consider the age category and compare the death counts of different regions to understand further of these regions.\n\nPlease see that\nThis is my third post on the internet so please be kind to tolerate my mistakes in grammar and spellings. I intend to post more statistics related materials in the future and learn accordingly. Thank you for reading.\nTHANK YOU"
  },
  {
    "objectID": "tidytuesday/2018/Week_30/index.html",
    "href": "tidytuesday/2018/Week_30/index.html",
    "title": "Week 30: Movie Profit",
    "section": "",
    "text": "Movie Profit, Not So Profit\nThis is my first post on Tidy Tuesday and the data-set in question is Movie profit data-set. Even though the title of data says Movie profit I am going to focus on the movies which did not generate any revenue domestic and suggest on gross in worldwide.\nThe packages that I have used here are magrittr, tidyverse, scales, ggthemr, knitr, kableExtra, ggthemr and lubridate. The theme I am using for plots is “flat dark”.\nUnderstand Genre and Mpaa Rating on Movies\n3401 movies with 8 variables of information which include numeric and categorical. There are 202 distributors for movies of four types of ratings which are G, PG, PG-13 and R, but 137 movies have no record of them. Also there are five categories for genre, where Drama with 1236, while horror with 298 movies.\n\n# Loading the packages\nlibrary(magrittr)\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(ggthemr)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(lubridate)\n\n# load the data\nMovie &lt;- movie_profit &lt;- read_csv(\"movie_profit.csv\", \n                                  col_types = cols(...1 = col_skip(), release_date = col_date(format = \"%m/%d/%Y\")))\n\n# Load the theme\nggthemr(\"flat dark\")\n\n# looking at dimensions\ndim(Movie)\nattach(Movie)\n\n\n# Bar plot to Genre\nggplot(Movie,aes(genre))+\n  geom_bar()+stat_count(aes(y=..count.., label=..count..), geom=\"text\", vjust=-.5)+\n  xlab(\"Genre\")+ylab(\"Frequency\")+\n  ggtitle(\"All movies in perspevtive of Genre \")\n\n\n\n\nThere are five types of ratings but around half of them are R rated, while 1094 are PG-13. While 573 are in category of PG and G rated movies are only 85. Finally, 137 movies do not have any ratings.\n\n# Bar plot to MPAA Ratings\nggplot(Movie,aes(mpaa_rating))+\n  geom_bar()+geom_bar()+\n  stat_count(aes(y=..count.., label=..count..),geom=\"text\", vjust=-.5)+\n  xlab(\"Rating\")+ylab(\"Frequency\")+\n  ggtitle(\"All movies in persepvtie of MPAA Rating\")\n\n\n\n\nComedy movies are mostly R rated(under 17 requires guardian) and PG-13 (some material is inappropriate to under 13). Where the frequencies are respectively 367 and 328. 309 movies of adventure genre could be watched by children with accompanying parents and 67 movies can be watched by all ages.\nYet 645 Drama movies are R-rated.There is only one action movie for general audiences(for all) and obviously no horror film should be watched by children alone, yet there are 7 movies which you can watch with your parents.\n\n#checking for bias in mpaa rating and genre\nkable(table(mpaa_rating,genre),\"html\") %&gt;%\nkable_styling(bootstrap_options = c(\"striped\"),full_width = T) %&gt;%\nadd_header_above(c(\"Contingency table in counts for Genre versus MPAA Rating\"=6))  \n\n\n\n\n\n\n\n\n\n\n\n\n\nContingency table in counts for Genre versus MPAA Rating\n\n\n\n\nAction\nAdventure\nComedy\nDrama\nHorror\n\n\n\n\nG\n1\n67\n6\n11\n0\n\n\nPG\n34\n309\n79\n144\n7\n\n\nPG-13\n225\n83\n328\n398\n58\n\n\nR\n286\n14\n367\n645\n202\n\n\n\n\n\nWe think horror movies are mostly R-rated then it is true. But only it is explainable by percentage. Yet considering the amount of horror movies made generally it is very low even in this random sample. Action and Comedy movies have very close percentages for PG-13 ratedness, while 52% are R rated for Action and 47% are comedy.\n\n# column percentage for above table\nkable(table(mpaa_rating,genre) %&gt;%\n    prop.table(margin=2) %&gt;%\n    round(digits = 2)) %&gt;%\n    kable_styling(bootstrap_options = c(\"striped\"),full_width = T) %&gt;%\n    add_header_above(c(\"Percentage table for Genre versus MPAA Rating\"=6))\n\n\n\n\n\n\n\n\n\n\n\n\n\nPercentage table for Genre versus MPAA Rating\n\n\n\n\nAction\nAdventure\nComedy\nDrama\nHorror\n\n\n\n\nG\n0.00\n0.14\n0.01\n0.01\n0.00\n\n\nPG\n0.06\n0.65\n0.10\n0.12\n0.03\n\n\nPG-13\n0.41\n0.18\n0.42\n0.33\n0.22\n\n\nR\n0.52\n0.03\n0.47\n0.54\n0.76\n\n\n\n\n\nThis data-set contains the release dates from 1956 to 2019. Even though it is not 2019 there is a movie which has been listed here. This explains the domestic and worldwide gross being zero as zero. Then again we have to be careful because there are movies which might not make profit at all, domestic or other wise.\nLets Focus of movies which has zero domestic gross\nNo revenue from 66 movies, that is interesting. So obviously Aqua man has a whopping more than 150 million dollars\nproduction budget and no profit because it was not released yet when this data set was compiled. Second rank is for “Wonder park” with 100 million dollars. This movie will be released in 2019.\nZero Domestic gross Point of View\nSurprisingly there are movies without any production budget information because I am very sure No movie is done for free. Specially it is odd to see “12 Angry Men” in this list, which leads to the conclusion not all Movies in this list are to be on-it in the first place. We have 66 movies to consider.\n\n# domestic gross zero only movies\nMovie_domestic_zero&lt;-subset.data.frame(Movie,c(domestic_gross==0)) \n# checking dimensions\ndim(Movie_domestic_zero)\nattach(Movie_domestic_zero)\n\n\n# Scatterplot for production budget\nggplot(Movie_domestic_zero,aes(x=reorder(movie,production_budget),\n                               y=production_budget))+\n  geom_point()+theme(axis.text.x =element_text(angle = 90, hjust = 1))+\n  scale_y_continuous(labels = dollar_format())+\n  ylab(\"Production budget\")+xlab(\"Movie names\")+\n  ggtitle(\"Domestic Gross Zero but how production budget varies in Movies\")\n\n\n\n\nGenre and MPAA Rating Point of View\nSo movies with R ratedness have the most count and they are also action and drama genre movies of count 10. Here also there are 11 movies with have not been classified into any rating. Finally, there is no G rated movie in this graphical representation. Majority of movies (31) are from R rated in related to rating. While considering genre the Drama category is represented by 24. Action, Drama and Horror movies includes missing rating.\n\n#plotting worldwide gross with genre\nMovie_domestic_zero %&gt;% \n  ggplot(aes(x=mpaa_rating,fill=genre)) +\n  geom_bar(position = \"stack\")+ylab(\"Frequency\")+xlab(\"MPAA Rating\")+\n  geom_text(aes(label=..count..),stat='count',position=position_stack(0.4))+\n  ggtitle(\"MPAA Rating counts with Genre\")\n\n\n\n#plotting worldwide gross with mpaa rating\nMovie_domestic_zero %&gt;% \n  ggplot(aes(fill=mpaa_rating,x=genre)) +\n  geom_bar(position = \"stack\")+ylab(\"Frequency\")+xlab(\"Genre\")+\n  geom_text(aes(label=..count..),stat='count',position=position_stack(0.4))+\n  ggtitle(\"Genre counts with MPAA Rating\")\n\n\n\n\nFinding Outliers in Perspective of Genre and MPAA Rating\nConsidering the box-plot there are 3 outliers in Drama while plotting data in perspective of genre. Most amount of production budget is concluded in Action genre, while the least is in Horror.\nIf we focus on the production budget with genre there are 7 outliers and one action movie has spent 100 million dollars, similarly a movie from adventure category spent more than 100 million dollars. Others production budget is way less than 50 million dollars.\n\n#plotting production budget with genre\nMovie_domestic_zero %&gt;% \n  ggplot(aes(genre,production_budget)) +\n  geom_boxplot()+ylab(\"Production Budget\")+\n  xlab(\"Genre\")+\n  scale_y_continuous(labels = dollar_format())+\n  expand_limits(y=0)+coord_flip()+\n  ggtitle(\"Boxplot for Production budget in perspective of Genre\")\n\n\n\n\nLeast amount budget is spent on movies of no rating mentioned while most is on PG-13 rated movies and it has one strong outlier. Previously with genre we had 7 outliers but according to MPAA rating there are only 6 outliers.\n\n#plotting production budget with genre\nMovie_domestic_zero %&gt;% \n  ggplot(aes(mpaa_rating,production_budget)) +\n  geom_boxplot()+ylab(\"Production Budget\")+\n  xlab(\"MPAA rating\")+\n  scale_y_continuous(labels = dollar_format())+\n  expand_limits(y=0)+coord_flip()+\n  ggtitle(\"Boxplot for Production budget in perspective of MPAA Rating\")\n\n\n\n\nProduction Budget and Worldwide Gross\nAccording to the ascending order in the list of 10 movies with lowest production budget only 2 have profited. One movie (All the Boys Love Mandy Lane) has considerably done good, but if you consider this list of 10 movies we have 12 angry men as well.\n\n# ascending order of top ten movies in production budget\nkable(Movie_domestic_zero[order(Movie_domestic_zero$production_budget),-c(1,4)] %&gt;% \n      head(10),\n      col.names=c(\"Movie Name\",\"Production Budget\",\"Wordlwide Gross\",\"Distributor\",\n                  \"MPAA Rating\",\"Genre\")) %&gt;% \nkable_styling(full_width = T,font_size = 13) %&gt;%\nadd_header_above(c(\"Top 10 Least production budget movies for Domestic gross 0\"=6))\n\n\n\n\n\n\n\n\n\n\n\n\n\nTop 10 Least production budget movies for Domestic gross 0\n\n\n\nMovie Name\nProduction Budget\nWordlwide Gross\nDistributor\nMPAA Rating\nGenre\n\n\n\n\n12 Angry Men\n340000\n0\nUnited Artists\nNA\nDrama\n\n\nMy Beautiful Laundrette\n400000\n0\nOrion Classics\nNA\nDrama\n\n\nEverything Put Together\n500000\n7890\nNA\nR\nDrama\n\n\nAll the Boys Love Mandy Lane\n750000\n1960521\nRadius\nR\nHorror\n\n\nJimmy and Judy\n1000000\n0\nOutrider Pictures\nR\nAction\n\n\nThe Poker House\n1000000\n0\nPhase 4 Films\nR\nDrama\n\n\nProud\n1000000\n0\nCastle Hill Product…\nPG\nDrama\n\n\nSteppin: The Movie\n1000000\n0\nWeinstein Co.\nPG-13\nComedy\n\n\nZombies of Mass Destruction\n1000000\n0\nAfter Dark\nR\nComedy\n\n\nGrand Theft Parsons\n1200000\n0\nSwipe Films\nPG-13\nDrama\n\n\n\n\n\nThis indicates that we didn’t have records how much of profit in home and away properly, because there is no way that people did not watch that movie and not make any gross. So we conclude that some movies which were released before 1970s did not pertain any information of gross domestic or worldwide.\nYears, Months and Days versus Production Budget\nThere are 4 Movies before 1972 with zero for domestic gross which can conclude loss of information. Oddly in year 2014 there are 8 movies with zero domestic gross and most of the movies are after year 2000.\n\n# plotting years vs movies released\nggplot(Movie_domestic_zero,aes(x=year(release_date)))+\n      geom_bar()+ylab(\"Frequency\")+xlab(\"Years\")+\n      scale_x_continuous(label=1956:2019,breaks=1956:2019)+\n      scale_y_continuous(labels = 0:8,breaks = 0:8)+\n      stat_count(aes(y=..count.., label=..count..), geom=\"text\", vjust=-.5)+\n      theme(axis.text.x = element_text(angle = 90,hjust = 2))\n\n\n\n\nConsidering the months there is no specialty most of the counts are in-between 3 and 8. In January there are only two movies.\n\nggplot(Movie_domestic_zero,aes(x=month(release_date)))+\n      geom_bar()+ylab(\"Frequency\")+xlab(\"Months\")+\n      scale_x_continuous(label=1:12,breaks=1:12)+\n      scale_y_continuous(labels = 0:8,breaks = 0:8)+\n      stat_count(aes(y=..count.., label=..count..), geom=\"text\", vjust=-.5)\n\n\n\n\nOnly the movies with release dates 19th and 25th have no domestic gross, while highest count of 6 occurs on 21st. Most of the days have the count of 1 movie.\n\nggplot(Movie_domestic_zero,aes(x=day(release_date)))+\n      geom_bar()+ylab(\"Frequency\")+xlab(\"Days\")+\n      scale_x_continuous(label=1:31,breaks=1:31)+\n      scale_y_continuous(labels = 0:6,breaks = 0:6)+\n      stat_count(aes(y=..count.., label=..count..), geom=\"text\", vjust=-.5)\n\n\n\n\nConclusion\nMy conclusion of the above plots and tables in point form\n\nIn the complete data set Drama Genre has most (1236) counts and least (298) count goes to Horror. While, most (1514) of the Movies are R rated, least count goes to Grated movies. Considering Genre and Rating, it is true that Horror movies are R rated while it represents 76%, and should not let children watch alone.\nWhile there are movies with No domestic gross some have not been released yet (Aqua man and Wonder Park). Further, some Movies do not even have worldwide gross. This causes missing information. Even though a famous movies such as “12 Angry Men”.\nThis missing information could be the related to the fact that there are 4 movies which were released before 1972. Oddly in 2014 there are 8 movies which do not contain domestic gross information. Further, most of these movies were released after year 2000.\nBox plot indicates Adventure genre have spent more range in production budget, while in perspective of MPAA rating PG-13 movies have most range in production budget with a clear outlier.\nFurther Analysis\n\nSimilarly we can focus on movies of world wide gross equals to zero with other variables.\nConduct scrutinized interest with movies of world wide gross zero and domestic gross zero.\n\nPlease see that This is my first post on the internet so please be kind to tolerate my mistakes in grammar and spellings. I intend to post more statistics related materials in the future and learn accordingly. Thank you for reading.\nTHANK YOU"
  },
  {
    "objectID": "publication/Presidential_Election/index.html",
    "href": "publication/Presidential_Election/index.html",
    "title": "SLPresElection: Presidential Election Data of Sri Lanka from 1982 to 2015.",
    "section": "",
    "text": "Presidential Election data of “Sri Lanka”” is stored in Pdf files, through Pdf scraping they are converted into data-frames and stored in this R package.\nCRAN     Website"
  },
  {
    "objectID": "publication/Nigeria_HIV_meta_analysis/index.html",
    "href": "publication/Nigeria_HIV_meta_analysis/index.html",
    "title": "Prevalence of human immunodeficiency virus (HIV) among pregnant women in Nigeria: a systematic review and meta-analysis.",
    "section": "",
    "text": "Objective - To estimate prevalence of HIV infection in Nigeria and to examine variations by geopolitical zones and study characteristics to inform policy, practice and research.\nMethods - We conducted a comprehensive search of bibliographic databases including PubMed, CINAHL, PsycINFO, Global Health, Academic Search Elite and Allied and Complementary Medicine Database (AMED) and grey sources for studies published between 1 January 2008 and 31 December 2019. Studies reporting prevalence estimates of HIV among pregnant women in Nigeria using a diagnostic test were included. Primary outcome was proportion (%) of pregnant women living with HIV infection. A review protocol was developed and registered (PROSPERO 2019 CRD42019107037).\nResults - Twenty-three studies involving 72728 pregnant women were included. Ten studies were of high quality and the remaining were of moderate quality. Twenty-one studies used two or more diagnostic tests to identify women living with HIV. Overall pooled prevalence of HIV among pregnant women was 7.22% (95% CI 5.64 to 9.21). Studies showed high degree of heterogeneity (I^2=97.2%) and evidence of publication bias (p=0.728). Pooled prevalence for most individual geopolitical zones showed substantial variations compared with overall prevalence. North-Central (6.84%, 95% CI 4.73 to 9.79) and South-West zones (6.27%, 95% CI 4.75 to 8.24) had lower prevalence whereas South-East zone (17.04%, 95% CI 9.01 to 29.86) had higher prevalence.\nConclusions - While robust national prevalence studies are sparse in Nigeria, our findings suggest 7 in every 100 pregnant women are likely to have HIV infection. These figures are consistent with reported prevalence rates in sub-Saharan African region. WHO has indicated much higher prevalence in Nigeria compared with our findings. This discrepancy could potentially be attributed to varied methodological approaches and regional focus of studies included in our review. The magnitude of the issue highlights the need for targeted efforts from local, national and international stakeholders for prevention, diagnosis, management and treatment.\nView the Article"
  },
  {
    "objectID": "publication/Nigeria_HIV_meta_analysis/index.html#abstract",
    "href": "publication/Nigeria_HIV_meta_analysis/index.html#abstract",
    "title": "Prevalence of human immunodeficiency virus (HIV) among pregnant women in Nigeria: a systematic review and meta-analysis.",
    "section": "",
    "text": "Objective - To estimate prevalence of HIV infection in Nigeria and to examine variations by geopolitical zones and study characteristics to inform policy, practice and research.\nMethods - We conducted a comprehensive search of bibliographic databases including PubMed, CINAHL, PsycINFO, Global Health, Academic Search Elite and Allied and Complementary Medicine Database (AMED) and grey sources for studies published between 1 January 2008 and 31 December 2019. Studies reporting prevalence estimates of HIV among pregnant women in Nigeria using a diagnostic test were included. Primary outcome was proportion (%) of pregnant women living with HIV infection. A review protocol was developed and registered (PROSPERO 2019 CRD42019107037).\nResults - Twenty-three studies involving 72728 pregnant women were included. Ten studies were of high quality and the remaining were of moderate quality. Twenty-one studies used two or more diagnostic tests to identify women living with HIV. Overall pooled prevalence of HIV among pregnant women was 7.22% (95% CI 5.64 to 9.21). Studies showed high degree of heterogeneity (I^2=97.2%) and evidence of publication bias (p=0.728). Pooled prevalence for most individual geopolitical zones showed substantial variations compared with overall prevalence. North-Central (6.84%, 95% CI 4.73 to 9.79) and South-West zones (6.27%, 95% CI 4.75 to 8.24) had lower prevalence whereas South-East zone (17.04%, 95% CI 9.01 to 29.86) had higher prevalence.\nConclusions - While robust national prevalence studies are sparse in Nigeria, our findings suggest 7 in every 100 pregnant women are likely to have HIV infection. These figures are consistent with reported prevalence rates in sub-Saharan African region. WHO has indicated much higher prevalence in Nigeria compared with our findings. This discrepancy could potentially be attributed to varied methodological approaches and regional focus of studies included in our review. The magnitude of the issue highlights the need for targeted efforts from local, national and international stakeholders for prevention, diagnosis, management and treatment.\nView the Article"
  },
  {
    "objectID": "publication/index.html",
    "href": "publication/index.html",
    "title": "Publication",
    "section": "",
    "text": "Welcome to my publications list, this contains brief summaries for publications and conference posters. Have a read through and let me know your thoughts.\n\n\n\n\n  \n\n\n\n\nA model robust subsampling approach for Generalised Linear Models in big data settings.\n\n\n\n\n\n\n\nLarge data set\n\n\nModel averaging\n\n\nOptimal subsampling\n\n\nExperimental design\n\n\n\n\n\n\n\n\n\n\n\nMay 27, 2023\n\n\n\n\n\n\n  \n\n\n\n\nPrevalence of human immunodeficiency virus (HIV) among pregnant women in Nigeria: a systematic review and meta-analysis.\n\n\n\n\n\n\n\npackage\n\n\nR\n\n\nmeta\n\n\nmetafor\n\n\nmeta-analysis\n\n\n\n\n\n\n\n\n\n\n\nDecember 31, 2021\n\n\n\n\n\n\n  \n\n\n\n\nPrevalence of antenatal depression in South Asia: a systematic review and meta-analysis.\n\n\n\n\n\n\n\npackage\n\n\nR\n\n\nmeta\n\n\nmetafor\n\n\nmeta-analysis\n\n\n\n\n\n\n\n\n\n\n\nApril 22, 2019\n\n\n\n\n\n\n  \n\n\n\n\nfitODBOD: An R Package to Model Binomial Outcome Data using Binomial Mixture and Alternate Binomial Distributions.\n\n\n\n\n\n\n\npackage\n\n\nR\n\n\nCRAN\n\n\nfitODBOD\n\n\nJOSS\n\n\n\n\n\n\n\n\n\n\n\nApril 22, 2019\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n  \n\n\n\n\nOlympicRshiny: ‘Shiny’ Application for Olympic Data.\n\n\n\n\n\n\n\npackage\n\n\nR\n\n\nRshiny\n\n\nOlympic\n\n\n\n\n\n\n\n\n\n\n\nFebruary 2, 2023\n\n\n\n\n\n\n  \n\n\n\n\nSLPresElection: Presidential Election Data of Sri Lanka from 1982 to 2015.\n\n\n\n\n\n\n\npackage\n\n\nR\n\n\nData\n\n\nSri Lanka\n\n\nElection\n\n\n\n\n\n\n\n\n\n\n\nJanuary 17, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "arXiv Preprint | Code\nOver the centuries, I have devoted countless hours to deciphering and translating the ancient scripts and dialects of the various peoples of Middle-earth. My goal is to gain a deeper understanding of the cultures and histories of these peoples by studying their languages. Currently, I am working on a monograph that explores the linguistic roots of the Elvish languages. Through extensive research and analysis, I hope to shed light on the connections between the different dialects of Elvish and their origins. This project has been particularly challenging, as Elvish is a complex and nuanced language, but I am determined to see it through to completion."
  },
  {
    "objectID": "projects/index.html#the-languages-of-middle-earth",
    "href": "projects/index.html#the-languages-of-middle-earth",
    "title": "Projects",
    "section": "",
    "text": "arXiv Preprint | Code\nOver the centuries, I have devoted countless hours to deciphering and translating the ancient scripts and dialects of the various peoples of Middle-earth. My goal is to gain a deeper understanding of the cultures and histories of these peoples by studying their languages. Currently, I am working on a monograph that explores the linguistic roots of the Elvish languages. Through extensive research and analysis, I hope to shed light on the connections between the different dialects of Elvish and their origins. This project has been particularly challenging, as Elvish is a complex and nuanced language, but I am determined to see it through to completion."
  },
  {
    "objectID": "projects/index.html#the-history-of-the-war-of-the-ring",
    "href": "projects/index.html#the-history-of-the-war-of-the-ring",
    "title": "Projects",
    "section": "The History of the War of the Ring",
    "text": "The History of the War of the Ring\n\narXiv Preprint | Code\nI am creating a comprehensive and detailed history of the conflict that goes beyond the surface-level events. By gathering information from a variety of sources, including my own memories, written accounts, and oral histories, I hope to shed new light on this important period in Middle-earth’s history and provide valuable insights into the motivations and actions of the various players involved.\n\nView source on GitHub"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Welcome to my statistics related blog posts.\n\n\n\n\n  \n\n\n\n\nNumber of subsamples, that is sampling\n\n\n\n\n\n\n\nBlog\n\n\nSubsampling\n\n\nBig data\n\n\n\n\n\n\n\n\n\n\n\nFebruary 5, 2023\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n  \n\n\n\n\nTree of Binomial Distribution\n\n\n\n\n\n\n\nBinomial\n\n\nBlog\n\n\nfitODBOD\n\n\n\n\n\n\n\n\n\n\n\nFebruary 14, 2019\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n  \n\n\n\n\nBenchmarking the mle and mle2 function\n\n\n\n\n\n\n\nbbmle\n\n\nfitODBOD\n\n\nmle\n\n\nmle2\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nDecember 29, 2018\n\n\n\n\n\n\n  \n\n\n\n\nBenchmarking the maxLik function\n\n\n\n\n\n\n\nBetaBinomial\n\n\nfitODBOD\n\n\nmaxLik\n\n\n\n\n\n\n\n\n\n\n\nDecember 20, 2018\n\n\n\n\n\n\n  \n\n\n\n\nBenchmarking the optim function\n\n\n\n\n\n\n\nfitODBOD\n\n\noptim\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nDecember 14, 2018\n\n\n\n\n\n\n  \n\n\n\n\nBenchmarking optimization functions in R\n\n\n\n\n\n\n\nbbmle\n\n\nBeta-Binomial\n\n\nfitODBOD\n\n\nmaxLik\n\n\nmle\n\n\nmle2\n\n\nnlm\n\n\nnlminb\n\n\noptim\n\n\nucminf\n\n\n\n\n\n\n\n\n\n\n\nDecember 4, 2018\n\n\nM.Amalan\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nExtract Presidential Election Data of 2015 from the Pdf file\n\n\n\n\n\n\n\nSL Election\n\n\nPresidential Election\n\n\n\n\n\n\n\n\n\n\n\nJuly 14, 2019\n\n\n\n\n\n\n  \n\n\n\n\nBuild Your Own Website or Blog Using R, RStudio and R Packages.\n\n\n\n\n\n\n\nR\n\n\nWebsite\n\n\nBlog\n\n\n\n\n\n\n\n\n\n\n\nJanuary 28, 2019\n\n\n\n\n\n\n  \n\n\n\n\nOlympic : Rshiny Approach\n\n\n\n\n\n\n\nRshiny\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJanuary 24, 2019\n\n\n\n\n\n\n  \n\n\n\n\nDeveloping an R package\n\n\n\n\n\n\n\nfitODBOD\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJanuary 10, 2019\n\n\n\n\n\n\n  \n\n\n\n\nBuild a New Package with Existing R packages\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nDecember 23, 2018\n\n\n\n\n\n\n  \n\n\n\n\nHow To Find Your R package ?\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nDecember 22, 2018\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Developing/southpark_Rshiny/index.html",
    "href": "posts/Developing/southpark_Rshiny/index.html",
    "title": "South Park Text Analytics Shiny App",
    "section": "",
    "text": "GitHub Code Link for the Rshiny App"
  },
  {
    "objectID": "posts/Developing/southpark_Rshiny/index.html#no-inputs-needed-to-generate-plots-or-results",
    "href": "posts/Developing/southpark_Rshiny/index.html#no-inputs-needed-to-generate-plots-or-results",
    "title": "South Park Text Analytics Shiny App",
    "section": "No Inputs needed to generate Plots or Results",
    "text": "No Inputs needed to generate Plots or Results\nPlots generated here are to summarize so far how the South park season 1 to 22 has changed. In terms of Swear words, stop words, all words, sentiment and much more. If we focus on sentiment analysis it is more clear when you read documents related to AFINN, bing and nrc.\n\nTrivia Sub Tab\nThis tab includes information in plots mainly generated by plotly with some memes from South Park. So patiently wait until they load, you can scroll through the page and read stuff.\n\n\nLines Sub Tab\nSummarized information for number of lines with relative to seasons, characters and episodes will be plotted here.\n\n\nWords Sub Tab\nSummarized information for number of words with relative to seasons, characters and episodes will be plotted here.\n\n\nSpecial Words Sub Tab\nSummarized information for words, words without stop words, swear words with relative to seasons, characters and episodes will be plotted here.\n\n\nRatings and Votes from IMDB Sub Tab\nData from southparkr package related to ratings and votes of IMDB will be used to generate plots in this tab. There are two animated plots and it might take some time to generate also therefore patiently wait.\n\n\nSentiment Analysis Sub Tab\nAs above mentioned here also there will be plots related sentiment analysis. Which are related to AFINN, bing and nrc techniques.\n\n\nBigram and Trigram Analysis Sub Tab\nThis is something rare to be useful and time consuming, but will still generate plots. Therefore patiently wait until the plots are done to be view-able."
  },
  {
    "objectID": "posts/Developing/southpark_Rshiny/index.html#inputs-needed-from-the-user-to-generate-plots-and-results",
    "href": "posts/Developing/southpark_Rshiny/index.html#inputs-needed-from-the-user-to-generate-plots-and-results",
    "title": "South Park Text Analytics Shiny App",
    "section": "Inputs needed from the user to generate Plots and Results",
    "text": "Inputs needed from the user to generate Plots and Results\nInputs from user where they can choose their own will be used to generate plots under these several tabs. Below generated are only plots but nothing more. These comparisons are mainly about Most number of lines, Most number of words, Most number of words without stop words, Swear words and sentiment analysis.\nSentiment Analysis is related to AFINN, bing and nrc techniques. Further, nrc has subgroups which are related to 10 different emotions. While AFINN and bing has only two emotions which are positive and negative.\n\nCompare Two Seasons Tab.\nTwo seasons of the users choice will be used to generate plots. It will take some time so patiently wait until they are plotted. Where all the users who were from those two seasons.\n\n\nCompare Two Characters Tab.\nTwo Characters of the users choice will be used to generate plots. It will take some time so patiently wait until they are plotted. All the seasons which they were active.\n\n\nCompare Two Characters but Same Season Tab.\nTwo Characters of the Same Season will be used to generate plots. It will take some time so patiently wait until they are plotted. Here, we will not consider characters which were active in this particular season, rather all Characters which were active throughout all seasons. Therefore sometimes their might not be meaningful plots.\n\n\nCompare Two Seasons but Same Character Tab.\nTwo Seasons of the Same Character will be used to generate plots. It will take some time so patiently wait until they are plotted. Here also we are not considering if the chosen character by the user is active in the chosen two seasons of choice. Therefore sometimes we might not generate meaningful plots."
  },
  {
    "objectID": "posts/Developing/southpark_Rshiny/index.html#about-the-author-tab.",
    "href": "posts/Developing/southpark_Rshiny/index.html#about-the-author-tab.",
    "title": "South Park Text Analytics Shiny App",
    "section": "About the Author Tab.",
    "text": "About the Author Tab.\nIf you Click this, it will automatically open a tab which will lead you to my personal website, which is this one.\nTHANK YOU"
  },
  {
    "objectID": "posts/Developing/PersonalWebsite/index.html",
    "href": "posts/Developing/PersonalWebsite/index.html",
    "title": "Build Your Own Website or Blog Using R, RStudio and R Packages.",
    "section": "",
    "text": "R enthusiasts are focused on developing packages and websites to promote their profile and share their knowledge to the world. Recently released packages such as hugo, blogdown, Rmarkdown, bookdown has played a significant amount of role in this popularity for R statistical software among general users and academics in every field of statistics.\nDue to this reason, I also wanted to develop my own R package to solve problem in hand and share it with the #rstats community. Even though Social Media is a strong way of sharing this amount of information, it is not sturdy over time. To resolve this only I chose to develop my own website using R and supportive tools from R. You are reading this post on my website which I developed in a very short period of time and have being maintaining regularly by posting articles."
  },
  {
    "objectID": "posts/Developing/PersonalWebsite/index.html#making-websites-with-rmarkdown-and-blogdown-by-yihui-xie.",
    "href": "posts/Developing/PersonalWebsite/index.html#making-websites-with-rmarkdown-and-blogdown-by-yihui-xie.",
    "title": "Build Your Own Website or Blog Using R, RStudio and R Packages.",
    "section": "“Making Websites with Rmarkdown and Blogdown by Yihui Xie.”",
    "text": "“Making Websites with Rmarkdown and Blogdown by Yihui Xie.”\nLink\n\nPresentation of 20 slides.\nMost of the basic information for packages which are necessary for website development.\nBrief introduction about the website structure and process."
  },
  {
    "objectID": "posts/Developing/PersonalWebsite/index.html#up-and-running-with-blogdown-by-alison-presmanes-hill.",
    "href": "posts/Developing/PersonalWebsite/index.html#up-and-running-with-blogdown-by-alison-presmanes-hill.",
    "title": "Build Your Own Website or Blog Using R, RStudio and R Packages.",
    "section": "“Up and Running with Blogdown by Alison Presmanes Hill.”",
    "text": "“Up and Running with Blogdown by Alison Presmanes Hill.”\nLink\n\nBrief information for blogdown and other development materials.\nDescribed information Deployment and maintaining the website with other tools related to R and Rstudio."
  },
  {
    "objectID": "posts/Developing/PersonalWebsite/index.html#rmarkdown-websites.",
    "href": "posts/Developing/PersonalWebsite/index.html#rmarkdown-websites.",
    "title": "Build Your Own Website or Blog Using R, RStudio and R Packages.",
    "section": "“Rmarkdown Websites.”",
    "text": "“Rmarkdown Websites.”\nLink\n\nBriefest description about using Rmarkdown/Rmd files for website development.\nThere are few other links which could be considered useful."
  },
  {
    "objectID": "posts/Developing/PersonalWebsite/index.html#how-to-make-an-rmarkdown-website-by-nick-strayer-lucy-dagostino-mcgowan.",
    "href": "posts/Developing/PersonalWebsite/index.html#how-to-make-an-rmarkdown-website-by-nick-strayer-lucy-dagostino-mcgowan.",
    "title": "Build Your Own Website or Blog Using R, RStudio and R Packages.",
    "section": "“How to make an RMarkdown Website by Nick Strayer & Lucy D’Agostino McGowan.”",
    "text": "“How to make an RMarkdown Website by Nick Strayer & Lucy D’Agostino McGowan.”\nLink\n\nOne example sample website developed and explained briefly by the authors.\nSeveral links to spark curiosity about website development using Rmarkdown."
  },
  {
    "objectID": "posts/Developing/PersonalWebsite/index.html#creating-websites-in-r-by-emily-c-zabor.",
    "href": "posts/Developing/PersonalWebsite/index.html#creating-websites-in-r-by-emily-c-zabor.",
    "title": "Build Your Own Website or Blog Using R, RStudio and R Packages.",
    "section": "“Creating websites in R by Emily C Zabor.”",
    "text": "“Creating websites in R by Emily C Zabor.”\nLink\n\nExplanation on different type of websites which can be produced by R.\nDeployment and additional requirements for website development."
  },
  {
    "objectID": "posts/Developing/PersonalWebsite/index.html#getting-started-with-blogdown-by-david-selby.",
    "href": "posts/Developing/PersonalWebsite/index.html#getting-started-with-blogdown-by-david-selby.",
    "title": "Build Your Own Website or Blog Using R, RStudio and R Packages.",
    "section": "“Getting Started with Blogdown by David Selby.”",
    "text": "“Getting Started with Blogdown by David Selby.”\nLink\n\nLimited amount of information regarding website development.\nThis was written for a talk for the “Warwick R User Group Talk” in 2017."
  },
  {
    "objectID": "posts/Developing/PersonalWebsite/index.html#getting-started-with-blogdown-by-danielle-navarro.",
    "href": "posts/Developing/PersonalWebsite/index.html#getting-started-with-blogdown-by-danielle-navarro.",
    "title": "Build Your Own Website or Blog Using R, RStudio and R Packages.",
    "section": "“Getting Started with Blogdown by Danielle Navarro.”",
    "text": "“Getting Started with Blogdown by Danielle Navarro.”\nLink\n\nExtensive amount of information about website development using blogdown.\nMore than enough information about the insides of the website.\nDetailed steps of writing posts and changing elements of the website."
  },
  {
    "objectID": "posts/Developing/PersonalWebsite/index.html#blogdown-by-peters-blog.",
    "href": "posts/Developing/PersonalWebsite/index.html#blogdown-by-peters-blog.",
    "title": "Build Your Own Website or Blog Using R, RStudio and R Packages.",
    "section": "“Blogdown by Peter’s Blog.”",
    "text": "“Blogdown by Peter’s Blog.”\nLink\n\n4 Tutorials explaining from scratch about how to develop your website using blogdown.\nTutorial 1 explains about website themes and setting up R and Rstudio.\nTutorial 2 is about hosting the website locally or sharing the work with others.\nTutorial 3 will be information about getting the site live.\nTutorial 4 describes how to bring the website online through Netlify or GitHub."
  },
  {
    "objectID": "posts/Developing/PersonalWebsite/index.html#academic-by-george-cushen",
    "href": "posts/Developing/PersonalWebsite/index.html#academic-by-george-cushen",
    "title": "Build Your Own Website or Blog Using R, RStudio and R Packages.",
    "section": "“Academic by George Cushen”",
    "text": "“Academic by George Cushen”\nLink\n\nAll the information about the academic theme this is very popular among people in rstats community.\nDescriptive amount of information about changing elements in the academic theme to make it more homely fo the user."
  },
  {
    "objectID": "posts/Developing/PersonalWebsite/index.html#happy-git-and-github-for-the-user.",
    "href": "posts/Developing/PersonalWebsite/index.html#happy-git-and-github-for-the-user.",
    "title": "Build Your Own Website or Blog Using R, RStudio and R Packages.",
    "section": "“Happy Git and GitHub for the User.”",
    "text": "“Happy Git and GitHub for the User.”\nLink\n\nGit and GitHub with R with all the information that anyone needs to know."
  },
  {
    "objectID": "posts/Developing/PersonalWebsite/index.html#rmarkdown-the-definitive-guide-by-yihui-xie-j.-j.-allaire-garrett-grolemund.",
    "href": "posts/Developing/PersonalWebsite/index.html#rmarkdown-the-definitive-guide-by-yihui-xie-j.-j.-allaire-garrett-grolemund.",
    "title": "Build Your Own Website or Blog Using R, RStudio and R Packages.",
    "section": "“RMarkdown : The Definitive Guide by Yihui Xie, J. J. Allaire, Garrett Grolemund.”",
    "text": "“RMarkdown : The Definitive Guide by Yihui Xie, J. J. Allaire, Garrett Grolemund.”\nLink\n\nBook with all the information related to Rmarkdown files.\nNo need to look anywhere for clarification regarding Rmarkdown."
  },
  {
    "objectID": "posts/Developing/PersonalWebsite/index.html#blogdown-creating-websites-with-r-markdown-by-yihui-xie-amber-thomas-alison-presmanes-hill.",
    "href": "posts/Developing/PersonalWebsite/index.html#blogdown-creating-websites-with-r-markdown-by-yihui-xie-amber-thomas-alison-presmanes-hill.",
    "title": "Build Your Own Website or Blog Using R, RStudio and R Packages.",
    "section": "“blogdown: Creating Websites with R Markdown by Yihui Xie, Amber Thomas, Alison Presmanes Hill.”",
    "text": "“blogdown: Creating Websites with R Markdown by Yihui Xie, Amber Thomas, Alison Presmanes Hill.”\nLink\n\nSimilar to the previous two books this is also the most useful for blogdown.\nNo need to look anywhere else for further understanding regarding blogdown."
  },
  {
    "objectID": "posts/Developing/Newpackage/index.html",
    "href": "posts/Developing/Newpackage/index.html",
    "title": "Build a New Package with Existing R packages",
    "section": "",
    "text": "Package development is a sense of accomplishment for any statistical programmer who needs self satisfaction. I developed the R package fitODBOD for the purpose of fitting Over dispersed Binomial Outcome Data using Binomial Mixture Distributions and Alternate Binomial Distributions. It was a an amazing journey learning how to develop an R package, which took me around 6 months while understanding the theoretical aspects of my research project and doing my 4th year courses.\nI am still learning new things related to R, which is helpful for this R package development. Making package version updates regularly is for the benefit of the user. I have learned new ways to express the theoretical concepts in the simplest form of functions, classes and methods. Currently, I am exploring the possibility of using Rshiny dashboard and GUI.\nIn the beginning, R package developers have used manual techniques (which mean difficult techniques)\nto develop R functions, documentation and examples for their packages. Over time it has changed rapidly, where currently we are using R packages to develop our own R package. In this post I shall briefly mention these packages which you can use. Using these packages it is possible to make package development stress free, time efficient and objective effective. Simultaneously we can make our R packages more attractive for the users, which would lead to lot of attention in the R community.\nThere are three types of packages in my perspective, first “Most Essential Packages” which cannot be ignored, second “Essential packages” it is your choice to ignore and finally, “Still I have not Used” packages."
  },
  {
    "objectID": "posts/Developing/Newpackage/index.html#devtools",
    "href": "posts/Developing/Newpackage/index.html#devtools",
    "title": "Build a New Package with Existing R packages",
    "section": "devtools",
    "text": "devtools\nCollection of packages which would significantly help the package development process. Functions such as dev_mode, check_failures, check_win and check_man.\nLink for the package\n\npkgbuild\nLocates compilers needed to build R packages on various platforms.\nLink for the package\n\n\npkgload\nSimulate the process of installing a package and then attacking it.\nLink for the package\n\n\nrcmdcheck\nRun “R CMD check” from R programmaticallly, and capture the results of the individual checks.\nLink for the package\n\n\nusethis\nAutomating few tasks related to package building.\nLink for the package"
  },
  {
    "objectID": "posts/Developing/Newpackage/index.html#roxygen2",
    "href": "posts/Developing/Newpackage/index.html#roxygen2",
    "title": "Build a New Package with Existing R packages",
    "section": "roxygen2",
    "text": "roxygen2\nGenerate Rd documentation, and Namespace file with simplicity which would save time, when package update occurs.\nLink for the package\n\nknitr\nUseful to develop vignettes related to R package development.\nLink for the package\n\n\nmarkdown, rmarkdown, rmdformats\nHtml formats to vignettes in R package development and special template styles for the vignettes.\nmarkdown rmarkdown rmdformats\n\n\nspelling\nChecking for spelling issues in Rd documentation files.\nLink for the package\n\n\ntrackmd\nTracking changes in markdown files for vignette.\nLink for the package"
  },
  {
    "objectID": "posts/Developing/Newpackage/index.html#testthat",
    "href": "posts/Developing/Newpackage/index.html#testthat",
    "title": "Build a New Package with Existing R packages",
    "section": "testthat",
    "text": "testthat\nChecking if functions work properly by testing them in multiple ways for errors, outputs and inputs.\nLink for the package"
  },
  {
    "objectID": "posts/Developing/Newpackage/index.html#git2r-and-gh",
    "href": "posts/Developing/Newpackage/index.html#git2r-and-gh",
    "title": "Build a New Package with Existing R packages",
    "section": "git2r and gh",
    "text": "git2r and gh\nAccess to GitHub so that version control would occur smoothly with integration in Rstudio.\ngit2r gh"
  },
  {
    "objectID": "posts/Developing/Newpackage/index.html#desc",
    "href": "posts/Developing/Newpackage/index.html#desc",
    "title": "Build a New Package with Existing R packages",
    "section": "desc",
    "text": "desc\nEditing the Description file using package rather than manually editing the file.\nLink to package"
  },
  {
    "objectID": "posts/Developing/Newpackage/index.html#covr",
    "href": "posts/Developing/Newpackage/index.html#covr",
    "title": "Build a New Package with Existing R packages",
    "section": "covr",
    "text": "covr\nChecking code coverage, which means does all functions have examples and are there tests for error messages, etc.\nLink to package"
  },
  {
    "objectID": "posts/Developing/Newpackage/index.html#badgecreatr-and-badger",
    "href": "posts/Developing/Newpackage/index.html#badgecreatr-and-badger",
    "title": "Build a New Package with Existing R packages",
    "section": "badgecreatr and badger",
    "text": "badgecreatr and badger\nAdding badges to GitHub repository, for example Download, CRAN status, code coverage, Release date, version and much more.\nbadgecreatr badger"
  },
  {
    "objectID": "posts/Developing/Newpackage/index.html#hexsticker",
    "href": "posts/Developing/Newpackage/index.html#hexsticker",
    "title": "Build a New Package with Existing R packages",
    "section": "hexSticker",
    "text": "hexSticker\nCreating a hexagon sticker for your package. Mostly just for the fun, but in a while its like promoting a brand.\nLink to package"
  },
  {
    "objectID": "posts/Developing/Newpackage/index.html#pkgdown",
    "href": "posts/Developing/Newpackage/index.html#pkgdown",
    "title": "Build a New Package with Existing R packages",
    "section": "pkgdown",
    "text": "pkgdown\nUsing man files, vignette of your package to develop a static website. Further, it is possible to promote this site to get more people interested in the package.\nLink to package"
  },
  {
    "objectID": "posts/Developing/Newpackage/index.html#packrat",
    "href": "posts/Developing/Newpackage/index.html#packrat",
    "title": "Build a New Package with Existing R packages",
    "section": "packrat",
    "text": "packrat\nManage the R packages in an isolated, portable and reproducible way.\nLink to package"
  },
  {
    "objectID": "posts/Developing/Newpackage/index.html#pkgconfig",
    "href": "posts/Developing/Newpackage/index.html#pkgconfig",
    "title": "Build a New Package with Existing R packages",
    "section": "pkgconfig",
    "text": "pkgconfig\nSet configuration options on a per-package basis.\nLink to package"
  },
  {
    "objectID": "posts/Developing/Newpackage/index.html#pkginspector",
    "href": "posts/Developing/Newpackage/index.html#pkginspector",
    "title": "Build a New Package with Existing R packages",
    "section": "pkginspector",
    "text": "pkginspector\nUnderstand internal structure of an R package.\nLink to package"
  },
  {
    "objectID": "posts/Developing/Newpackage/index.html#rvcheck",
    "href": "posts/Developing/Newpackage/index.html#rvcheck",
    "title": "Build a New Package with Existing R packages",
    "section": "rvcheck",
    "text": "rvcheck\nCheck latest release version of R and R packages.\nLink to package"
  },
  {
    "objectID": "posts/Developing/Newpackage/index.html#rversions",
    "href": "posts/Developing/Newpackage/index.html#rversions",
    "title": "Build a New Package with Existing R packages",
    "section": "rversions",
    "text": "rversions\nFocusing on R version ‘r-release’ and ‘r-oldrel’. Further all previous R versions.\nLink to package"
  },
  {
    "objectID": "posts/Developing/Newpackage/index.html#formatr",
    "href": "posts/Developing/Newpackage/index.html#formatr",
    "title": "Build a New Package with Existing R packages",
    "section": "formatR",
    "text": "formatR\nSpaces and Indent for the code automatically added\nLink to package"
  },
  {
    "objectID": "posts/Developing/Newpackage/index.html#whoami",
    "href": "posts/Developing/Newpackage/index.html#whoami",
    "title": "Build a New Package with Existing R packages",
    "section": "whoami",
    "text": "whoami\nUsername and full-name of current user, also email address and GitHub username.\nLink to package"
  },
  {
    "objectID": "posts/Binomial_Distribution/Tree_of_Binomial_Distribution/index.html",
    "href": "posts/Binomial_Distribution/Tree_of_Binomial_Distribution/index.html",
    "title": "Tree of Binomial Distribution",
    "section": "",
    "text": "The binomial distribution can be defined, using the binomial expansion\n\\[ (q+p)^n = \\sum_{x=0}^{n} {n \\choose k} p^k q^{(n-k)} = \\sum_{x=0}^{n} \\frac{n!} {k! (n-k)!} p^k q^{(n-k)}\\]\nas the distribution of a random variable X for which\n\\[Pr[X=x] = {n \\choose k} p^k q^{(n-k)}\\]\n\\(x=0,1,2,...,n.\\) where \\(q+p=1\\), \\(p,q&gt;0\\) and \\(n\\) is a positive integer. When \\(n=1\\), the distribution is known as the Bernoulli distribution. The mean and variance are \\(\\mu=np\\) and \\(\\mu_2 = npq\\).\nIf \\(n\\) independent trials are made and in each there is probability \\(p\\) that the outcome \\(E\\) will occur, then the number of trials in which \\(E\\) occurs can be represented by a random variable \\(X\\) having the binomial distribution with parameters \\(n\\), \\(p\\).\nThis situation occurs when a sample of fixed size \\(n\\) is taken from an infinite population where each element in the population has an “equal” and “independent” probability \\(p\\) of possession of a specified attribute. The situation also arises when a sample of fixed size \\(n\\) is taken from a infinite population where each element in the population has an “equal” and “independent” probability \\(p\\) of having a specified attribute and elements are sampled independently and sequentially with replacement.\nThe distribution was derived by James Bernoulli (in his treatise Ars Conjectandi, published in 1713), for the case \\(p = r/(r+s)\\), where \\(r\\) and \\(s\\) are positive integers.\nThe binomial distribution is of such importance in applied probability and statistics that it is frequently necessary to calculate probabilities based on this distribution. Although the calculation of sums of the form\n\\[ \\sum_{x} {n \\choose x} p^x q^{(n-x)}\\]\nis straightforward, it can be tedious, especially when n and x are large and when there are a large number of terms in the summation. It is not surprising that a great deal of attention and ingenuity have been applied to constructing useful approximations for sums of this kind.\n\n\nThe binomial distribution arises whenever underlying events have two possible outcomes, the chances of which remain constant. The importance of the distribution has extended from its original application in gaming to many other areas.\nIts use in genetics arises because the inheritance of biological characteristics depends on genes that occur in pairs; see, for example, Fisher and Matheras (1936) analysis of data on straight versus wavy hair in mice.\nMore recent application in genetics is the study of the number of nucleotides that are in the same state in two DNA sequences (Kaplan and Risko, 1982).\nThe number of defectives found in random samples of size n from a stable production process is a binomial variable; acceptance sampling is a very important application of the test for the mean of a binomial sample against a hypothetical value.\nSeber (1982b) has given a number of instances of the use of the binomial distribution in animal ecology, for example, in mark-recapture estimation of the size of an animal population. Boswell, Ord, and Patil (1979) gave applications in plant ecology.\n\nAlthough appealing in their simplicity, the assumptions of independence and constant probability for the binomial distribution are not often precisely satisfied. Published critical appraisals of the extent of departure from these assumptions in actual situations are rather rare."
  },
  {
    "objectID": "posts/Binomial_Distribution/Tree_of_Binomial_Distribution/index.html#applications",
    "href": "posts/Binomial_Distribution/Tree_of_Binomial_Distribution/index.html#applications",
    "title": "Tree of Binomial Distribution",
    "section": "",
    "text": "The binomial distribution arises whenever underlying events have two possible outcomes, the chances of which remain constant. The importance of the distribution has extended from its original application in gaming to many other areas.\nIts use in genetics arises because the inheritance of biological characteristics depends on genes that occur in pairs; see, for example, Fisher and Matheras (1936) analysis of data on straight versus wavy hair in mice.\nMore recent application in genetics is the study of the number of nucleotides that are in the same state in two DNA sequences (Kaplan and Risko, 1982).\nThe number of defectives found in random samples of size n from a stable production process is a binomial variable; acceptance sampling is a very important application of the test for the mean of a binomial sample against a hypothetical value.\nSeber (1982b) has given a number of instances of the use of the binomial distribution in animal ecology, for example, in mark-recapture estimation of the size of an animal population. Boswell, Ord, and Patil (1979) gave applications in plant ecology.\n\nAlthough appealing in their simplicity, the assumptions of independence and constant probability for the binomial distribution are not often precisely satisfied. Published critical appraisals of the extent of departure from these assumptions in actual situations are rather rare."
  },
  {
    "objectID": "posts/Binomial_Distribution/Tree_of_Binomial_Distribution/index.html#other-with-no-sub-groups",
    "href": "posts/Binomial_Distribution/Tree_of_Binomial_Distribution/index.html#other-with-no-sub-groups",
    "title": "Tree of Binomial Distribution",
    "section": "Other With No Sub-Groups",
    "text": "Other With No Sub-Groups\nFirst sub group in discussion is “Other With No Sub-Groups”, which is an abbreviation for Binomial distributions which were direcly created for the purpose of satisfying specific real world situations and theoretical concepts.\n\nDiagrammeR(\"graph TB;\n            B[Binomial &lt;br&gt; Distribution]--&gt;A[Other with &lt;br&gt; no sub groups];\n            A--&gt;D[Grassia Binomial &lt;br&gt; Distribution];\n            A--&gt;E[Zero Modified &lt;br&gt; Binomial Distribution];\n            A--&gt;F[Dandekar's Modified &lt;br&gt; Binomial Distribution];\n            A--&gt;G[Simplex Binomial &lt;br&gt; Mixture Model];\n            A--&gt;H[Double Binomial &lt;br&gt; Distribution];\n            A--&gt;I[Finite Binomial &lt;br&gt; Mixtures];\n            A--&gt;J[Binomial Distribution &lt;br&gt; of order K];\n            A--&gt;K[Truncated Binomial &lt;br&gt; Distribution];\n            A--&gt;L[Weighted Binomial &lt;br&gt; Distribution];\",width=1000,height=400)\n\nBelow is the list of Distributions in this sub group with article references.\nGrassia Binomial Distribution\nKemp, A. W., and Kemp, C. D (2004). Factorial moment characterizations for certain binomial-type distributions, Communications in Statistics-Theory and Methods, 33, 3059-3068.\nHarkness, W. L. (1970). The classical occupancy problem revisited, Random Counts in Scientific Work, Vol. 3:Random Counts in Physical Science, Geo Science, and Business, G. P. Patil (editor), 107-126. University Park: Pennsylvania State University Press.\nWeiss, G. H. (1965). A model for the spread of epidemics by carriers, Biometrics, 21, 481-490.\nDietz, K. (1966). On the model of Weiss for the spread of epidemics by carriers, Journal of Applied Probability, 3, 375-382.\nDownton, F. (1967). Epidemics with carriers: A note on a paper by Dietz, Journal of Applied Probability, 4, 264-270.\nDaley, D. J., and Gani, J. (1999). Epidemic Modelling:An Introduction,Cambridge: Cambridge University Press.\nGrassia, A. (1977). On a family of distributions with argument between 0 and 1 obtained by transformation of the gamma and derived compound distributions, Australian Journal of Statistics, 19, 108-114.\nAlanko, T., and Duffy, J. C. (1996). Compound binomial distributions for modelling consumption data, The Statistician, 45, 269-286.\nChatfield, C., and Goodhardt, G. J. (1970). The beta-binomial model for consumer purchasing behaviour, Applied Statistics, 19, 240-250.\nConsul, P. C., and Jain, G. C. (1971). On the log-gamma distribution and its properties, Statistische Hefte, 12, 100-106.\nZero Modified Binomial Distribution\nDowling, M. M., and Nakamura, M. (1997). Estimating parameters for discrete distributions via the empirical probability generating function, Communications in Statistics-Simulation and Computation, 26, 301-313.\nKhatri, C. G. (1961). On the distributions obtained by varying the number of trials in a binomial distribution, Annals of the Institute of Statistical Mathematics, Tokyo, 13, 47-51.\nDandekar’s Modified Binomial Distribution\nPatil, G. P., Boswell, M. T., Joshi, S. W., and Ratnaparkhi, M. V. (1984). Dictionary and Bibliography of Statistical Distributions in Scientific Work, Vol. 1: Discrete Models, Fairland, MD: International Co-operative Publishing House.\nDandekar, V. M. (1955). Certain modified forms of binomial and Poisson distributions, Sankhya, 15, 237-250.\nSimplex Binomial Mixture Model\nBarndorff-Neilsen, O. E., and Jorgensen, B. (1991). Some parametric models on the simplex, Journal of Multivariate Analysis, 39, 106-116.\nJorgensen, B. (1997). The Theory of Regression Models, London: Chapman & Hall/CRC.\nDouble Binomial Distribution\nLindsey, J. K. (1995). Modelling Frequency and Count Data, Oxford: Oxford University Press.\nEfron, B. (1986). Double exponential families and their use in generalized linear regression, Journal of the American Statistical Association, 81, 709-721.\nFinite Biomial Mixtures\nTeicher, H. (1961). Identifiability of mixtures, Annals of Mathematical Statistics, 32, 244-248.\nBlischke, W. R. (1962). Moment estimation for the parameters of a mixture of two binomial distributions, Annals of Mathematical Statistics, 33, 444-454.\nBlischke, W. R. (1964). Estimating the parameters of mixtures of binomial distributions, Journal of the American Statistical Association, 59, 510-528.\nBlischke, W. R. (1965). Mixtures of discrete distributions, Classical and Contagious Discrete Distributions, G. P. Patil (editor), 351-372. Calcutta: Statistical Publishing Society; Oxford: Pergamon.\nEveritt, B. S., and Hand, D. J. (1981). Finite Mixture Distributions, London: Chapman & Hall.\nBondesson, L. (1988). On the gain by spreading seeds: A statistical analysis of sowing experiments, Scandinavian Journal of Forest Research, 305-314.\nGelfand, A. E., and Soloman, H. (1975). Analysing the decision making process of the American jury, Journal of the American Statistical Association, 70, 305-310.\nHasselblad, V. (1969). Estimation of finite mixtures of distributions from the exponential family, Journal of the American Statistical Association, 64, 1459-1471.\nRider, P. R. (1962a). Estimating the parameters of mixed Poisson, binomial and Weibull distributions, Bulletin of the International Statistical Institute, 39(2), 225-232.\nBinomial Distribution of order K\nLing, K. D. (1988). On binomial distributions of order k, Statistics and Probability Letters, 6, 371-376.\nShanthikumar, J. G. (1985). Discrete random variate generation using uniformization, European Journal of Operational Research, 21, 387-398.\nChiang, D., and Niu, S. C. (1981). Reliability of consecutive-k-out-of-n:F systems, IEEE Transactions on Reliability, R-30, 87-89.\nBollinger, R. C., and Salvia, A. A. (1982). Consecutive-k-out-of-n: F networks, IEEE Transactions on Reliability, R-31, 53-55.\nHirano, K. (1986). Some properties of the distributions of order k, Fibonacci Numbers and Their Applications, A. N. Philippou, G. E. Bergum, and A. F. Horadam (editors), 43-53. Dordrecht: Reidel.\nPhilippou, A. N., and Makri, F. S. (1986). Success runs and longest runs, Statistics and Probability Letters, 4, 101-105 (corrected version 211-215).\nFeller, W. (1957). An Introduction to Probability Theory and Its Applications (second edition), Vol. 1, New York: Wiley.\nAki, S., and Hirano, K. (1988). Some characteristics of the binomial distribution of order k and related distributions, Statistical Theory and Data Analysis, Vol. 2, K. Matusita (editor), 211-222. Amsterdam: Elsevier.\nTruncated Binomial Distribution\nStephan, F. F. (1945). The expected value and variance of the reciprocal and other negative powers of a positive Bernoullian variate, Annals of Mathematical Statistics, 16, 50-61.\nShah, S. M. (1966). On estimating the parameter of a doubly truncated binomial distribution, Journal of the American Statistical Association, 61, 259-263.\nNewell, D. J. (1965). Unusual frequency distributions, Biometrics, 21, 159-168.\nGrab, E. L., and Savage, I. R. (1954). Tables of the expected value of 1/x for positive Bernoulli and Poisson variables, Journal of the American Statistical Association, 49, 169-177.\nFinney, D. J. (1949). The truncated binomial distribution, Annals of Eugenics, London, 14, 319-328.\nWeighted Binomial Distribution\nPatil, G. P., Rao, C. R., and Zelen, M. (1986). A Computerized Bibliography of Weighted Distributions and Related Weighted Methods for Statistical Analysis and Interpretations of Encountered Data, Observational Studies, Representativeness Issues, and Resulting Inferences, University Park, PA: Centre for Statistical Ecology and Environmental Statistics, Pennsylvania State University.\nPatil, G. P., Rao, C. R., and Ratnaparkhi, M. V. (1986). On discrete weighted distributions and their use in model choice for observed data, Communications in Statistics-Theory and Methods, 15, 907-918.\nRao, C. R. (1965). On discrete distributions arising out of methods of ascertainment, Classical and Contagious Discrete Distributions,G. P Patil (editor), 320-332. Calcutta: Statistical Publishing Society; Oxford: Pergamon. (Republished Sankhya, A27, 1965, 311-324.)\nRao, C. R. (1985). Weighted distributions arising out of methods of ascertainment: What populations does a sample represent?, A Celebration of Statistics: ISI Centenary Volume, A. C. Atkinson and S. E. Fienberg (editors), 543-569. New York: SpringerVerlag."
  },
  {
    "objectID": "posts/Binomial_Distribution/Tree_of_Binomial_Distribution/index.html#binomial-not-parent",
    "href": "posts/Binomial_Distribution/Tree_of_Binomial_Distribution/index.html#binomial-not-parent",
    "title": "Tree of Binomial Distribution",
    "section": "Binomial Not Parent",
    "text": "Binomial Not Parent\nThis is the sub group where Binomial Distribution is not the parent but rather some other different distribution like Hypergeometric or Negative Binomial or Poisson.\n\nDiagrammeR(\"graph TB;\n            A[Binomial &lt;br&gt; Distribution]--&gt;B[Binomial Not Parent];\n            B--&gt;BA[Hypergeometric &lt;br&gt; + Binomial];\n            B--&gt;BB[Negative Binomial &lt;br&gt; + Binomial];\n            B--&gt;BC[Poisson + &lt;br&gt; Binomial];\",width=800,height=400\n)\n\nHypergeometric + Binomial\n\\[Hypergeometric (n,Y,N) \\bigwedge_Y Binomial(N,p)\\]\nNegative Binomial + Binomial\n\\[Negative Binomial (kY,P) \\bigwedge_Y Binomial(n,p)\\]\nPoisson + Binomial\n\\[ Poisson (\\theta) \\bigwedge_{\\theta/\\phi} Binomial(n,p)\\]"
  },
  {
    "objectID": "posts/Binomial_Distribution/Tree_of_Binomial_Distribution/index.html#alternate-binomial-distributions",
    "href": "posts/Binomial_Distribution/Tree_of_Binomial_Distribution/index.html#alternate-binomial-distributions",
    "title": "Tree of Binomial Distribution",
    "section": "Alternate Binomial Distributions",
    "text": "Alternate Binomial Distributions\nThis is a sub group of distributions which can be used as an alternative to Binomial Distribution.\n\nDiagrammeR(\"graph TB;\n            A[Binomial &lt;br&gt; Distribution]--&gt;B[Alternate Binomial &lt;br&gt; Distribution];\n            B--&gt;C[Additive Binomial &lt;br&gt; Distribution];\n            B--&gt;D[Beta-Correlated &lt;br&gt; Binomial Distribution];\n            B--&gt;E[COM-Poisson Binomial &lt;br&gt; Distribution];\n            B--&gt;F[Correlated Binomial &lt;br&gt; Distribution];\n            B--&gt;G[Multiplicative Binomial &lt;br&gt; Distribution];\",width=800,height=400\n)\n\nAdditive Binomial Distribution\nJohnson, N. L., Kemp, A. W., & Kotz, S. (2005). Univariate discrete distributions (Vol. 444). Hoboken, NJ: Wiley-Interscience.\nL. L. Kupper, J.K.H., 1978. The Use of a Correlated Binomial Model for the Analysis of Certain Toxicological Experiments. Biometrics, 34(1), pp.69-76.\nPaul, S.R., 1985. A three-parameter generalization of the binomial distribution. Communications in Statistics - Theory and Methods, 14(6), pp.1497-1506.\nBeta-Correlated Binomial Distribution\nPaul, S.R., 1985. A three-parameter generalization of the binomial distribution. Communications in Statistics - Theory and Methods, 14(6), pp.1497-1506.\nCOM-Poisson Binomial Distribution\nBorges, P., Rodrigues, J., Balakrishnan, N. and Bazan, J., 2014. A COM-Poisson type generalization of the binomial distribution and its properties and applications. Statistics & Probability Letters, 87, pp.158-166.\nCorrelated Binomial Distribution\nJohnson, N. L., Kemp, A. W., & Kotz, S. (2005). Univariate discrete distributions (Vol. 444). Hoboken, NJ: Wiley-Interscience.\nL. L. Kupper, J.K.H., 1978. The Use of a Correlated Binomial Model for the Analysis of Certain Toxicological Experiments. Biometrics, 34(1), pp.69-76.\nPaul, S.R., 1985. A three-parameter generalization of the binomial distribution. Communications in Statistics - Theory and Methods, 14(6), pp.1497-1506.\nJorge G. Morel and Nagaraj K. Neerchal. Overdispersion Models in SAS. SAS Institute, 2012.\nMultiplicative Binomial Distribution\nJohnson, N. L., Kemp, A. W., & Kotz, S. (2005). Univariate discrete distributions (Vol. 444). Hoboken, NJ: Wiley-Interscience.\nL. L. Kupper, J.K.H., 1978. The Use of a Correlated Binomial Model for the Analysis of Certain Toxicological Experiments. Biometrics, 34(1), pp.69-76.\nPaul, S.R., 1985. A three-parameter generalization of the binomial distribution. Communications in Statistics - Theory and Methods, 14(6), pp.1497-1506."
  },
  {
    "objectID": "posts/Binomial_Distribution/Tree_of_Binomial_Distribution/index.html#neyman-type-a-distribution",
    "href": "posts/Binomial_Distribution/Tree_of_Binomial_Distribution/index.html#neyman-type-a-distribution",
    "title": "Tree of Binomial Distribution",
    "section": "Neyman Type A Distribution",
    "text": "Neyman Type A Distribution\n\nDiagrammeR(\"graph TB;\n            A[Binomial &lt;br&gt; Distribution]--&gt;B[ Neyman Type A &lt;br&gt; Distribution];\n            B--&gt;C[Poisson + Binomial +  Beta];\n            B--&gt;D[Poisson + Poisson + Binomial + Beta &lt;br&gt; or Poisson + Binomial + Poisson + Beta];\n            B--&gt;E[Poisson + Binomial + Poisson + Beta &lt;br&gt; or Binomial + Poisson + Poisson + Beta];\",width=800,height=400\n)\n\nPoisson + Binomial + Beta\n\\[Poisson(\\phi) \\bigvee Binomial(1,P) \\bigwedge_P Beta(\\alpha,\\beta)\\]\nPoisson + Poisson + Binomial + Beta or Poisson + Binomial + Poisson + Beta\n\\[ Poisson(\\lambda) \\bigvee [\\{Poisson(\\phi) \\vee Binomial(1,P) \\} \\bigwedge_P Beta(\\alpha,\\beta)]\\] or \\[ Poisson(\\lambda) \\bigvee [\\{Binomial(M,P) \\bigvee_M Poisson(\\phi) \\} \\bigwedge_P Beta(\\alpha,\\beta)]\\] ### Poisson + Binomial + Poisson + Beta or Binomial + Poisson + Poisson + Beta\n\\[[\\{ Poisson(\\lambda) \\bigvee Binomial(1,P)\\} \\bigvee Poisson(\\phi)] \\bigwedge_P Beta(a,b)\\] or \\[[\\{ Binomial(N,P) \\bigvee Poisson(\\phi)\\} \\bigwedge_N Poisson(\\lambda)] \\bigwedge_P Beta(a,b)\\]\nGurland, J. (1958). A generalized class of contagious distributions, Biometrics, 14, 229-249.\nFeller, W. (1943). On a general class of “contagious” distributions, Annals of Mathematical Statistics, 14, 389-400.\nNeyman, J. (1939). On a new class of “contagious” distributions applicable in entomology and bacteriology, Annals of Mathematical Statistics, 10, 35-57.\nSubrahmaniam, Kocherlakota (1966). On a general class of contagious distributions: The Pascal-Poisson distribution, Trabajos de Estadistica, 17, 109-127.\nSubrahmaniam, Kathleen (1978). The Pascal-Poisson distribution revisited: Estimation and efficiency, Communications in Statistics-Theory and Methods, A7, 673-683."
  },
  {
    "objectID": "posts/Binomial_Distribution/Tree_of_Binomial_Distribution/index.html#hermite-distribution",
    "href": "posts/Binomial_Distribution/Tree_of_Binomial_Distribution/index.html#hermite-distribution",
    "title": "Tree of Binomial Distribution",
    "section": "Hermite Distribution",
    "text": "Hermite Distribution\n\nDiagrammeR(\"graph TB;\n           A[Binomial &lt;br&gt; Distribution]--&gt;B[Hermite &lt;br&gt; Distribution];\n           B--&gt;C[Binomial + Poisson];\n           B--&gt;D[Poisson + Binomial];\",width=800,height=400\n)\n\nBinomial + Poisson\n\\[Binomial(N,p) \\bigwedge_{N/2} Poisson(\\lambda)\\]\nPoisson + Binomial\n\\[Poisson(\\lambda) \\bigvee Binomial(2,p)\\]\nSkellam, J. G. (1952). Studies in statistical ecology I: Spatial pattern, Biometrika, 39, 346-362.\nMcGuire, J. U., Brindley, T. A., and Bancroft, T. A. (1957). The distribution of European corn borer Pyrausta Nubilalis (Hbn.) in field corn, Biometrics, 13, 65-78 [errata and extensions (1958) 14, 432-434].\nKemp, C. D., and Kemp, A. W. (1965). Some properties of the “Hermite” distribution, Biometrika, 52, 381-394.\nFisher, R. A. (1951). Properties of the functions, (Part of introduction to) British Association Mathematical Tables (third edition), Vol. 1, London: British Association."
  },
  {
    "objectID": "posts/Binomial_Distribution/Tree_of_Binomial_Distribution/index.html#binomial-parent",
    "href": "posts/Binomial_Distribution/Tree_of_Binomial_Distribution/index.html#binomial-parent",
    "title": "Tree of Binomial Distribution",
    "section": "Binomial Parent",
    "text": "Binomial Parent\nThis is the sub group where Binomial distribution is the parent distribution. By considering the distribution and its parameters we can use possible different mixing distributions and generate new Binomial Mixture Distributions.\nN/n , K and Y Mixtures\nThe first mixtures with few distributions are mentioned below.\n\nDiagrammeR(\"graph TB;\n           A[Binomial &lt;br&gt; Distribution]--&gt;B[Binomial &lt;br&gt; Parent];\n           B--&gt;C[Mixing Parameter];\n           C--&gt;D[N/n &lt;br&gt; Binomial];\n           D--&gt;DA[Poisson];\n           D--&gt;DB[Binomial];\n           D--&gt;DC[Negative Binomial];\n           D--&gt;DD[Logarithmic];\n           C--&gt;E[K &lt;br&gt; Binomial];\n           E--&gt;EA[Poisson];\n           C--&gt;F[Y &lt;br&gt; Binomial];\n           F--&gt;FA[Hypergeometric];\",width=800,height=400\n)\n\nPoisson Mix for N/n of Binomial distributon\n\\[Binomial(N,p) \\bigwedge_{N/n} Poisson(\\lambda)\\]\nNegative Binomial Mix for N/n of Binomial distributon\n\\[Binomial(N,p) \\bigwedge_{N/n} Negative Binomial(k,P`)\\]\nBinomial Mix for N/n of Binomial distributon\n\\[Binomial(N,p) \\bigwedge_{N/n} Binomial(N`,p`)\\]\nLogarithmic Mix for N/n of Binomial distribution\n\\[Binomial(N,p) \\bigwedge_{N/n} Logarithmic(\\theta)\\]\nPoisson Mix for K of Poisson distributon\n\\[Binomial(nK,p) \\bigwedge_K Poisson(\\theta)\\]\nHypergeometric Mix for Y of Binomial distributon\n\\[Binomial(m,\\frac{Y}{n}) \\bigwedge_{Y} Hypergeometric(n,Np,N)\\]\np Transformed Binomial\n\nDiagrammeR(\"graph LR;\n           A[Binomial &lt;br&gt; Distribution]--&gt;B[Binomial &lt;br&gt; Parent];\n           B--&gt;C[Mixing Parameter];\n           C--&gt;D[p transformed &lt;br&gt; Binomial];\n           D--&gt;E[p = 1 - exp_-t_ ];\n           E--&gt;EA[Binomial Exponential &lt;br&gt; Distribution];\n           E--&gt;EB[Binomial Gamma 1 &lt;br&gt; Distribution];\n           E--&gt;EC[Binomial Gamma 2 &lt;br&gt; Distribution];\n           E--&gt;ED[Binomial Generalized &lt;br&gt; Exponential 1 Distribution];\n           E--&gt;EE[Binomial Generalized &lt;br&gt; Exponential 2 Distribution];\n           D--&gt;F[p = exp_-t_ ];\n           F--&gt;FA[Binomial Exponential &lt;br&gt; Distribution];\n           F--&gt;FB[Binomial Gamma 1 &lt;br&gt; Distribution];\n           F--&gt;FC[Binomial Gamma 2 &lt;br&gt; Distribution];\n           F--&gt;FD[Binomial Generalized &lt;br&gt; Exponential 1 Distribution];\n           F--&gt;FE[Binomial Generalized &lt;br&gt; Exponential 2 Distribution];\n           F--&gt;FF[Binomial Variated &lt;br&gt; Exponential Distribution];\n           F--&gt;FG[Binomial Variated &lt;br&gt; Gamma 2,alpha Distribution];\n           F--&gt;FH[Binomial Inverse &lt;br&gt; Gaussian Distribution];\n           D--&gt;G[p = cy];\n           G--&gt;GA[Binomial Generalized Beta 4 Distribution];\",width=800,height=1000\n)\n\nBowman, K. O., Shenton, L. R., Kastenbaum, M. A., & Broman, K. (1992). Overdispersion: Notes on Discrete distributions. Oak Ridge Tennessee : Oak Ridge National Laboratory.\nAlanko, T., & Duffy, J. C. (1996). Compound Binomial distributions for modeling consumption data. Journal of the Royal Statistical society, series D (The Statistician) Vol. 45, No. 3 ,269-286.\nGerstenkorn, T. (2004). A compound of the Generalized Negative Binomial distribution with the Generalized Beta distribution. Central European Science journals, CEJM 2 (4), 527-537.\nLog Inverse Distribution [0,1] Domain\n\nDiagrammeR(\"graph LR;\n           A[Binomial &lt;br&gt; Distribution]--&gt;B[Binomial &lt;br&gt; Parent];\n           B--&gt;C[Mixing Parameter];\n           C--&gt;D[Log Inverse &lt;br&gt; Distribution &lt;br&gt; 0,1 Domain];\n           D--&gt;DA[Binomial Type 1 &lt;br&gt; Log Inverse &lt;br&gt; Exponential Distribution];\n           D--&gt;DB[Binomial Type 2 &lt;br&gt; Log Inverse &lt;br&gt; Exponential Distribution];\n           D--&gt;DC[Binomial Type 1 &lt;br&gt; Log Inverse &lt;br&gt; Gamma 1 Distribution];\n           D--&gt;DD[Binomial Type 2 &lt;br&gt; Log Inverse &lt;br&gt; Gamma 1 Distribution];\n           D--&gt;DE[Binomial Type 1 &lt;br&gt; Log Inverse &lt;br&gt; Gamma 2 Distribution];\n           D--&gt;DF[Binomial Type 2 &lt;br&gt; Log Inverse &lt;br&gt; Gamma 2 Distribution];\n           D--&gt;DG[Binomial Type 1 &lt;br&gt; Log Inverse &lt;br&gt; Generalized &lt;br&gt; Exponential 1 &lt;br&gt; Distribution];\n           D--&gt;DH[Binomial Type 2 &lt;br&gt; Log Inverse &lt;br&gt; Generalized &lt;br&gt; Exponential 1 &lt;br&gt; Distribution];\n           D--&gt;DJ[Binomial Type 1 &lt;br&gt; Log Inverse &lt;br&gt; Generalized &lt;br&gt; Exponential 2 &lt;br&gt; Distribution];\n           D--&gt;DK[Binomial Type 2 &lt;br&gt; Log Inverse &lt;br&gt; Generalized &lt;br&gt; Exponential 2 &lt;br&gt; Distribution];\n           D--&gt;DL[Binomial Type 1 &lt;br&gt; Log Inverse &lt;br&gt; Gamma 3 &lt;br&gt; Distribution];\",width=800,height=1000\n)\n\nGrassia, A. (1977). On a family of distributions with argument between 0 and 1 obtained by transformations of the Gamma and derived compound distributions. Australian journal of Statistics, 19 (2) 108-114.\nMcDonald, J. B., & Yexiao, J. X. (1995). A generalization of the Beta distribution with applications. Journal of Econometrics 66,133-152.\nCumulative Distribution Function\n\nDiagrammeR(\"graph TB;\n            A[Binomial &lt;br&gt; Distribution]--&gt;B[Binomial Parent];\n            B--&gt;C[Mixing Parameter];\n            C--&gt;D[Cumulative Distribution];\n            D--&gt;DA[Beta Generated &lt;br&gt; Distribution];\n            DA--&gt;DAA[Binomial &lt;br&gt; Beta Exponential &lt;br&gt; Distribution];\n            DA--&gt;DAB[Binomial &lt;br&gt; Beta Generalized &lt;br&gt; Exponential Distribution];\n            DA--&gt;DAC[Binomial &lt;br&gt; Beta Power &lt;br&gt; Distribution];\n            D--&gt;DB[Kumaraswamy &lt;br&gt; Generated &lt;br&gt; Distribution];\n            DB--&gt;DBA[Binomial &lt;br&gt; Kumaraswamy &lt;br&gt; Power Distribution];\n            DB--&gt;DBB[Binomial &lt;br&gt; Kumaraswamy &lt;br&gt; Exponential &lt;br&gt; Distribution];\",width=1000,height=750\n)\n\nEugene, N., Lee, C., & Famoye, F. (2002). Beta-normal distributions and its applications. Communications in Statistics-Theory and Methods 31, 4 ,497-512.\nNadarajah, S., & Kotz, S. (2006). The Beta Exponential distribution. Reliability engineering and system safety, Vol. 91, Issue 6 ,689-697\nBarreto-Souza, W., Santos, A., & Cordeiro, G. M. (2009). The Beta Generalized Exponential distribution. Journal of Statistical Computation and Simulation , 1-14.\np Binomial\n\nDiagrammeR(\"graph LR;\n           A[Binomial &lt;br&gt; Distribution]--&gt;B[ Binomial &lt;br&gt; Parent];\n           B--&gt;C[Mixing Parameter];\n           C--&gt;D[p &lt;br&gt; Binomial];\n           D--&gt;DA[Beyond Beta &lt;br&gt; Distribution];\n           DA--&gt;DAA[Binomial Triangular &lt;br&gt; Distribution];\n           DA--&gt;DAB[Binomial Kumaraswamy 2 &lt;br&gt; Distribution];\n           DA--&gt;DAC[Binomial Kumaraswamy 1 &lt;br&gt; Distribution];\n           DA--&gt;DAD[Binomial Truncated &lt;br&gt; Exponential Distribution];\n           DA--&gt;DAE[Binomial Truncated &lt;br&gt; Gamma Distribution];\n           DA--&gt;DAF[Binomial - MinusLog &lt;br&gt; Distribution];\n           DA--&gt;DAG[Binomial Standard &lt;br&gt; Two Sied Power &lt;br&gt; Distribution];\n           DA--&gt;DAH[Binomial Ogive &lt;br&gt; Distribution];\n           DA--&gt;DAI[Binomial - Two Sided &lt;br&gt; Ogive Distribution];\n           D--&gt;DB[Beta Distribution];\n           DB--&gt;DBA[Beta - Binomial &lt;br&gt; Distribution];\n           DB--&gt;DBB[McDonald Generalized &lt;br&gt; Beta - Binomial &lt;br&gt; Distribution];\n           DB--&gt;DBC[Libby and Novick &lt;br&gt; Generalized Beta - Binomial &lt;br&gt; Distribution];\n           DB--&gt;DBD[Gauss Hypergeometric &lt;br&gt; Binomial Distribution];\n           DB--&gt;DBE[Confluent Hypergeometric &lt;br&gt; Binomial Distribution];\n           DB--&gt;DBF[Binomial Uniform &lt;br&gt; Distribution];\n           DB--&gt;DBG[Binomial Power &lt;br&gt; Function Distribution];\n           DB--&gt;DBH[Binomial Truncated &lt;br&gt; Beta Distribution];\n           DB--&gt;DBI[Binomial Arcsine &lt;br&gt; Distribution];\",width=700,height=1200\n)\n\nKarlis, D., & Xekalaki, E. (2006). The Polygonal distributions, ntemational Conference on Mathematical and Statistical modeling in honnor of Enrique Castillo. University of Castilla-La Mancha.\nJones, M. C. (2009). Kumaraswamy’s distribution: a beta-type distibution with some tractability advantages. Statistical Methodology Vol. 6, Issue 1 ,70-81.\nJones, M. C. (2007). The Minimax distribution: A Beta type distribution with some tractability advantages. Retrieved from The Open University: http://stats-www.open.ac.uk/TechnicalReports/minimax.pdf\nKumaraswamy, P. (1980). Genralized probability density functions for double-bounded random processes. Journal of hydrology, 79-88.\nDorp, J. R., & Kotz, S. (2003). Generalizations f Two-Sided Power distributions and their Convolution. Communications in Statistics-Theory and Methods, Vol. 32, Issue 9 ,1703-1723.\nArmero, S., & Bayarri, M. J. (1994). Prior assessments for prediction in queues. The Statistician 43,139- 153.\nBhattacharya, S. K. (1968). Bayes approach to compound distributions arising from truncated mixing densities. Annals of the institute of Statistical Mathematics, Vol 20, No. 1 ,375-381.\nJohnson, N. L., Kotz, S., & Kemp, A. (1992). Univariate Discrete distributions, Second Edition. New York: John Wiley and Sons.\nLibby, D. I., 8t Novick, M. R. (1982). Multivariate generalized beta-distributions with applicatons to utility assessment. Journal of Educational Statistics 9 ,163-175.\nNadarajah, S., 8i Kotz, S. (2007). Multitude of Beta distributions with applications. Statistics: a journal of theoretical and applied statistics, Vol. 41, No. 2 ,153-179.\nSivaganesan, S., 8i Berger, J. (1993). Robust Bayesian analysis of the Binomial empirical Bayes problem. The Canadian journal of Statistics, 21,107-119."
  },
  {
    "objectID": "posts/Benchmarking/mleand2/index.html",
    "href": "posts/Benchmarking/mleand2/index.html",
    "title": "Benchmarking the mle and mle2 function",
    "section": "",
    "text": "NOTE : Below post is valid for Package version 1.4.0 and Before."
  },
  {
    "objectID": "posts/Benchmarking/mleand2/index.html#mle",
    "href": "posts/Benchmarking/mleand2/index.html#mle",
    "title": "Benchmarking the mle and mle2 function",
    "section": "mle",
    "text": "mle\nmle function is from the stats4 package. If we intend to use this function for the estimation of shape parameters a and b of the Beta-Binomial distribution wtih Binomial Outcome Data, then we need to use the EstMLEBetaBin function from the fitODBOD package. This is not enough because for limitations in the mle we need to make changes in our EstMLEBetaBin function as mentioned below.\n\nlibrary(stats4)\nlibrary(fitODBOD)\n\n#new function to facilitate mle criteria \nformle&lt;-function(a,b)\n{\n  EstMLEBetaBin(x=Alcohol_data$Days,freq = Alcohol_data$week1,a,b)\n}\n\n# optimizing values for a,b using default analytial method\nmle_answer&lt;-mle(minuslogl = formle,start = list(a=0.1,b=0.2))\n\nWe are going to use the Alcohol Consumption data of week 1. In the above code chunk we are using the mle function for our task of finding the optimum shape parameter values for a and b while using the given Binomial Outcome data. Also If you wish you study about the mle function by referring this link from my previous post."
  },
  {
    "objectID": "posts/Benchmarking/mleand2/index.html#mle2",
    "href": "posts/Benchmarking/mleand2/index.html#mle2",
    "title": "Benchmarking the mle and mle2 function",
    "section": "mle2",
    "text": "mle2\nbbmle package holds the mle2 function. It is simply an updated version for the mle function. Although there need to be no changes in the EstMLEBetaBin function to satisfy the mle2 function’s criteria. Now it will be possible to use it.\n\nlibrary(bbmle)\n\n# optimizing values for a,b using default analytical method\nmle2_answer&lt;-mle2(minuslogl= EstMLEBetaBin,start = list(a=0.1,b=0.2),\n                  data = list(x=Alcohol_data$Days,freq=Alcohol_data$week1))\n\nStill if someone needs a brief introduction to mle2 function they can refer my previous brief through this link."
  },
  {
    "objectID": "posts/Benchmarking/fitODBOD/index.html",
    "href": "posts/Benchmarking/fitODBOD/index.html",
    "title": "Benchmarking optimization functions in R",
    "section": "",
    "text": "NOTE : Below post is valid for Package version 1.4.0 and Before."
  },
  {
    "objectID": "posts/Benchmarking/fitODBOD/index.html#optim-function",
    "href": "posts/Benchmarking/fitODBOD/index.html#optim-function",
    "title": "Benchmarking optimization functions in R",
    "section": "optim Function",
    "text": "optim Function\noptim is the first function in concern. Documentation of the optim function is useful and it indicates that this function is only used on one input situations only. This means our EstMLEBetaBin function has to be modified. Reason for this is only the parameters that should be estimated need to be input values but our EstMLEBetaBin function has four parameters which are a,b,x(Binomial Random Variable) and freq(corresponding frequency values).\nWhile using optim function first index refers to shape parameter a and second index refers to shape parameter b. Further, we have to input the observations or in our case the Binomial random variable values and their respective frequencies. I think it is inconvenient to modify the EstMLEBetaBin function, because if we want to estimate parameters for different data-sets it would become tedious. After modification we have a new function foroptim which can be used for demonstration and comparison.\nBelow is the code to estimation and going through the outputs. It should be noted that we have to provide initial parameter values as an input to the optim function, and it is best to provide values in the domain of shape parameter values which we want to estimate.\nHere the shape parameters a and b are in the region of greater than zero but less than positive infinity (\\(+\\infty &gt;a,b&gt;0\\)). So for the initial parameters of a=0.1 and b=0.2 we are finding parameters which would minimize the Negative Log Likelihood function of Beta-Binomial distribution with the Alcohol Consumption data.\n\n# new function to facilitate optim criteria\n# only one input but has two elements\nforoptim&lt;-function(a)\n  {\n  EstMLEBetaBin(x=Alcohol_data$Days, freq=Alcohol_data$week1,a=a[1],b=a[2])\n  }\n\n# optimizing values for a,b using default mathematical method\noptim_answer&lt;-optim(par=c(0.1,0.2),fn=foroptim)\n\n# obtaining class of output\nclass(optim_answer)\n\n#length of output\nlength(optim_answer)\n\n# the outputs\noptim_answer$par # estimated values for a, b\noptim_answer$value # minimized function value \noptim_answer$counts  # see the documentation to understand\noptim_answer$convergence # indicates successful completion\noptim_answer$message # additional information\n\n# fitting the Beta-Binomial distribution with estimated shape parameter  values\nfitBetaBin(Alcohol_data$Days,Alcohol_data$week1,\n           optim_answer$par[1],optim_answer$par[2])\n\nSo the foroptim function can be used as above and parameters are estimated for \\(\\alpha\\) and \\(\\beta\\) (or a, b) for the Alcohol Consumption data week 1. Further we have scrutinized the function as below.\n\npackage : stats\nNo of Inputs: 7\nMinimum required Inputs : 2\nClass of output : list\nNo of outputs: 5\nNo of Analytical Methods : 6\nDefault Method : Nelder-Mead"
  },
  {
    "objectID": "posts/Benchmarking/fitODBOD/index.html#nlm-function",
    "href": "posts/Benchmarking/fitODBOD/index.html#nlm-function",
    "title": "Benchmarking optimization functions in R",
    "section": "nlm Function",
    "text": "nlm Function\nnlm function is also similar to optim but only one analytical method will be used, which is a Newton-type Algorithm. Here also there needs to be changes made to our EstMLEBetaBin function as previously. After making those changes we have called the new Negative Log-likelihood function of Beta-Binomial distribution as fornlm. Then we can use the nlm function and estimate a and b for the initial shape parameter values of 0.1 and 0.2 respectively. Documentation of nlm function is very useful so that we can understand how it works.\nBelow is the code for using nlm function appropriately and fiddling with the results.\n\n#new function to facilitate nlm criteria\nfornlm&lt;-function(a)\n  {\n  EstMLEBetaBin(x=Alcohol_data$Days, freq=Alcohol_data$week1,a=a[1],a[2])\n  }\n\n#optimizing values for a,b using the only analytical method\nnlm_answer&lt;-nlm(f=fornlm,p=c(0.1,0.2))\n\n#obtaining class of output\nclass(nlm_answer)\n\n#length of output\nlength(nlm_answer)\n\n# the outputs\nnlm_answer$estimate # estimated values for a, b\nnlm_answer$minimum # minimized function value \nnlm_answer$gradient  # gradient at the estimated minimum of given funciton\nnlm_answer$code # indicates successful completion\nnlm_answer$iterations # number of iterations performed\n\n# fitting the Beta-Binomial distribution with estimated shape parameter  values\nfitBetaBin(Alcohol_data$Days,Alcohol_data$week1,\n           nlm_answer$estimate[1],nlm_answer$estimate[2])\n\nSimilarly for the fornlm function we have estimated values for a and b which would fit the Alcohol consumption data of week 1. Below is a point form summary of nlm function.\n\npackage : stats\nNo of Inputs: 12\nMinimum required Inputs : 2\nClass of output : list\nNo of outputs: 5\nNo of Analytical Methods : 1\nDefault Method : Newton-type Algorithm"
  },
  {
    "objectID": "posts/Benchmarking/fitODBOD/index.html#nlminb-function",
    "href": "posts/Benchmarking/fitODBOD/index.html#nlminb-function",
    "title": "Benchmarking optimization functions in R",
    "section": "nlminb Function",
    "text": "nlminb Function\nnlminb is also similar and requires the EstMLEBetaBin function to be restructured as similar to previous situations. After this task we now have a function called fornlminb. nlminb function is based on analytical method of unconstrained and box-constrained optimization using PORT routines.\nAfter choosing initial parameter values for a and b, which are respectively 0.1 and 0.2 the estimation was done following the process below\n\n# new function to facilitate nlminb criteria\nfornlminb&lt;-function(a)\n  {\n  EstMLEBetaBin(x=Alcohol_data$Days, freq=Alcohol_data$week1,a=a[1],a[2])\n}\n\n# optimizing values for a,b using default analytical method\nnlminb_answer&lt;-nlminb(start=c(0.1,0.2), objective=fornlminb)\n\n# obtaining class of output\nclass(nlminb_answer)\n\n# length of output\nlength(nlminb_answer)\n\n# the outputs\nnlminb_answer$par # estimated values for a, b\nnlminb_answer$objective # minimized function value \nnlminb_answer$evaluations  # see the documentation to understand\nnlminb_answer$convergence # indicates successful completion\nnlminb_answer$message # additional information\nnlminb_answer$iterations # number of iterations performed\n\n# fitting the Beta-Binomial distribution with estimated shape parameter values\nfitBetaBin(Alcohol_data$Days,Alcohol_data$week1,\n           nlminb_answer$par[1],nlminb_answer$par[2])\n\nDocumentation includes the information related to the function therefore referring it will be useful. After estimating values for a and b using nlminb function these were noticed regarding the function in concern\n\npackage : stats\nNo of Inputs: 8\nMinimum required Inputs : 2\nClass of output : list\nNo of outputs: 6\nNo of Analytical Methods : 1\nDefault Method : Unconstrained and box-constrained optimization using PORT routines"
  },
  {
    "objectID": "posts/Benchmarking/fitODBOD/index.html#ucminf-function",
    "href": "posts/Benchmarking/fitODBOD/index.html#ucminf-function",
    "title": "Benchmarking optimization functions in R",
    "section": "ucminf Function",
    "text": "ucminf Function\nPackage ucminf produces the ucminf function, as previously mentioned functions here also we have to change the EstMLEBetaBin function. After making the changes we will be using the forucminf function to the estimation process of shape parameters a and b.\nWhen initial parameter values are set to a=0.1 and b=0.2 we will obtain results from ucminf function, which will minimize the Negative Log-likelihood value of Beta-Binomial distribution. Below is the code for estimation and using the results to understand the function ucminf.\n\nlibrary(ucminf)\n\n# new function to facilitate ucminf criteria\nforucminf&lt;-function(a)\n  {\n  EstMLEBetaBin(x=Alcohol_data$Days,freq = Alcohol_data$week1,a=a[1],a[2])\n}\n\n# optimizing values for a,b using default analytical method\nucminf_answer&lt;-ucminf(par=c(0.1,0.2),fn=forucminf)\n\n#obtaining class\nclass(ucminf_answer)\n\n# length of output\nlength(ucminf_answer)\n\n# the outputs\nucminf_answer$par # estimated values for a, b\nucminf_answer$value # minimized function value \nucminf_answer$invhessian.lt  # see the documentation understand\nucminf_answer$convergence # indicates successful completion\nucminf_answer$message # additional information\nucminf_answer$info\n\n# fitting the Beta-Binomial distribution with estimated shape parameter values\nfitBetaBin(Alcohol_data$Days,Alcohol_data$week1,\n           ucminf_answer$par[1],ucminf_answer$par[2])\n\nWith the help of R Documentation the estimation was done for shape parameter values a and b using the ucminf function. Below is the initial understanding of the ucminf function\n\npackage : ucminf\nNo of Inputs: 5\nMinimum required Inputs : 2\nClass of output : list\nNo of outputs: 6\nNo of Analytical Methods : 1\nDefault Method : Quasi-Newton Algorithm type with BFGS updating"
  },
  {
    "objectID": "posts/Benchmarking/fitODBOD/index.html#maxlik-function",
    "href": "posts/Benchmarking/fitODBOD/index.html#maxlik-function",
    "title": "Benchmarking optimization functions in R",
    "section": "maxLik Function",
    "text": "maxLik Function\nmaxLik function is from the maxLik package, which only maximizes the Log Likelihood function. Therefore we have to restructure EstMLEBetaBin as previously mentioned, but as an addition a negative sign is added for the output. This new function will be called as formaxLik.\nFor the initial parameter values where a=0.1 and b=0.2 the maxLik function will be used and results will be evaluated as below.\n\nlibrary(maxLik)\n\n# new function to facilitate maxLik criteria\nformaxLik&lt;-function(a)\n  {\n  -EstMLEBetaBin(x=Alcohol_data$Days,freq = Alcohol_data$week1,a=a[1],a[2])\n  }\n\n# optimizing values for a,b using default analytical method\nmaxLik_answer&lt;-maxLik(logLik =formaxLik, start = c(0.1,0.2))\n\n# obtaining class of output\nclass(maxLik_answer)\n\n# length of output\nlength(maxLik_answer)\n\n# the outputs\nmaxLik_answer$estimate # estimated values for a, b\nmaxLik_answer$maximum # minimized function value \nmaxLik_answer$iterations  # no of iterations to succeed\nmaxLik_answer$gradient # last gradient value which was calculated\nmaxLik_answer$message # additional information\nmaxLik_answer$hessian # hessian matrix\nmaxLik_answer$code # indicates successful completion\nmaxLik_answer$fixed # logical vector indicating which parameters are constants\nmaxLik_answer$type # type of maximization\nmaxLik_answer$last.step # list describing the last unsuccessful step\nmaxLik_answer$control # see the documentation understand\n\n# fitting the Beta-Binomial distribution with estimated shape parameter values\nfitBetaBin(Alcohol_data$Days,Alcohol_data$week1,\n           maxLik_answer$estimate[1],maxLik_answer$estimate[2])\n\nUsing the Documentation of maxLik and the Documentation of maxNR function the above analysis was done for the maxLik function. According to the above code, below are the findings from the maxLik function in point form\n\npackage : maxLik\nNo of Inputs: 6\nMinimum required Inputs : 2\nClass of output : list or class of maxim or class of maxLik\nNo of outputs: 11\nNo of Analytical Methods : 7\nDefault Method : Automatically chosen"
  },
  {
    "objectID": "posts/Benchmarking/fitODBOD/index.html#mle-function",
    "href": "posts/Benchmarking/fitODBOD/index.html#mle-function",
    "title": "Benchmarking optimization functions in R",
    "section": "mle Function",
    "text": "mle Function\nstats4 package is installed so that mle function can be operated for the purpose of estimating a and b parameters. Here also as previously we need to make some changes as below and create a new Negative Log Likelihood function called formle.\nFor the initial parameter values of a=0.1 and b=0.2 the Negative Log Likelihood value of Beta-Binomial distribution has been minimized using mle function. Below is the code for estimation and investigation from the outputs of mle after estimation.\n\nlibrary(stats4)\n\n# new function to facilitate mle criteria\nformle&lt;-function(a,b)\n  {\n  EstMLEBetaBin(x=Alcohol_data$Days,freq = Alcohol_data$week1,a,b)\n  }\n\n# optimizing values for a,b using default analytical method\nmle_answer&lt;-mle(minuslogl=formle,start = list(a=0.1,b=0.2))\n\n# obtaining class\nclass(mle_answer)\n\n# length of output\nlength(mle_answer)\n\n# the outputs\nmle_answer@call # inputs i have used \nmle_answer@coef # estimated values for a,b\nmle_answer@fullcoef # all values, even the fixed values we did not want to estimate\nmle_answer@vcov # variance covariance matrix for a,b\nmle_answer@min # minimized function value\nmle_answer@details # details after estimation process\nmle_answer@nobs # number of observations to be used for computing only if given \nmle_answer@method # optimization methods used\n\n# Methods used\nconfint(mle_answer) # confidence intervals for estimated values\nlogLik(mle_answer) # Negative loglikelihood value for estimated values \nprofile(mle_answer) # Likelihood profile generation.\nnobs(mle_answer) # number of observations to be used for computing only if given\nshow(mle_answer) # display object briefly\nsummary(mle_answer) # generate a summary\n#update()   # updating if we have new data and need to estimate new values\nvcov(mle_answer) # variance covariance matrix\n\n# fitting the Beta-Binomial distribution with estimated shape parameter values\nfitBetaBin(Alcohol_data$Days,Alcohol_data$week1,\n           mle_answer@coef[1],mle_answer@coef[2])\n\nIt should be noted that in all 5 previous functions we had only outputs in the form of lists, but through mle we are seeing a class of mle output.The Documention explains the inputs and outputs of mle. Documentation for mle-class explains further about the methods that can be used. Below is a list of findings based on the outputs of the mle function.\n\npackage : stats4\nNo of Inputs: 5\nMinimum required Inputs : 2\nClass of output : class of mle\nNo of outputs: 9\nMethods for output: 8\nNo of Analytical Methods :6\nDefault Method : Nelder and Mead"
  },
  {
    "objectID": "posts/Benchmarking/fitODBOD/index.html#mle2-function",
    "href": "posts/Benchmarking/fitODBOD/index.html#mle2-function",
    "title": "Benchmarking optimization functions in R",
    "section": "mle2 Function",
    "text": "mle2 Function\nmle2 function is advanced than mle function and it is from the package bbmle. Here, there is no need to modify the EstMLEBetabin function from the fitODBOD package to estimation as all previous situations .\nFor the initial parameter values of a=0.1 and b=0.2 Negative Log Likelihood function of Beta-Binomial distribution will be minimized where outputs will be investigated and methods related to output of class mle2 will be used.\nBelow is the code for using mle2 function and scrutinizing the output and methods related to it.\n\nlibrary(bbmle)\n\n# optimizing values for a,b using default analytical method\nmle2_answer&lt;-mle2(minuslogl= EstMLEBetaBin,start = list(a=0.1,b=0.2),\n                  data = list(x=Alcohol_data$Days,freq=Alcohol_data$week1))\n\n# obtaining class\nclass(mle2_answer)\n\n# length of output\nlength(mle2_answer)\n\n# the outputs\nmle2_answer@call # inputs generally considered \nmle2_answer@call.orig # inputs i have given\nmle2_answer@coef # estimated values for a,b\nmle2_answer@fullcoef # all values, even the fixed values we did not want to estimate\nmle2_answer@vcov # variance covariance matrix for a,b\nmle2_answer@min # minimized function value\nmle2_answer@details # details after estimation process\nmle2_answer@method # optimization methods used\nmle2_answer@data # data used for estimation \nmle2_answer@formula # if a formula was specified in the input \nmle2_answer@optimizer # function used for optimizing\n\n# Methods used\ncoef(mle2_answer) # extrat the estimated values\nconfint(mle2_answer) # confidence intervals for estimated values\nshow(mle2_answer) # display object briefly\nsummary(mle2_answer) # generate a summary\n#update()   #updating if we have new data and need to estimate new values\nvcov(mle2_answer) # variance covariance matrix\n#formula(mle2_answer) # if a formula was specified in the input \n#plot(mle2_answer) # plot the profile\nlogLik(mle2_answer) # Negative loglikelihood value for estimated values \nprofile(mle2_answer) # profile of estimated values\n\n# fitting the Beta-Binomial distribution with estimated shape parameter values\nfitBetaBin(Alcohol_data$Days,Alcohol_data$week1,\n           mle2_answer@coef[1],mle2_answer@coef[2])\n\nDocumention of mle2 function and Documentation of mle2-class\nprovide a few findings as mentioned below in point form.\n\npackage : bbmle\nNo of Inputs: 22\nMinimum required Inputs : 3\nClass of output : class of mle2\nNo of outputs: 12\nMethods for output: 9\nNo of Analytical Methods : 6\nDefault Method : Nelder and Mead"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Amalan Mahendran",
    "section": "",
    "text": "Twitter\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     R Universe\n  \n  \n    \n     Email\n  \n\n      \nI am a PhD student at the School of Mathematical Sciences, Queensland University of Technology working on “New methods of experimental design for subsampling under big data” and expecting to graduate in 2024.\nI completed my Bachelor of Science (Statistics Special) on the subjects Statistics, Mathematics and Computer Science from the University of Peradeniya, Sri Lanka.\nI am interested in applying statistical methods efficiently and effectively through the R statistical programming language. Conducting complex computations, writing reports and publishing them through R is my special skill over 7 years of experience. Yes, I am proficient in R related tools base, tidyverse, Rshiny, golem, bookdown, rmarkdown and quarto.\nI have developed the below listed R packages and Shiny apps:\n\n\n\nfitODBOD: Modeling Over Dispersed Binomial Outcome Data Using BMD and ABD\n\n\n\n\n\nOlympicRshiny: ‘Shiny’ Application for Olympic Data\n\n\n\n\n\nSLPresElection: Presidential Election Data of “Sri Lanka” from 1982 to 2015.\n\n\n\nOn this website, I offer a glimpse into my vast body of knowledge, ongoing research, publications, conference posters and my interests. If you are interested in my work and have any questions or would like to discuss do not hesitate to contact me"
  },
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "Curriculum vitae",
    "section": "",
    "text": "Download my academic CV"
  },
  {
    "objectID": "interests/index.html",
    "href": "interests/index.html",
    "title": "Interests",
    "section": "",
    "text": "Purely for entertainment purposes except for the books\n\nBooksMoviesTv shows\n\n\n\nI started reading books from month of April, 2018. Which is very useful for my life as a believer of self-learning. I hope to share these books with you so that it will be useful. There is no meaning in numbers except that they are just for ordering from 1 to further in no hierarchy. Genre specificity is not my thing therefore these books are not divided into groups.\n\nApril 2018 - December 2018\n\nRise the Dark by Michael Koryta\nThe Bourne Enigma by Eric Van LustBader\nJack Reacher thriller, A Wanted Man by Lee Child\nDigital Fortress by Dan Brown\nThe Selfish Gene by Richard Dawkins\nHow Democracies Die by Steven Levitsky and Daniel Ziblatt\nElon Musk by Ashlee Vance\nRoll of Thunder Hear My Cry by Mildred Taylor\nBoy by Roald Dohl\nAnimal Farm by George Orwell\nThe power of Habit by Charles Duhigg\n\n\n\nOctober 2019\n\nThe Lost Symbol by Dan Brown\nJack Reacher thriller, Die Trying by Lee Child\nLike a Virgin by Richard Branson\nCrash Bang Wallop\n\n\n\nNovember 2019\n\nThe Gambler by Fyodor Dostoevsky\n\n\n\nBooks Related to R, Data Science and Statistics\n\nBig Book of R\n\nTHANK YOU\n\n\n\n\nWatching movies since 2012 with much interest in the story line and characters. I have watched more than 500 movies, but the below mentioned are in my mind. They are from different decades as well, however do not hesitate to watch. If you like go through the IMDB link for more information.\nlast updated on 17/06/2023\n\n0 to 9\n\n12 Angry Men\n13 Hours The Secret Soldiers of Benghazi\n2012\n\n\n\nA\n\nA History of Violence\nA Private War\nAmerican Gangster\nAnt man\nAnt man and the wasp\nAnt man and the wasp quantumania\nArgo\nArmageddon\nArrival\nAsterix and Obelix Mansion of Gods\nAsterix and Obelix The Secret of The Magic Potion\nAvatar the way of water\nThe Avengers\nAvengers Age of Ultron\nAvengers Infinity War\nAvengers Endgame\n\n\n\nB\n\nBad Boys\nBad Boys II\nBad Boys for Life\nBatman Begins\nThe Dark Knight\nThe Dark Knight Rises\nBetween Two Ferns The Movie\nBlack Hawk Down\nBlack Panther\nBlack Panther Wakanda Forever\nBlack Widow\nBlacKkKlansman\nBohemian Rhaspody\nBorat\nBorat Subsequent Moviefilm\nBridge of Spies\nBroken City\nBruce Almighty\n\n\n\nC\n\nCaptain America The First Avenger\nCaptain American The Winter Soldier\nCaptain American Civil War\nCaptain Marvel\nCaptive State\nCasino\nChaos\nCharlie Brown\nCharlie Chaplin Modern Times\nCharlie Wilsons War\nConstantine\nCoup 53\nCasablanca\nCast Away\n\n\n\nD\n\nThe Da Vinci Code\nAngels and Demons\nInferno\nDave Chappelle The closer\nAquaman\nBatman vs Superman Dawn of Justice\nBirds of Prey\nBlack Adam\nJoker\nJustic League\nMan of Steel\nShazam\nSuicide Squad\nThe Batman\nThe Suicide Squad\nWonder Woman\nZack snyders Justice League\nShazam Fury of Gods\nDeadpool\nDeadpool 2\nDeep Impact\nDeja Vu\nDie Hard 1\nDie Hard 2\nDie Hard with a Vengeance\nA Good Day to Die Hard\nDilruk jayasinha Bundle of Joy\nDoctor Strange\nDoctor Strange in the Multiverse of Madness\nDune Part 1\nDunkirk\nDead Poets Society\nDr. Strangelove\nDespicable Me Collection\n\n\n\nE\n\nEnola Holmes\nEnola Holmes 2\nEntourage\nEternals\n\n\n\nF\n\nFargo\nFast Furious Collection\nFight Club\nFord v Ferrari\nFury\nForrest Gump\n\n\n\nG\n\nG. I. Joe Collection\nGlass\nGodfather Trilogy\nGodzilla Collection\nGodzilla vs. Kong\nGone Baby Gone\nGood Will Hunting\nGreen Book\nGuardians of Galaxy Vol 1\nGuardians of Galaxy Vol 2\nGuardians of Galaxy Vol 3\nGladiator\n\n\n\nH\n\nHarry Potter Saga\nHeat\nHot Fuzz\nHow to train your dragon\n\n\n\nI\n\nInception\nIndependence Day Dualogy\nIndiana Jones Quadrapology\nInternal Affairs\nInglorious Bastards\nInside Man\nInsomnia\nInterstellar\nIp man\nIron Man Trilogy\nII Testimone Invisible\n\n\n\nJ\n\nJack Reacher Dualogy\nJack Ryan Collection\nJackie Brown\nJames Bond 007 Collection\nJohn Wick Trilogy\nJoJo Rabbit\n\n\n\nK\n\nKill Bill Dualogy\nKiller Elite\nKnight and Day\nKungfu Panda\n\n\n\nL\n\nL.A. Confidential\nLaw abiding Citizen\nLawrence of Arabia\nLiar Liar\nLimitless\nLock, Stock and Two Smoking Barrels\nLooper\nLord of War\nLucky Number Slevin\n\n\n\nM\n\nMadagascar\nMargin Call\nMen in Black Trilogy\nMinions\nMirage\nMission Impossible Collection\nMoneyball\nMorbius\nMortal Kombat\nMr and Mrs Smith\nMurder on the Orient Express Dualogy\nMemento\nMillion Dollar Baby\n\n\n\nN\n\nNo Country for Old Men\nNobody\nNow You See Me Collection\n\n\n\nO\n\nOceans Collection\nOn the Basis Of Sex\nOnce upon a time…in Hollywood\nOnce upon a Time in the West\nOur Brand is Crisis\nOne flew over the cuckoos nest\n\n\n\nP\n\nPaddington\nPain and Gain\nParasite\nPaycheck\nPirates of the Caribbean Collection\nPokemon Detective Pikachu\nPulp Fiction\n\n\n\nQ\n\nQuiz Show\n\n\n\nR\n\nRambo Movie Collection\nRear Window\nReservior Dogs\nRock n Rolla\nRonin\n\n\n\nS\n\nSafe House\nSalt\nSaving Private Ryan\nSchindlers List\nSeven Psychopaths\nShaft Dualogy\nShang Chi and the legend of the ten rings\nShaun of The Dead\nSicario\nSicario Day of the Soldado\nSnatch\nSpider Man Collection\nSplit\nSpy Game\nStar Wars Collection\nStargate\nSwordfish\nSe7en\nShutter Island\n\n\n\nTamil Movies\n\n\nT\n\nTaken Trilogy\nTed Dualogy\nTenet\nThe A Team\nThe Accountant\nThe Addams Family\nThe Addams Family 2\nThe Bank Job\nThe Big Short\nThe Body\nThe Bone Collector\nThe Book of Eli\nThe Bourne Collection\nThe Bridge to the River Kwai\nThe Dark Tower\nThe Day After\nThe Departed\nThe Drop\nThe Expendables Collection\nThe Girl With Collection\nThe Good The Bad and The Ugly\nThe Great Escape\nThe Gunman\nThe Highwaymen\nThe Hobbit Collection\nThe Hurt Locker\nThe Intouchables\nThe Invisible Guest\nThe Italian Job\nThe Killing Fields\nThe King of Comedy\nThe Kingdom\nThe Lego Movie\nThe Lives of Others\nThe Lord of The Rings Collection\nThe Martian\nThe Mask\nThe Matrix Trilogy\nThe Mechanic\nThe Monuments Men\nThe Peacemaker\nThe Raid\nThe Revenant\nThe Secret Life of Pets\nThe Silence of the Lambs\nThe Sting\nThe Terminator Collection\nThe Tourist\nThe Untouchables\nThe Usual Suspects\nThe Irishman\nThings to Come\nThor\nTinker Tailor Soldier Spy\nToy Story\nTrain to Busan 1\nTrain to Busan Presents: Peninsula\nTraining Day\nTriple Frontier\nTaxi Driver\nThe Prestige\nThe Shawshank Redemption\nThe Shining\nThe Terminal\nThe Truman Show\nTitanic\nTropic Thunder\n\n\n\nU\n\nUnforgiven\nUnknown\nUp\nUnbreakable\n\n\n\nV\n\nVantage Point\nVenom\nVenom Let there be carnage\nVice\nV for Vendetta\n\n\n\nW\n\nWall-E\nWanted\nWatchmen\nWorld War Z\n\n\n\nX\n\nX-Men Collection\n\n\n\nY\n\nYes Man\n\n\n\nZ\n\nZero Dark Thirty\nZombieland Dualogy\n\nTHANK YOU\n\n\n\n More than 100 TV shows are on my list and I have watched all of them. There are a lot of shows in this list where all would agree to be great and enticing. There is no specific genre of my interest and I do not mind re-watching these TV shows again.\nlast updated on 17/06/2023\n\n0 to 9\n\n1983\n24\n\n\n\nA\n\nA bit of Fry and Laurie\nA Perfect Planet\nAfter life\nAgents of Chaos\nAltered Carbon\nAmerican Dad\nAnsatsu Kyoushitsu\nArcher\nAsian Provocateur\nAsur\nAttack on Titan\n\n\n\nB\n\nBand of Brothers\nBanshee\nBarry\nBeelzebub\nBehind Her Eyes\nBig Train\nBlack Mirror\nBlood of Zeus\nBlue Planet I,II\nBodyGuard\nBoston Legal\nBreaking Bad\nBridgerton\nBroadChurch\nBrooklynn Nine Nine\n\n\n\nC\n\nCastle\nChance\nCheers\nChernobyl\nClarksons Farm\nCollateral\nCommunity\nCondor\nCosmos A space time odyssey\nCricket Fever MI\nCriminal Minds\nCriminal : UK\n\n\n\nD\n\nDark Desire\nDeath Note\nDerry Girls\nDes\nDexter\nDom\nDOTA: Dragon’s Blood\nDucktales\nDynasties\n\n\n\nE\n\nEntourage\nEpisodes\nExtras\n\n\n\nF\n\nFamily Guy\nFawlty Towers\nFleabag\nFrasier\nFriends\n\n\n\nG\n\nGadget Man\nGame of Thrones\nGood Omens\nGoodness Gracious Me\n\n\n\nH\n\nHarley Quinn\nHawkeye\nHellbound\nHonour\nHouse\nHouse of Cards US\nHouse of Cards UK\nHouse of the Dragon\nHow I Met Your Mother\nHow to Become a Tyrant\n\n\n\nI\n\nI am Groot\nIndustry\nInformer\nInside Edge\nInvincible\n\n\n\nJ\n\nJeeves and Wooster\nJeffrey Epstein Filthy Rich\nJessica Jones\nJohnny Bravo\n\n\n\nK\n\nKeeping Up Appearances\nKey and Peele\nKilling Eve\nKoombiyo\n\n\n\nL\n\nLine of Duty\nLoki\nLove, Death and Robots\nLucifer\nLupin\nLuther\n\n\n\nM\n\nMalcolm in the middle\nManhunt\nMarvels Hit Monkey\nMaster of None\nMayor of Kingstown\nMcCartney 321\nMind Your Language\nMindhunter\nMiss Marvel\nModern Family\nMoney Heist\nMonk\nMonty Python The Flying Circus\nMr. Robot\nMy Hero Academia\n\n\n\nN\n\nNarcos\nNever have i ever\nNew Girl\nNot the Nine O’Clock News\n\n\n\nO\n\nObi Wan Kenobi\nObsession\nOne Punch Man\nOnly Murders in the Building\nOzark\n\n\n\nP\n\nParks and Recreation\nPatrick Melrose\nPeaky Blinders\nPerson of Interest\nPlanet Earth I,II\nPreacher\nPress\n\n\n\nQ\n\nQuiz\n\n\n\nR\n\nRagnarok\nReacher\nRick and Morty\nRicky Gervais\n\n\n\nS\n\nSahodaraya\nSeinfeld\nSex Education\nShe Hulk\nSherlock\nSouthpark\nSpace Force\nSquid Game\nStranger Things\nSuccession\nSuits\nSuper Crooks\nSupernatural\nSuzhal\n\n\n\nT\n\nTed Lasso\nThanamalvila Kollek\nThat 70’s Show\nThe ABC Murders\nThe Big Bang Theory\nThe Blacklist\nThe Book of Boba fett\nThe Boys\nThe Catherine Tate Show\nThe Crown\nThe Falcon and the Winter Soldier\nThe Following\nThe Fresh Prince of Bel-Air\nThe Good Place\nThe Grand Tour\nThe IT Crowd\nThe Killing\nThe Last Dance\nThe last of Us\nThe Legend of Korra\nThe Lord of the Rings : The rings of the power\nThe Mandalorian\nThe Mentalist\nThe Missing\nThe Morring Show\nThe Newsroom\nThe Night Manager\nThe Night of\nThe Office US\nThe peacemaker\nThe Punisher\nThe Serpent\nThe Silent Sea\nThe Sopranos\nThe Spy\nThe Stranger\nThe Thin Blue Line\nThe Two Ronnies\nThe West Wing\nThe Wire\nThe Witcher\nThe Night Agent\nThe Sandman\nThe Undeclared War\nThick of It\nThis is Going to Hurt\nTiger King\nTom Clancys Jack Ryan\nTop Gear\nTravel man\nTrue Detective\nTwin peaks\nTwo and a Half Men\n\n\n\nU\n\nUnforgotten\n\n\n\nV\n\nVeep\n\n\n\nW\n\nWandavision\nWatchmen\nWe own this city\nWednesday\nWestworld\nWhat If\nWhite Collar\nWorlds Most wanted\nWu Assassins\n\n\n\nY\n\nYears and Years\nYes Minister\nYes, Prime Minister\nYou\nYoung Sheldon\n\nTHANK YOU"
  },
  {
    "objectID": "posts/Benchmarking/maxLik/index.html",
    "href": "posts/Benchmarking/maxLik/index.html",
    "title": "Benchmarking the maxLik function",
    "section": "",
    "text": "NOTE : Below post is valid for Package version 1.4.0 and Before."
  },
  {
    "objectID": "posts/Benchmarking/maxLik/index.html#brief-of-maxlik-function",
    "href": "posts/Benchmarking/maxLik/index.html#brief-of-maxlik-function",
    "title": "Benchmarking the maxLik function",
    "section": "Brief of maxLik Function",
    "text": "Brief of maxLik Function\nSmall section about the maxLik function will be very useful to understand this blog post.\nReference : Henningsen, A. and Toomet, O. (2011): maxLik: A package for maximum likelihood estimation in R Computational Statistics 26, 443–458 Marquardt, D.W., (1963)\nAn Algorithm for Least-Squares Estimation of Nonlinear Parameters, Journal of the Society for Industrial & Applied Mathematics 11, 2, 431–441\nSo for the initial parameters of a=0.1 and b=0.2 we will be finding estimated parameters from different analytical methods which would maximize the Log Likelihood value of the Beta-Binomial distribution.\nFirst we are transforming the given EstMLEBetaBin function to satisfy the maxLik function conditions.\n\n# new function to facilitate maxLik criteria\n# only one input but has two elements\nformaxLik&lt;-function(a)\n  {\n  -EstMLEBetaBin(x=Alcohol_data$Days, freq=Alcohol_data$week1,a=a[1],b=a[2])\n  }\n\nSo the formaxLik function can be used as above and parameters are estimated for \\(\\alpha\\) and \\(\\beta\\) (or a, b) for the Alcohol Consumption data week 1. Further the maxLik function can be scrutinized as below.\n\npackage : maxLik\nNo of Inputs: 6\nMinimum required Inputs : 2\nClass of output : list or class of maxim or class of maxLik\nNo of outputs: 11\nNo of Analytical Methods : 7\nDefault Method : Automatically chosen"
  },
  {
    "objectID": "posts/Benchmarking/maxLik/index.html#nr-method",
    "href": "posts/Benchmarking/maxLik/index.html#nr-method",
    "title": "Benchmarking the maxLik function",
    "section": "NR method",
    "text": "NR method\nNR is an abbreviation for Unconstrained and equality-constrained maximization based on the quadratic approximation (Newton) method. The idea of the Newton method is to approximate the function at a given location by a multidimensional quadratic function, and use the estimated maximum as the start value for the next iteration.\n\nlibrary(maxLik)\n\n# optimizing values for a,b using NR analytical method\nNR_answer&lt;-maxLik(logLik =formaxLik, start = c(0.1,0.2),method = \"NR\")\n\n# obtaining class of output\nclass(NR_answer)\n\n# length of output\nlength(NR_answer)\n\n# the outputs\nNR_answer$estimate # estimated values for a, b\nNR_answer$maximum # minimized function value \nNR_answer$iterations  # no of iterations to succeed\nNR_answer$gradient # last gradient value which was calculated\nNR_answer$message # additional information\nNR_answer$hessian # hessian matrix\nNR_answer$code # indicates successful completion\nNR_answer$fixed # logical vector indicating which parameters are constants\nNR_answer$type # type of maximization\nNR_answer$last.step # list describing the last unsuccessful step\nNR_answer$control # see the documentation understand\n\n# fitting the Beta-Binomial distribution with estimated shape parameter values\nfitBetaBin(Alcohol_data$Days,Alcohol_data$week1,\n           NR_answer$estimate[1],NR_answer$estimate[2])"
  },
  {
    "objectID": "posts/Benchmarking/maxLik/index.html#bfgs-method",
    "href": "posts/Benchmarking/maxLik/index.html#bfgs-method",
    "title": "Benchmarking the maxLik function",
    "section": "BFGS method",
    "text": "BFGS method\nBFGS is a Quasi-Newton method (also known as a variable metric algorithm), specifically that published simultaneously in 1970 by Broyden, Fletcher, Goldfarb and Shanno. This uses function values and gradients to build up a picture of the surface to be optimized.\nReference : Broyden, C.G., 1967. Quasi-Newton methods and their application to function minimization. Mathematics of Computation, 21(99), pp.368-381.\n\n# optimizing values for a,b using BFGS analytical method\nBFGS_answer&lt;-maxLik(logLik =formaxLik, start = c(0.1,0.2),method = \"BFGS\")\n\n# obtaining class of output\nclass(BFGS_answer)\n\n# length of output\nlength(BFGS_answer)\n\n# the outputs\nBFGS_answer$estimate # estimated values for a, b\nBFGS_answer$maximum # minimized function value \nBFGS_answer$iterations  # no of iterations to succeed\nBFGS_answer$gradient # last gradient value which was calculated\nBFGS_answer$message # additional information\nBFGS_answer$hessian # hessian matrix\nBFGS_answer$code # indicates successful completion\nBFGS_answer$fixed # logical vector indicating which parameters are constants\nBFGS_answer$type # type of maximization\nBFGS_answer$last.step # list describing the last unsuccessful step\nBFGS_answer$control # see the documentation understand\n\n# fitting the Beta-Binomial distribution with estimated shape parameter values\nfitBetaBin(Alcohol_data$Days,Alcohol_data$week1,\n           BFGS_answer$estimate[1],BFGS_answer$estimate[2])"
  },
  {
    "objectID": "posts/Benchmarking/maxLik/index.html#bfgsr-method",
    "href": "posts/Benchmarking/maxLik/index.html#bfgsr-method",
    "title": "Benchmarking the maxLik function",
    "section": "BFGSR method",
    "text": "BFGSR method\nCombination of two methods which are Newton-Raphson, BFGS (Broyden 1970, Fletcher 1970, Goldfarb 1970, Shanno 1970).\nReference : Broyden, C.G., 1967. Quasi-Newton methods and their application to function minimization. Mathematics of Computation, 21(99), pp.368-381.\n\n# optimizing values for a,b using BFGSR analytical method\nBFGSR_answer&lt;-maxLik(logLik =formaxLik, start = c(0.1,0.2),method = \"BFGSR\")\n\n# obtaining class of output\nclass(BFGSR_answer)\n\n# length of output\nlength(BFGSR_answer)\n\n# the outputs\nBFGSR_answer$estimate # estimated values for a, b\nBFGSR_answer$maximum # minimized function value \nBFGSR_answer$iterations  # no of iterations to succeed\nBFGSR_answer$gradient # last gradient value which was calculated\nBFGSR_answer$message # additional information\nBFGSR_answer$hessian # hessian matrix\nBFGSR_answer$code # indicates successful completion\nBFGSR_answer$fixed # logical vector indicating which parameters are constants\nBFGSR_answer$type # type of maximization\nBFGSR_answer$last.step # list describing the last unsuccessful step\nBFGSR_answer$control # see the documentation understand\n\n# fitting the Beta-Binomial distribution with estimated shape parameter values\nfitBetaBin(Alcohol_data$Days,Alcohol_data$week1,\n           BFGSR_answer$estimate[1],BFGSR_answer$estimate[2])"
  },
  {
    "objectID": "posts/Benchmarking/maxLik/index.html#bhhh-method",
    "href": "posts/Benchmarking/maxLik/index.html#bhhh-method",
    "title": "Benchmarking the maxLik function",
    "section": "BHHH method",
    "text": "BHHH method\nBHHH method (Berndt, Hall, Hall, Hausman 1974). The BHHH (information equality) approximation is only valid for log-likelihood functions. It requires the score (gradient) values by individual observations and hence those must be returned by individual observations by grad or fn. With the complexity of BHHH method I choose not to discuss it here, but a reference is mentioned to anyone who has interest in this analytical method.\nReference : Berndt, E., Hall, B., Hall, R. and Hausman, J. (1974): Estimation and Inference in Nonlinear Structural Models, Annals of Social Measurement 3, 653–665."
  },
  {
    "objectID": "posts/Benchmarking/maxLik/index.html#sann-method",
    "href": "posts/Benchmarking/maxLik/index.html#sann-method",
    "title": "Benchmarking the maxLik function",
    "section": "SANN method",
    "text": "SANN method\nMethod SANN is by default a variant of simulated annealing given in Belisle (1992). Simulated-annealing belongs to the class of stochastic global optimization methods. It uses only function values but is relatively slow. It will also work for non-differential functions. This implementation uses the Metropolis function for the acceptance probability.\nBy default the next candidate point is generated from a Gaussian Markov kernel with scale proportional to the actual temperature. If a function to generate a new candidate point is given, method SANN can also be used to solve combinatorial optimization problems. Temperatures are decreased according to the logarithmic cooling schedule as given in Belisle (1992, p.890); specifically, the temperature is set to \\(temp / log(((t-1) %/% tmax)*tmax + exp(1))\\), where \\(t\\) is the current iteration step and temp and tmax are specifiable via control.\nNote that the SANN method depends critically on the settings of the control parameters. It is not a general-purpose method but can be very useful in getting to a good value on a very rough surface.\nReference : Belisle, C.J., 1992. Convergence theorems for a class of simulated annealing algorithms on R d. Journal of Applied Probability, 29(4), pp.885-895.\n\n# optimizing values for a,b using SANN analytical method\nSANN_answer&lt;-maxLik(logLik =formaxLik, start = c(0.1,0.2),method = \"SANN\")\n\n# obtaining class of output\nclass(SANN_answer)\n\n# length of output\nlength(SANN_answer)\n\n# the outputs\nSANN_answer$estimate # estimated values for a, b\nSANN_answer$maximum # minimized function value \nSANN_answer$iterations  # no of iterations to succeed\nSANN_answer$gradient # last gradient value which was calculated\nSANN_answer$message # additional information\nSANN_answer$hessian # hessian matrix\nSANN_answer$code # indicates successful completion\nSANN_answer$fixed # logical vector indicating which parameters are constants\nSANN_answer$type # type of maximization\nSANN_answer$last.step # list describing the last unsuccessful step\nSANN_answer$control # see the documentation understand\n\n# fitting the Beta-Binomial distribution with estimated shape parameter values\nfitBetaBin(Alcohol_data$Days,Alcohol_data$week1,\n           SANN_answer$estimate[1],SANN_answer$estimate[2])"
  },
  {
    "objectID": "posts/Benchmarking/maxLik/index.html#cg-method",
    "href": "posts/Benchmarking/maxLik/index.html#cg-method",
    "title": "Benchmarking the maxLik function",
    "section": "CG method",
    "text": "CG method\nMethod CG is a conjugate gradients method based on that by Fletcher and Reeves (1964) (but with the option of Polak-Ribiere or Beale-Sorenson updates). Conjugate gradient methods will generally be more fragile than the BFGS method, but as they do not store a matrix they may be successful in much larger optimization problems.\nReference : Fletcher, R. and Reeves, C.M., 1964. Function minimization by conjugate gradients. The computer journal, 7(2), pp.149-154.\n\n# optimizing values for a,b using CG analytical method\nCG_answer&lt;-maxLik(logLik =formaxLik, start = c(0.1,0.2),method = \"CG\")\n\n# obtaining class of output\nclass(CG_answer)\n\n# length of output\nlength(CG_answer)\n\n# the outputs\nCG_answer$estimate # estimated values for a, b\nCG_answer$maximum # minimized function value \nCG_answer$iterations  # no of iterations to succeed\nCG_answer$gradient # last gradient value which was calculated\nCG_answer$message # additional information\nCG_answer$hessian # hessian matrix\nCG_answer$code # indicates successful completion\nCG_answer$fixed # logical vector indicating which parameters are constants\nCG_answer$type # type of maximization\nCG_answer$last.step # list describing the last unsuccessful step\nCG_answer$control # see the documentation understand\n\n# fitting the Beta-Binomial distribution with estimated shape parameter values\nfitBetaBin(Alcohol_data$Days,Alcohol_data$week1,\n           CG_answer$estimate[1],CG_answer$estimate[2])"
  },
  {
    "objectID": "posts/Benchmarking/maxLik/index.html#nm-method",
    "href": "posts/Benchmarking/maxLik/index.html#nm-method",
    "title": "Benchmarking the maxLik function",
    "section": "NM method",
    "text": "NM method\nNM is the abbreviation to Nelder and Mead method. According to the documentation it uses only function values and is robust but relatively slow. It will work reasonably well for non-differential functions.\nReference : Nelder, J.A. and Mead, R., 1965. A simplex method for function minimization. The computer journal, 7(4), pp.308-313.\n\n# optimizing values for a,b using NM analytical method\nNM_answer&lt;-maxLik(logLik =formaxLik, start = c(0.1,0.2),method = \"NM\")\n\n# obtaining class of output\nclass(NM_answer)\n\n# length of output\nlength(NM_answer)\n\n# the outputs\nNM_answer$estimate # estimated values for a, b\nNM_answer$maximum # minimized function value \nNM_answer$iterations  # no of iterations to succeed\nNM_answer$gradient # last gradient value which was calculated\nNM_answer$message # additional information\nNM_answer$hessian # hessian matrix\nNM_answer$code # indicates successful completion\nNM_answer$fixed # logical vector indicating which parameters are constants\nNM_answer$type # type of maximization\nNM_answer$last.step # list describing the last unsuccessful step\nNM_answer$control # see the documentation understand\n\n# fitting the Beta-Binomial distribution with estimated shape parameter values\nfitBetaBin(Alcohol_data$Days,Alcohol_data$week1,\n           NM_answer$estimate[1],NM_answer$estimate[2])"
  },
  {
    "objectID": "posts/Benchmarking/optim/index.html",
    "href": "posts/Benchmarking/optim/index.html",
    "title": "Benchmarking the optim function",
    "section": "",
    "text": "NOTE : Below post is valid for Package version 1.4.0 and Before."
  },
  {
    "objectID": "posts/Benchmarking/optim/index.html#brief-of-optim-function",
    "href": "posts/Benchmarking/optim/index.html#brief-of-optim-function",
    "title": "Benchmarking the optim function",
    "section": "Brief of optim Function",
    "text": "Brief of optim Function\nReading through the optim function brief from the previous post it will help the reader regarding operational questions of the function.\nSo for the initial parameters of a=0.1 and b=0.2 we will be finding estimated parameters from different analytical methods which would minimize the Negative Log Likelihood value of the Beta-Binomial distribution.\nFirst we are transforming the given EstMLEBetaBin function to satisfy the optim function conditions.\n\n# new function to facilitate optim criteria\n# only one input but has two elements\nforoptim&lt;-function(a)\n  {\n  EstMLEBetaBin(x=Alcohol_data$Days, freq=Alcohol_data$week1,a=a[1],b=a[2])\n  }\n\nSo the foroptim function can be used as above and parameters are estimated for \\(\\alpha\\) and \\(\\beta\\) (or a, b) for the Alcohol Consumption data week 1. Further the optim function can be scrutinized as below.\n\npackage : stats\nNo of Inputs: 7\nMinimum required Inputs : 2\nClass of output : list\nNo of outputs: 5\nNo of Analytical Methods : 6\nDefault Method : Nelder-Mead"
  },
  {
    "objectID": "posts/Benchmarking/optim/index.html#nelder-and-mead-method",
    "href": "posts/Benchmarking/optim/index.html#nelder-and-mead-method",
    "title": "Benchmarking the optim function",
    "section": "Nelder and Mead method",
    "text": "Nelder and Mead method\nDefault analytical method is Nelder and Mead method. According to the documentation it uses only function values and is robust but relatively slow. It will work reasonably well for non-differential functions.\nReference : Nelder, J.A. and Mead, R., 1965. A simplex method for function minimization. The computer journal, 7(4), pp.308-313.\nBelow is the code of using optim function with Nelder and Mead analytical method.\n\n# optimizing values for a,b using default analytical method or Nelder and Mead\nNM_answer&lt;-optim(par=c(0.1,0.2),fn=foroptim)\n\n# the outputs\nNM_answer$par # estimated values for a, b\nNM_answer$value # minimized function value \nNM_answer$counts  # see the documentation to understand\nNM_answer$convergence # indicates successful completion\nNM_answer$message # additional information\n\n# fitting the Beta-Binomial distribution with estimated shape parameter  values\nfitBetaBin(Alcohol_data$Days,Alcohol_data$week1,NM_answer$par[1],NM_answer$par[2])"
  },
  {
    "objectID": "posts/Benchmarking/optim/index.html#bfgs-method",
    "href": "posts/Benchmarking/optim/index.html#bfgs-method",
    "title": "Benchmarking the optim function",
    "section": "BFGS method",
    "text": "BFGS method\nThe documentation indicates that BFGS is a Quasi-Newton method (also known as a variable metric algorithm), specifically that published simultaneously in 1970 by Broyden, Fletcher, Goldfarb and Shanno. This uses function values and gradients to build up a picture of the surface to be optimized.\nReference : Broyden, C.G., 1967. Quasi-Newton methods and their application to function minimization. Mathematics of Computation, 21(99), pp.368-381.\nBelow is the code for using optim function with BFGS analytical method\n\n# optimizing values for a,b using BFGS inputs\nBFGS_answer&lt;-optim(par=c(0.1,0.2),fn=foroptim,method = \"BFGS\")\n\n# the outputs\nBFGS_answer$par # estimated values for a, b\nBFGS_answer$value # minimized function value \nBFGS_answer$counts  # see the documentation to understand\nBFGS_answer$convergence # indicates successful completion\nBFGS_answer$message # additional information\n\n# fitting the Beta-Binomial distribution with estimated shape parameter  values\nfitBetaBin(Alcohol_data$Days,Alcohol_data$week1,\n           BFGS_answer$par[1],BFGS_answer$par[2])"
  },
  {
    "objectID": "posts/Benchmarking/optim/index.html#cg-method",
    "href": "posts/Benchmarking/optim/index.html#cg-method",
    "title": "Benchmarking the optim function",
    "section": "CG method",
    "text": "CG method\nThe documentation indicates the Method CG is a conjugate gradients method based on that by Fletcher and Reeves (1964) (but with the option of Polak–Ribiere or Beale–Sorenson updates). Conjugate gradient methods will generally be more fragile than the BFGS method, but as they do not store a matrix they may be successful in much larger optimization problems.\nReference : Fletcher, R. and Reeves, C.M., 1964. Function minimization by conjugate gradients. The computer journal, 7(2), pp.149-154.\nUsing CG method with optim function is explained below\n\n# optimizing values for a,b using CG inputs\nCG_answer&lt;-optim(par=c(0.1,0.2),fn=foroptim,method = \"CG\")\n\n# the outputs\nCG_answer$par # estimated values for a, b\nCG_answer$value # minimized function value \nCG_answer$counts  # see the documentation to understand\nCG_answer$convergence # indicates successful completion\nCG_answer$message # additional information\n\n# fitting the Beta-Binomial distribution with estimated shape parameter  values\nfitBetaBin(Alcohol_data$Days,Alcohol_data$week1,CG_answer$par[1],CG_answer$par[2])"
  },
  {
    "objectID": "posts/Benchmarking/optim/index.html#l-bfgs-b-method",
    "href": "posts/Benchmarking/optim/index.html#l-bfgs-b-method",
    "title": "Benchmarking the optim function",
    "section": "L-BFGS-B method",
    "text": "L-BFGS-B method\nMethod L-BFGS-B is that of Byrd et. al. (1995) which allows box constraints, that is each variable can be given a lower and/or upper bound. The initial value must satisfy the constraints. This uses a limited-memory modification of the BFGS quasi-Newton method. If non-trivial bounds are supplied, this method will be selected, with a warning.\nReference : Byrd, R.H., Lu, P., Nocedal, J. and Zhu, C., 1995. A limited memory algorithm for bound constrained optimization. SIAM Journal on Scientific Computing, 16(5), pp.1190-1208.\nRefer the below code chunk to under the L-BFGS-B method from optim function\n\n# optimizing values for a,b using L-BFGS-B inputs\nL_BFGS_B_answer&lt;-optim(par=c(0.1,0.2),fn=foroptim,method = \"L-BFGS-B\")\n\n# the outputs\nL_BFGS_B_answer$par # estimated values for a, b\nL_BFGS_B_answer$value # minimized function value \nL_BFGS_B_answer$counts  # see the documentation to understand\nL_BFGS_B_answer$convergence # indicates successful completion\nL_BFGS_B_answer$message # additional information\n\n# fitting the Beta-Binomial distribution with estimated shape parameter  values\nfitBetaBin(Alcohol_data$Days,Alcohol_data$week1,\n           L_BFGS_B_answer$par[1],L_BFGS_B_answer$par[2])"
  },
  {
    "objectID": "posts/Benchmarking/optim/index.html#sann-method",
    "href": "posts/Benchmarking/optim/index.html#sann-method",
    "title": "Benchmarking the optim function",
    "section": "SANN method",
    "text": "SANN method\nMethod SANN is by default a variant of simulated annealing given in Belisle (1992). Simulated-annealing belongs to the class of stochastic global optimization methods. It uses only function values but is relatively slow. It will also work for non-differential functions. This implementation uses the Metropolis function for the acceptance probability.\nBy default the next candidate point is generated from a Gaussian Markov kernel with scale proportional to the actual temperature. If a function to generate a new candidate point is given, method SANN can also be used to solve combinatorial optimization problems. Temperatures are decreased according to the logarithmic cooling schedule as given in Belisle (1992, p.890); specifically, the temperature is set to \\(temp / log(((t-1) %/% tmax)*tmax + exp(1))\\), where \\(t\\) is the current iteration step and temp and tmax are specifiable via control.\nNote that the SANN method depends critically on the settings of the control parameters. It is not a general-purpose method but can be very useful in getting to a good value on a very rough surface.\nReference : Belisle, C.J., 1992. Convergence theorems for a class of simulated annealing algorithms on R d. Journal of Applied Probability, 29(4), pp.885-895.\nBelow mentioned code chunk is simply using SANN method for optim function\n\n# optimizing values for a,b using default inputs\nSANN_answer&lt;-optim(par=c(0.1,0.2),fn=foroptim,method = \"SANN\")\n\n# the outputs\nSANN_answer$par # estimated values for a, b\nSANN_answer$value # minimized function value \nSANN_answer$counts  # see the documentation to understand\nSANN_answer$convergence # indicates successful completion\nSANN_answer$message # additional information\n\n# fitting the Beta-Binomial distribution with estimated shape parameter  values\nfitBetaBin(Alcohol_data$Days,Alcohol_data$week1,\n           SANN_answer$par[1],SANN_answer$par[2])"
  },
  {
    "objectID": "posts/Benchmarking/optim/index.html#brent-method",
    "href": "posts/Benchmarking/optim/index.html#brent-method",
    "title": "Benchmarking the optim function",
    "section": "Brent method",
    "text": "Brent method\nBrent Method is for one-dimensional problems only, using optimize(). It can be useful in cases where optim() is used inside other functions where only method can be specified, such as in mle from package stats4. Brent method does not work for our situation.\nReference : Brent, R.P., 2013. Algorithms for minimization without derivatives. Courier Corporation."
  },
  {
    "objectID": "posts/Developing/FindRpackage/index.html",
    "href": "posts/Developing/FindRpackage/index.html",
    "title": "How To Find Your R package ?",
    "section": "",
    "text": "How to find your R package is simply a blog post helping people to provide a list of websites where they can find R packages. These websites were useful for me while developing my own R package fitODBOD. So that I would be sure that fitODBOD is a unique package and what its functions should be able to do.\nThis is a list with 16 items\n\n1. Google\nWhen you have no idea to find a package first thing is to “Google”.\nLink \n\n\n2. CRAN\nOfficial website to find standard packages. The packages downloaded here will have documentation manuals, vignettes and sometimes journal articles which would simplify work for people who use them.\nLink \n\n\n3. Bio - Conductor\nAnother standard location to publish your R package, but only related to the field of Biology.\nLink \n\n\n4. GitHub pages\nIf CRAN or Bio - Conductor is with high standards or too much work for your package you can still publish it and the ideal place for this is GitHub.\nLink \n\n\n5. Rdocumentation\nA place to find interactive documentation for the packages in CRAN, Bio - Conductor and GitHub. They simply include everything in the manual of a package but in html format.\nLink \n\n\n6. Crantastic\nAll packages which are a part of CRAN is in this website. We can search packages based on Authors, package name, reviews and tags.\nLink \n\n\n7. rpackages\nSimilar to crantastic this website also provides information to CRAN packages, but it is better because package related statistics is also shown here.\nLink \n\n\n8. R - Opensci\nSearch range for R packages in this website has more categories which is informative. I would say better than above mentioned ones.\nLink \n\n\n9. Rseek\nThis is like a google search engine for R packages.\nLink \n\n\n10. R Site Search\nWebsite dedicated to search R functions, package vignettes and task views.\nLink \n\n\n11. R-forge\nProjects related to R are mentioned in this website and how progress has been made on them is also here. Most of these projects will be published as packages later with significant importance.\nLink \n\n\n12. AwesomeR\nThis is a website which has R packages based on topics related to statistics. Some of these topics are Machine Learning, Bayesian, Optimization, Bio statistics and much more.\nLink \n\n\n13. CRAN Task View\nTopic related R packages are bundled together in this website. Further, the topics give a brief explanation, but the webpages give an extensive amount of information about what is unique is these packages. Also you do not need internet to use this because it is part of Rstudio help.\nLink \n\n\n14. Rstudio - Rpackages\nSeveral crucial packages which would be very useful are considered here. All of these packages are projects. Further they are very popular in the R community.\nLink \n\n\n15. stack overflow - r\nIf you cannot achieve something very specific related to R coding it is possible to use the website. It provides answers from other R users, sometimes even blogs related to the issues with solutions.\nLink \n\n\n16. CRANalerts\nThis is an email service which would alert us regarding specific packages accordance to our request. Whenever there is an update for a chosen package we would receive an email alert.\nLink \nThis is my list of places for reaching out to help with related to R packages and R programming. I use them constantly and they are very much helpful to me. Finally, I hope this post would be useful to anyone who wants to find or use R packages.\nTHANK YOU"
  },
  {
    "objectID": "posts/Developing/OlympicRshiny/index.html",
    "href": "posts/Developing/OlympicRshiny/index.html",
    "title": "Olympic : Rshiny Approach",
    "section": "",
    "text": "Rshiny is very popular in the rstats community. The glamourous interface and functionality has helped for this level of popularity. In perspective of using an Rshiny App anyone can use it with minimal amount of knowledge. Which is very useful in bringing statistical analysis to consumers or general public without any trouble.\nI initially wanted to develop an Rshiny App for my fitODBOD package, but I thought it would be best to test the waters. That is what I have done here. Using the Olympic data from kaggle I have found a very convenient way to understand specific results for a choosen country from the Rshiny App.\n\nAt the beginning I wanted to compare between diferent countries or sports or seasons and come to a conclusion. Well, what kind of a conclusion would make sense bothered me, therefore I turned towards an Rshiny Approach.\nThis data-set includes information from 1896 to 2016. Analyzing the data-set would take tedious amount of time and in my opinion unnecessary amount of complications will arise when it comes to concluding. Information from the data-set includes about Medals, participants name, country, sports, events, season and year.\nKaggle Olympic Data\nOlympic Rshiny App\nGitHub Code\n\n\nEasiest way to build your own shiny app is to refer the official website. It provides an extensive amount of information regarding Rshiny development. Already developed Rshiny Apps and Templates are also available, which would come in handy. Further, when you do start an Rshiny App through Rstudio you will initially receive a sample App with its code. A few tweaks and changes would lead to necessary changes that you need.\nOfficial Rshiny Website\n\n\n\nInstructions are also listed in the Rshiny App panel.\n\n\nFirst Choose a country that you want to study and find the three letter NOC CODE from the “NOC CODE” tab.\n\n\n\nChoose the “GRAPH” tab to understand how medals were won for a chosen country over the years with respective to Gender.\n\n\n\nChoose the “DATA” tab to look at the data for the chosen. Further you can scroll through this data and find specific attendee’s Name, Sex, Age, Year, Season, City, Sport, Event and Medal.\n\n\n\nUsing “DESCRIBE” tab you can simply study the descriptive statistics for the data of the chosen country.\n\n\n\n“G/Years” tab is there to explain the Gender representation over the years of the chosen country through a bar plot.\n\n\n\n“S/Years” tab shows a bar plot which has the representation of the Gender of the Sports event participants of the chosen country.\n\n\n\n“H/W/Sport” tab explores how participants Height and Weight relationship for each Sporting event with respective to Gender for the chosen country.\n\n\n\nRepeat the Steps 1 to 7 and be amused of the results from different countries.\nPLEASE NOTE - You should remember that as a user of this Rshiny Application not all countries have won atleast one medal at the Olympics. At these occurences “MEDAL GRAPH” tab does not show any graph but only an error. This can be confirmed by the “DESCRIBE” tab which will produce the summary for that chosen country.\nTHANK YOU"
  },
  {
    "objectID": "posts/Developing/OlympicRshiny/index.html#material-useful-for-rsiny-development",
    "href": "posts/Developing/OlympicRshiny/index.html#material-useful-for-rsiny-development",
    "title": "Olympic : Rshiny Approach",
    "section": "",
    "text": "Easiest way to build your own shiny app is to refer the official website. It provides an extensive amount of information regarding Rshiny development. Already developed Rshiny Apps and Templates are also available, which would come in handy. Further, when you do start an Rshiny App through Rstudio you will initially receive a sample App with its code. A few tweaks and changes would lead to necessary changes that you need.\nOfficial Rshiny Website"
  },
  {
    "objectID": "posts/Developing/OlympicRshiny/index.html#how-to-use-the-olympic-rshiny-app",
    "href": "posts/Developing/OlympicRshiny/index.html#how-to-use-the-olympic-rshiny-app",
    "title": "Olympic : Rshiny Approach",
    "section": "",
    "text": "Instructions are also listed in the Rshiny App panel.\n\n\nFirst Choose a country that you want to study and find the three letter NOC CODE from the “NOC CODE” tab.\n\n\n\nChoose the “GRAPH” tab to understand how medals were won for a chosen country over the years with respective to Gender.\n\n\n\nChoose the “DATA” tab to look at the data for the chosen. Further you can scroll through this data and find specific attendee’s Name, Sex, Age, Year, Season, City, Sport, Event and Medal.\n\n\n\nUsing “DESCRIBE” tab you can simply study the descriptive statistics for the data of the chosen country.\n\n\n\n“G/Years” tab is there to explain the Gender representation over the years of the chosen country through a bar plot.\n\n\n\n“S/Years” tab shows a bar plot which has the representation of the Gender of the Sports event participants of the chosen country.\n\n\n\n“H/W/Sport” tab explores how participants Height and Weight relationship for each Sporting event with respective to Gender for the chosen country.\n\n\n\nRepeat the Steps 1 to 7 and be amused of the results from different countries.\nPLEASE NOTE - You should remember that as a user of this Rshiny Application not all countries have won atleast one medal at the Olympics. At these occurences “MEDAL GRAPH” tab does not show any graph but only an error. This can be confirmed by the “DESCRIBE” tab which will produce the summary for that chosen country.\nTHANK YOU"
  },
  {
    "objectID": "posts/Developing/SLelection/Presidential_Election/index.html",
    "href": "posts/Developing/SLelection/Presidential_Election/index.html",
    "title": "Extract Presidential Election Data of 2015 from the Pdf file",
    "section": "",
    "text": "Sri Lanka will face a presidential election by the end of this year, but in order to understand how people have voted so far in previous elections we need data. I was hoping this data could be in opendata initiative website, unfortunately I was not happy with the results. Only one set of data was presented here, which was for District Registered Electors from 2007 - 2017.\n\nWell I did not give up, because there is the Elections Commission website. Hopefully, I found the data which was needed under the title Presidential Election Results. Still there is an issue where all the election results are in pdf files. Each pdf file had data of each presidential election. After skimming through these six election results in pdf files it was clear that not all of them have the same format or pattern in representing the data. But this will not be an issue if we had the data in a csv or excel file which could be useful for researchers or investigative journalists.\nSo my knowledge as an R programmer will come in handy for extracting tables from these pdfs. Also this blog post is about extracting data from the pdf file of Presidential Election results in 2015. I will be using the packages pdftools, stringr, data.table and splitstackshape."
  },
  {
    "objectID": "posts/Developing/SLelection/Presidential_Election/index.html#where-is-the-data",
    "href": "posts/Developing/SLelection/Presidential_Election/index.html#where-is-the-data",
    "title": "Extract Presidential Election Data of 2015 from the Pdf file",
    "section": "",
    "text": "Well I did not give up, because there is the Elections Commission website. Hopefully, I found the data which was needed under the title Presidential Election Results. Still there is an issue where all the election results are in pdf files. Each pdf file had data of each presidential election. After skimming through these six election results in pdf files it was clear that not all of them have the same format or pattern in representing the data. But this will not be an issue if we had the data in a csv or excel file which could be useful for researchers or investigative journalists.\nSo my knowledge as an R programmer will come in handy for extracting tables from these pdfs. Also this blog post is about extracting data from the pdf file of Presidential Election results in 2015. I will be using the packages pdftools, stringr, data.table and splitstackshape."
  },
  {
    "objectID": "posts/Developing/SLelection/Presidential_Election/index.html#page-types",
    "href": "posts/Developing/SLelection/Presidential_Election/index.html#page-types",
    "title": "Extract Presidential Election Data of 2015 from the Pdf file",
    "section": "Page types",
    "text": "Page types\n\n\nNo Info\n\n\n\nTwo Tables\n\n\n\nOne Table"
  },
  {
    "objectID": "posts/Developing/SLelection/Presidential_Election/index.html#table-types",
    "href": "posts/Developing/SLelection/Presidential_Election/index.html#table-types",
    "title": "Extract Presidential Election Data of 2015 from the Pdf file",
    "section": "Table types",
    "text": "Table types\n\n\nTwo Electorate Results\n\n\n\nOne Electorate and Postal Votes\n\n\n\nPostal Votes and Final District Results\n\n\n\nFinal District Results only\n\nSo from these tables only we need to extract information and create one large data-set."
  },
  {
    "objectID": "posts/Developing/SLelection/Presidential_Election/index.html#step-1",
    "href": "posts/Developing/SLelection/Presidential_Election/index.html#step-1",
    "title": "Extract Presidential Election Data of 2015 from the Pdf file",
    "section": "Step 1",
    "text": "Step 1\nUsing the pdf_text function on the pdf file we will produce a character class output where each page is represented by a list(so 111 lists in this one output). Now consider one list which represents a page but it is still one line character. Convert this one line character into a multiple row list using str_split function and *pattern. So for the first table the information is from row 4 to 26 and second table data is from row 29 to 51.\n\n# load the packages \nlibrary(pdftools)\n\nUsing poppler version 22.04.0\n\nlibrary(stringr)\nlibrary(splitstackshape)\nlibrary(data.table)\n\n# load the file \nSL_PE_2015&lt;-pdf_text(\"PresidentialElections2015.pdf\")\n\npage&lt;-2\n\n# split the one large list into a data frame of lines and separate them into two tables\ntable1&lt;-data.table(str_split(SL_PE_2015[[page]],\"\\n\")[[1]][4:26])\ntable2&lt;-data.table(str_split(SL_PE_2015[[page]],\"\\n\")[[1]][29:51])\n\n# name the only column in these two data tables\nnames(table1)&lt;-\"hello\"\nnames(table2)&lt;-\"hello\"\n\n\nNow we have two one column data-frames with exact rows as the page of the pdf file we are extracting."
  },
  {
    "objectID": "posts/Developing/SLelection/Presidential_Election/index.html#step-2",
    "href": "posts/Developing/SLelection/Presidential_Election/index.html#step-2",
    "title": "Extract Presidential Election Data of 2015 from the Pdf file",
    "section": "Step 2",
    "text": "Step 2\nBeginning information in these two tables contain the same information as mentioned earlier. So after removing this information we will be limited to another one column data-table but now only with votes and percentage values. This one column table can be separated by column splitting based on the pattern ” “.\n\n# creating the list with same type of information\nNames=c(\"Aithurus Mohamed Illias\",\"Ibrahim Miflar\",\"Prasanna Priyankara\",\n        \"Wimal Geeganage\",\"Sirithunga Jayasuriya\",\"M. B. Theminimulla\",\n        \"Pani Wijesiriwardane\",\"Duminda Nagamuwa\",\n        \"Panagoda Don Prince Soloman Anura Liyanage\",\n        \"Maithripala Sirisena\",\"Ruwanthilaka Peduru Arachchi\",\n        \"Anuruddha Polgampala\",\"Baththaramulle Seelarathana Thero\",\n        \"Sarath Manamendra\",\"Arachchige Rathnayaka Sirisena\",\n        \"Mahinda Rajapaksa\",\"Namal Rajapaksa\",\"Sundaram Mahendran\",\n        \"Jayantha Kulathunga\",\"Valid Votes\",\"Rejected Votes\",\n        \"Total Polled\",\"Regis.Electors\")\n\n# first using the above list remove the first column info\n# then split all rows of one column into two columns based on \" \" pattern\nTop&lt;-cSplit(lapply(table1, function(x) str_remove(x,Names)),\"hello\",\" \")\n\nWarning in type.convert.default(X[[i]], ...): 'as.is' should be specified by\nthe caller; using TRUE\n\nWarning in type.convert.default(X[[i]], ...): 'as.is' should be specified by\nthe caller; using TRUE\n\nWarning in type.convert.default(X[[i]], ...): 'as.is' should be specified by\nthe caller; using TRUE\n\nWarning in type.convert.default(X[[i]], ...): 'as.is' should be specified by\nthe caller; using TRUE\n\nBottom&lt;-cSplit(lapply(table2, function(x) str_remove(x,Names)),\"hello\",\" \")\n\nWarning in type.convert.default(X[[i]], ...): 'as.is' should be specified by\nthe caller; using TRUE\n\nWarning in type.convert.default(X[[i]], ...): 'as.is' should be specified by\nthe caller; using TRUE\n\nWarning in type.convert.default(X[[i]], ...): 'as.is' should be specified by\nthe caller; using TRUE\n\nWarning in type.convert.default(X[[i]], ...): 'as.is' should be specified by\nthe caller; using TRUE\n\nWarning in type.convert.default(X[[i]], ...): 'as.is' should be specified by\nthe caller; using TRUE\n\nWarning in type.convert.default(X[[i]], ...): 'as.is' should be specified by\nthe caller; using TRUE\n\nWarning in type.convert.default(X[[i]], ...): 'as.is' should be specified by\nthe caller; using TRUE\n\nWarning in type.convert.default(X[[i]], ...): 'as.is' should be specified by\nthe caller; using TRUE\n\n# Extract the information of sub heading above each table by removing the pattern \"\\r\"\nName1&lt;-str_remove(str_split(SL_PE_2015[[page]],\"\\n\")[[1]][2],\"\\r\")\nName2&lt;-str_remove(str_split(SL_PE_2015[[page]],\"\\n\")[[1]][27],\"\\r\")"
  },
  {
    "objectID": "posts/Developing/SLelection/Presidential_Election/index.html#step-3",
    "href": "posts/Developing/SLelection/Presidential_Election/index.html#step-3",
    "title": "Extract Presidential Election Data of 2015 from the Pdf file",
    "section": "Step 3",
    "text": "Step 3\nNow create a proper table with three columns which has that same type of information, votes(with commas) and percentages (with percentage sign). Well by creating these Top1 and Bottom1 tables we will remove the percentage sign. Then we will remove the commas(,) in votes columns and convert them into numeric class. Further, even though the percentage column looks numeric it is not. To resolve it we shall convert these values also from factor to character and then finally to numeric.\n\n# creating the new dataset with three columns without percentage sign\nTop1&lt;-data.table(\"ColumnNames\"=Names,\"Votes\"=Top$hello_1,\"Percentage\"=str_remove(Top$hello_2,\"%\"))\nBottom1&lt;-data.table(\"ColumnNames\"=Names,\"Votes\"=Bottom$hello_1,\"Percentage\"=str_remove(Bottom$hello_2,\"%\"))\n\n# remove the commas from votes columns and convert them to numeric class\nTop1[,2] &lt;- lapply(Top1[,2], function(x) as.numeric(as.character(str_remove_all(x,\",\"))))\n\nWarning in FUN(X[[i]], ...): NAs introduced by coercion\n\nBottom1[,2] &lt;- lapply(Bottom1[,2], function(x) as.numeric(as.character(str_remove_all(x,\",\"))))\n\nWarning in FUN(X[[i]], ...): NAs introduced by coercion\n\n# convert the percentage values to numeric class\nTop1[,3]&lt;-lapply(Top1[,3], function(x) as.numeric(as.character(x)))\n\nWarning in FUN(X[[i]], ...): NAs introduced by coercion\n\nBottom1[,3]&lt;-lapply(Bottom1[,3], function(x) as.numeric(as.character(x)))\n\nWarning in FUN(X[[i]], ...): NAs introduced by coercion"
  },
  {
    "objectID": "posts/Developing/SLelection/Presidential_Election/index.html#step-4",
    "href": "posts/Developing/SLelection/Presidential_Election/index.html#step-4",
    "title": "Extract Presidential Election Data of 2015 from the Pdf file",
    "section": "Step 4",
    "text": "Step 4\nNow our final tables are prepared. We shall name the columns. First column is for year, second column is for district name, third column is for that same type of information, fourth column is for number of votes and final column is for the percentage values.\n\nElectorate1&lt;-data.table(\"Year\"=2015,\n                        \"District\"=str_remove(str_split(SL_PE_2015[[page]],\"\\n\")[[1]][1],\n                                                          \" Districts Results\\r\"), \n                        \"Electorate\"=Name1,\"ColNames\"=Top1$ColumnNames,\"Votes\"=Top1$Votes,\n                        \"Percentage\"=Top1$Percentage)\nElectorate2&lt;-data.table(\"Year\"=2015,\n                        \"District\"=str_remove(str_split(SL_PE_2015[[page]],\"\\n\")[[1]][1],\n                                                          \" Districts Results\\r\"),\n                        \"Electorate\"=Name2,\"ColNames\"=Bottom1$ColumnNames,\"Votes\"=Bottom1$Votes,\n                        \"Percentage\"=Bottom1$Percentage)\n\nFinally, we have extracted two clear tables under the name Electorate1 and Electorate2. First three table types mentioned above can be extracted now."
  },
  {
    "objectID": "posts/Developing/YourOwnPackage/index.html",
    "href": "posts/Developing/YourOwnPackage/index.html",
    "title": "Developing an R package",
    "section": "",
    "text": "R package development is no longer as it was before 2010 because now most of the work can be done by just a simple mouse-click or with the use of a function. My intention of writing this blog post is not to give a thorough demonstration of how to develop your own R package. But it will briefly explain the process with the most important steps, and will include valuable blog posts and websites which helped me to develop my own R package fitODBOD."
  },
  {
    "objectID": "posts/Developing/YourOwnPackage/index.html#coding-standards-coding-to-understand",
    "href": "posts/Developing/YourOwnPackage/index.html#coding-standards-coding-to-understand",
    "title": "Developing an R package",
    "section": "1) Coding Standards (Coding to Understand)",
    "text": "1) Coding Standards (Coding to Understand)\n\nFocus on naming conventions.\nFocus on input parameters and outputs.\nFocus on indentation.\nComment regularly to make sense of the functions.\n\n\n\nSample Code"
  },
  {
    "objectID": "posts/Developing/YourOwnPackage/index.html#package-structure",
    "href": "posts/Developing/YourOwnPackage/index.html#package-structure",
    "title": "Developing an R package",
    "section": "2) Package Structure",
    "text": "2) Package Structure\n\nVery Important.\nInitially few files will be originated in the designated project folder.\nOver time we might add folders or create files manually.\nExample - tests directory, README.Rmd, …\n\n Package structure inside your project folder."
  },
  {
    "objectID": "posts/Developing/YourOwnPackage/index.html#description-file",
    "href": "posts/Developing/YourOwnPackage/index.html#description-file",
    "title": "Developing an R package",
    "section": "3) DESCRIPTION file",
    "text": "3) DESCRIPTION file\n\nFile explaining basic things related to your package.\nExample - package name, other packages needed, authors name, …\nCan edit manually or use specific R package.\n\n After changes the DESCRIPTION file"
  },
  {
    "objectID": "posts/Developing/YourOwnPackage/index.html#readme-file",
    "href": "posts/Developing/YourOwnPackage/index.html#readme-file",
    "title": "Developing an R package",
    "section": "4) README file",
    "text": "4) README file\n\nVery much optional.\nOnly used in related to GitHub submission.\nUsing Rmarkdown to generate a GitHub output document.\n\n Rmarkdown document\n GitHub document\n Preview of GitHub document"
  },
  {
    "objectID": "posts/Developing/YourOwnPackage/index.html#r-directory",
    "href": "posts/Developing/YourOwnPackage/index.html#r-directory",
    "title": "Developing an R package",
    "section": "5) /R directory",
    "text": "5) /R directory\n\nMost important directory.\nThe place where all your R code is written by you.\nBest to have separate R script files for each function.\nNeed to have a R script file for Data as well.\nR scripts can be modified further in order to create RDocumentation files(Rd files).\nThese RDocumentation files will explain about the function.\nProcessed R script files will automatically generate Rd files in the man directory.\n\n R script file with necessary roxygen tags to develop RDocumentation files."
  },
  {
    "objectID": "posts/Developing/YourOwnPackage/index.html#data-directory",
    "href": "posts/Developing/YourOwnPackage/index.html#data-directory",
    "title": "Developing an R package",
    "section": "6) /data directory",
    "text": "6) /data directory\n\nNot compulsory.\nEasy to use your own data therefore its worth it.\nThis directory will include the data-sets.\nR directory can have an R script to generate Rd files for these data-sets.\n\n data directory which includes data-sets.\n Rscript file which includes necessary roxygen tags to generate Rd files."
  },
  {
    "objectID": "posts/Developing/YourOwnPackage/index.html#tests-directory",
    "href": "posts/Developing/YourOwnPackage/index.html#tests-directory",
    "title": "Developing an R package",
    "section": "7) /tests directory",
    "text": "7) /tests directory\n\nIf your package is going to be in CRAN or going to be in a platform with large range of users this would be useful.\nUnit tests to check if functions are working properly.\nTesting if the data sets are in proper form.\n\n tests directory and files in side that directory.\n sub directory testthat which includes test R scripts for all functions and data sets.\n R script to test a function.\n R script to test a data set."
  },
  {
    "objectID": "posts/Developing/YourOwnPackage/index.html#man-directory",
    "href": "posts/Developing/YourOwnPackage/index.html#man-directory",
    "title": "Developing an R package",
    "section": "8) /man directory",
    "text": "8) /man directory\n\nThis directory will include the Rd files for all functions and data sets.\nIf you use roxygen tags there is no need to manually type them.\n\n With the help of R script files these RDocumentation files will be generated for each function and will be in the man directory.\n The RDocumentation files can be processed into html outputs or into a pdf manual.\n Rd file of a data-set which is created with the help of data R script.\n Html file which is generated with the help of Rd file for the data-set."
  },
  {
    "objectID": "posts/Developing/YourOwnPackage/index.html#namsespace-file",
    "href": "posts/Developing/YourOwnPackage/index.html#namsespace-file",
    "title": "Developing an R package",
    "section": "9) NAMSESPACE file",
    "text": "9) NAMSESPACE file\n\nA file which will have all the functions that you created for your package.\nIf a function is exported then it will be in this file.\n\n NAMESPACE file and its components."
  },
  {
    "objectID": "posts/Developing/YourOwnPackage/index.html#rbuildignore-file",
    "href": "posts/Developing/YourOwnPackage/index.html#rbuildignore-file",
    "title": "Developing an R package",
    "section": "10) .Rbuildignore file",
    "text": "10) .Rbuildignore file\n\nA document which includes what kind of files should not be used when building the package.\nExtensions of a file or partial or full name of the file can be added into this document.\n\n .Rbuildignore file of fitODBOD package."
  },
  {
    "objectID": "posts/Developing/YourOwnPackage/index.html#gitignore-file",
    "href": "posts/Developing/YourOwnPackage/index.html#gitignore-file",
    "title": "Developing an R package",
    "section": "11) .gitignore file",
    "text": "11) .gitignore file\n\nA document which includes what kind of files should not be pushed to the GitHub repository.\nExtensions of a file or partial or full name of the file can be added into this document.\n\n .gitignore file of fitODBOD package."
  },
  {
    "objectID": "posts/Subsampling/index.html",
    "href": "posts/Subsampling/index.html",
    "title": "Number of subsamples, that is sampling",
    "section": "",
    "text": "Analysing big data is computationally intensive and to resolve this scalable methods are developed, for example subsampling. Obtaining a subset of the big data based on a research aim and analysing this subset is called subsampling. Obviously it is not possible to identify the most optimal or informative subsample, however there are methods that can provide sub-optimal subsamples. A common technique is identifying a distribution for the subsampling probabilities that assigned to each observation of the big data and obtain a subsample based on these probabilities.\nIn this blog post I explore the relationship between number of subsamples with the big data and subsample size. I consider two possible scenarios, 1) subsample without repetition and 2) subsample with repetition. The number of subsamples for the first scenario can be obtained by \\(P(N,n)=\\frac{N!}{(N-n)!}\\) and for the latter through \\(P_{Rep}(N,n)=\\frac{(N+n-1)!}{n!(N-1)!}\\), where \\(N\\) is the big data size and \\(n\\) is the subsample size. Using subsampling probabilities is similar to with repetition.\nComparing \\(N,n\\) with \\(P(N,n)\\) and \\(P_{Rep}(N,n)\\) under big data is computationally impossible. Amid this to complete the comparison the following small values \\(N=40,50,\\ldots,100\\) and \\(n_i=N_i/2,N_i/2+5,N_i/2+10,\\ldots,N_i-1\\) are used. Here \\(n_i\\) represents the \\(i\\)-th element of the sequence, starting from \\(N_i/2\\) and increasing by \\(5\\) each step until reaching or surpassing \\(N_i−1\\). Note that the sequence ends before \\(N_i\\), as it is specified to go up to \\(N_i−1\\).\nBased on the setup the number of subsamples were calculated and are plotted below.\n\nlibrary(ggplot2)\n\nN_size&lt;-seq(40,100,10)\nn_size&lt;-Final_Result_No_Repetition&lt;-Final_Result_Repetition&lt;-list()\n\nfor (i in 1:length(N_size)) \n{\n  Temp_n_values&lt;-as.matrix(seq((N_size[1]/2),(N_size[i]-1),5))\n  n_size&lt;-apply(Temp_n_values,1,function(Temp_n_values){\n    factorial(N_size[i])/(factorial(Temp_n_values)*factorial(N_size[i]-Temp_n_values)) })\n  \n  Final_Result_No_Repetition[[i]]&lt;-data.frame(\"Big_data_size\"=paste0(\"N = \",N_size[i]),\n                                              \"Subsample_size\"=Temp_n_values,\n                                              \"No_of_Subsamples\"=n_size)\n  \n  n_size&lt;-apply(Temp_n_values,1,function(Temp_n_values){\n    factorial(N_size[i]+Temp_n_values -1)/(factorial(Temp_n_values)*factorial(N_size[i]-1)) })\n  \n  Final_Result_Repetition[[i]]&lt;-data.frame(\"Big_data_size\"=factor(paste0(\"N = \",N_size[i])),\n                                           \"Subsample_size\"=Temp_n_values,\n                                           \"No_of_Subsamples\"=n_size)\n}\n\nFinal_Result_No_Repetition&lt;-do.call(rbind.data.frame,Final_Result_No_Repetition)\nFinal_Result_Repetition&lt;-do.call(rbind.data.frame,Final_Result_Repetition)\n\nFinal_Result_Repetition$Big_data_size&lt;-factor(Final_Result_Repetition$Big_data_size,\n                                              levels = paste0(\"N = \",N_size),\n                                              labels = paste0(\"N = \",N_size))\n\nFinal_Result_No_Repetition$Big_data_size&lt;-factor(Final_Result_No_Repetition$Big_data_size,\n                                                 levels = paste0(\"N = \",N_size),\n                                                 labels = paste0(\"N = \",N_size))\n\nBased on subsampling without replacement it seems that the number of samples increase until the \\(n=N/2\\), and after this the number of samples descreases. This highest no of subsamples is still large even if \\(N\\) and \\(n\\) small values, for actual big data this value might not be calculable.\n\nggplot(Final_Result_No_Repetition,\n       aes(x=factor(Subsample_size),y=log10(No_of_Subsamples),group=Big_data_size,color=Big_data_size))+\n  geom_point()+geom_line()+\n  xlab(\"n (Subsample size)\")+ylab(\"log10(No of subsamples)\")+\n  ggtitle(\"Changes in number of subsamples for different big data and subsample sizes\",\n          subtitle = \"Subsamples without replacement\")+\n  scale_color_viridis_d()+labs(color=\"Big data size\")+\n  theme_bw()+theme(legend.position = \"bottom\")\n\n\n\n\nFor subsapling with replacement the number of subsamples increase gradually with the subsequent subsample sizes. Compared to subsampling without replacement with replacement has higher number of subsamples under the same set of big data and subsample sizes.\n\nggplot(Final_Result_Repetition,\n       aes(x=factor(Subsample_size),y=log10(No_of_Subsamples),group=Big_data_size,color=Big_data_size))+\n  geom_point()+geom_line()+\n  xlab(\"n (Subsample size)\")+ylab(\"log10(No of Subsamples)\")+\n  ggtitle(\"Changes in number of subsamples for different big data and subsample sizes\",\n          subtitle = \"Subsamples with replacement\")+\n  scale_color_viridis_d()+labs(color=\"Big data size\")+\n  theme_bw()+theme(legend.position = \"bottom\")\n\n\n\n\nSo in conclusion it is difficult to find an informative subsample as the number of subsamples increase exponentially. This leads to the necessity of subsampling methods that efficiently identify subsamples.\nFurther we explore the the number of subsamples under a reasonable big data and different subsample sizes with the help of the R package RcppAlgos. Here rather than focusing on the number of subsamples we look at the number of digits in the possible number of subsamples. For this we set \\(N=10000,20000,\\ldots,50000\\) and \\(n=200,300,\\ldots,1500\\), which are common values throughout articles related to subsampling for big data.\n\nlibrary(RcppAlgos)\n\nN_size&lt;-c(1:5)*10000\nn_size&lt;-c(2:15)*100\nFinal_Result_No_Repetition&lt;-Final_Result_Repetition&lt;-NULL\nFinal_Results&lt;-list()\n\nfor (i in 1:length(N_size)) \n{\n  for (j in 1:length(n_size)) \n  {\n  Final_Result_No_Repetition[j]&lt;-gmp::log10.bigz(comboCount(v=N_size[i],m=n_size[j],repetition=FALSE))\n  Final_Result_Repetition[j]&lt;-gmp::log10.bigz(comboCount(v=N_size[i],m=n_size[j],repetition=TRUE))\n  }\n  \n  data.frame(\"Big_data_size\"=factor(N_size[i]),\"Subsample_size\"=rep(n_size,2),\n             \"Type\"=factor(rep(c(\"Without Repetition\",\"With Repetition\"),each=length(n_size))),\n             \"No_of_digits\"=c(Final_Result_No_Repetition,Final_Result_Repetition))-&gt;Final_Results[[i]]\n}\n\nFinal_Results&lt;-do.call(rbind,Final_Results)\n\nggplot(Final_Results,aes(x=factor(Subsample_size),y=No_of_digits,color=Big_data_size,shape=Type,linetype=Type,\n                         group=interaction(Big_data_size,Type)))+\n  geom_point()+geom_line()+\n  scale_shape_manual(values = c(8,16) )+scale_linetype_manual(values = c(\"dotted\",\"dashed\"))+\n  xlab(\"n (Subsample size)\")+ylab(\"log10(No of digits)\")+\n  ggtitle(\"Changes in number of subsamples for different big data and subsample sizes\",\n          subtitle = \"Subsamples with and without replacement\")+\n  scale_color_viridis_d()+labs(color=\"With/without repetition\")+\n  theme_bw()+theme(legend.position = \"bottom\") +\n  guides(color=guide_legend(nrow = 2))\n\n\n\n\nAccording to the above plot the number of subsamples by with repetition has more number of digits than without repetition. The number of digits increase when the subsample size increase and they further have higher values when the big data size is larger as well. For example, for \\(N=50000\\) and \\(n=400\\) the number of subsamples has close to 1000 digits under log scale of power 10, that is the number of subsamples is close to \\(10^1000\\). For clarity 1 trillion is \\(10^12\\), hence why we need subsampling methods as it is impossible to find the best subsample through going through all possible subsamples.\nTHANK YOU"
  },
  {
    "objectID": "publication/fitODBOD/index.html",
    "href": "publication/fitODBOD/index.html",
    "title": "fitODBOD: An R Package to Model Binomial Outcome Data using Binomial Mixture and Alternate Binomial Distributions.",
    "section": "",
    "text": "The R package fitODBOD can be used to identify the best-fitting model for Over-dispersed Binomial Outcome Data (BOD). The Triangular Binomial (TriBin), Beta-Binomial (BetaBin), Kumaraswamy Binomial (KumBin), Gaussian Hypergeometric Generalized Beta-Binomial (GHGBB), Gamma Binomial (GammaBin), Grassia II Binomial (GrassiaIIBin) and McDonald Generalized Beta-Binomial (McGBB) distributions in the Family of Binomial Mixture Distributions (FBMD) are considered for model fitting in this package. Alternate Binomial Distributions such as Additive Binomial (AddBin), Beta-Correlated Binomial (BetaCorrBin), COM Poisson Binomial (COMPBin), Correlated Binomial (CorrBin), Lovinson Multiplicative Binomial (LMBin) and Multiplicative Binomial (MultiBin) distributions are used as well, replacing the traditional binomial distribution. Further, Probability Mass Function (PMF), Cumulative Probability Mass Function (CPMF), Negative Log Likelihood, Over-dispersion and parameter estimation (shape and distribution distinct parameters) can be explored for each fitted model with the fitODBOD package.\nView the Article     CRAN     Website"
  },
  {
    "objectID": "publication/fitODBOD/index.html#abstract",
    "href": "publication/fitODBOD/index.html#abstract",
    "title": "fitODBOD: An R Package to Model Binomial Outcome Data using Binomial Mixture and Alternate Binomial Distributions.",
    "section": "",
    "text": "The R package fitODBOD can be used to identify the best-fitting model for Over-dispersed Binomial Outcome Data (BOD). The Triangular Binomial (TriBin), Beta-Binomial (BetaBin), Kumaraswamy Binomial (KumBin), Gaussian Hypergeometric Generalized Beta-Binomial (GHGBB), Gamma Binomial (GammaBin), Grassia II Binomial (GrassiaIIBin) and McDonald Generalized Beta-Binomial (McGBB) distributions in the Family of Binomial Mixture Distributions (FBMD) are considered for model fitting in this package. Alternate Binomial Distributions such as Additive Binomial (AddBin), Beta-Correlated Binomial (BetaCorrBin), COM Poisson Binomial (COMPBin), Correlated Binomial (CorrBin), Lovinson Multiplicative Binomial (LMBin) and Multiplicative Binomial (MultiBin) distributions are used as well, replacing the traditional binomial distribution. Further, Probability Mass Function (PMF), Cumulative Probability Mass Function (CPMF), Negative Log Likelihood, Over-dispersion and parameter estimation (shape and distribution distinct parameters) can be explored for each fitted model with the fitODBOD package.\nView the Article     CRAN     Website"
  },
  {
    "objectID": "publication/Model_Average/index.html",
    "href": "publication/Model_Average/index.html",
    "title": "A model robust subsampling approach for Generalised Linear Models in big data settings.",
    "section": "",
    "text": "In today’s modern era of big data, computationally efficient and scalable methods are needed to support timely insights and informed decision making. One such method is subsampling, where a subset of the big data is analysed and used as the basis for inference rather than considering the whole data set. A key question when applying subsampling approaches is how to select an informative subset based on the questions being asked of the data. A recent approach for this has been proposed based on determining subsampling probabilities for each data point, but a limitation of this approach is that the appropriate subsampling probabilities rely on an assumed model for the big data. In this article, to overcome this limitation, we propose a model robust approach where a set of models is considered, and the subsampling probabilities are evaluated based on the weighted average of probabilities that would be obtained if each model was considered singularly. Theoretical results are derived to inform such an approach. Our model robust subsampling approach is applied in a simulation study and in two real-world applications where performance is compared to current subsampling practices. The results show that our model robust approach outperforms alternative methods.\nView the Article"
  },
  {
    "objectID": "publication/Model_Average/index.html#abstract",
    "href": "publication/Model_Average/index.html#abstract",
    "title": "A model robust subsampling approach for Generalised Linear Models in big data settings.",
    "section": "",
    "text": "In today’s modern era of big data, computationally efficient and scalable methods are needed to support timely insights and informed decision making. One such method is subsampling, where a subset of the big data is analysed and used as the basis for inference rather than considering the whole data set. A key question when applying subsampling approaches is how to select an informative subset based on the questions being asked of the data. A recent approach for this has been proposed based on determining subsampling probabilities for each data point, but a limitation of this approach is that the appropriate subsampling probabilities rely on an assumed model for the big data. In this article, to overcome this limitation, we propose a model robust approach where a set of models is considered, and the subsampling probabilities are evaluated based on the weighted average of probabilities that would be obtained if each model was considered singularly. Theoretical results are derived to inform such an approach. Our model robust subsampling approach is applied in a simulation study and in two real-world applications where performance is compared to current subsampling practices. The results show that our model robust approach outperforms alternative methods.\nView the Article"
  },
  {
    "objectID": "publication/OlympicRshiny/index.html",
    "href": "publication/OlympicRshiny/index.html",
    "title": "OlympicRshiny: ‘Shiny’ Application for Olympic Data.",
    "section": "",
    "text": "‘Shiny’ Application to visualize Olympic Data. From 1896 to 2016. Even Winter Olympics events are included. Data is from Kaggle.\nCRAN     Rshiny Application"
  },
  {
    "objectID": "publication/southasia_antenatal_depression/index.html",
    "href": "publication/southasia_antenatal_depression/index.html",
    "title": "Prevalence of antenatal depression in South Asia: a systematic review and meta-analysis.",
    "section": "",
    "text": "Objective - To estimate the prevalence of antenatal depression in South Asia and to examine variations by country and study characteristics to inform policy, practice and future research.\nMethods - We conducted a comprehensive search of 13 databases including international databases and databases covering scientific literature from South Asian countries in addition to Google Scholar and grey sources from 1 January 2007 to 31 May 2018. Studies reporting prevalence estimates of antenatal depression using a validated diagnostic/screening tool were identified, screened, selected and appraised. Primary outcome was proportion (%) of pregnant women identified as having antenatal depression.\nResults - Thirty-three studies involving 13087 pregnant women were included in the meta-analysis. Twelve studies were rated as high quality and 21 studies were of moderate quality. Overall pooled prevalence of antenatal depression was 24.3 % (95% Confidence Interval (CI) 19.03 to 30.47). Studies showed a high degree of heterogeneity (I2=97.66%) and evidence of publication bias (p=0.668). Prevalence rates for India (17.74%, 95% CI 11.19 to 26.96) and Sri Lanka (12.95%, 95% CI 8.29 to 19.68) were lower compared with the overall prevalence, whereas prevalence rates for Pakistan (32.2%, 95% CI 23.11 to 42.87) and Nepal (50%, 95% CI 35.64 to 64.36) were higher.\nConclusions - While robust prevalence studies are sparse in most South Asian countries, available data suggest one in four pregnant women is likely to experience antenatal depression in the region. Findings highlight the need for recognition of the issue in health policy and practice and for resource allocation for capacity building at regional and national levels for prevention, diagnosis and treatment.\nView the Article"
  },
  {
    "objectID": "publication/southasia_antenatal_depression/index.html#abstract",
    "href": "publication/southasia_antenatal_depression/index.html#abstract",
    "title": "Prevalence of antenatal depression in South Asia: a systematic review and meta-analysis.",
    "section": "",
    "text": "Objective - To estimate the prevalence of antenatal depression in South Asia and to examine variations by country and study characteristics to inform policy, practice and future research.\nMethods - We conducted a comprehensive search of 13 databases including international databases and databases covering scientific literature from South Asian countries in addition to Google Scholar and grey sources from 1 January 2007 to 31 May 2018. Studies reporting prevalence estimates of antenatal depression using a validated diagnostic/screening tool were identified, screened, selected and appraised. Primary outcome was proportion (%) of pregnant women identified as having antenatal depression.\nResults - Thirty-three studies involving 13087 pregnant women were included in the meta-analysis. Twelve studies were rated as high quality and 21 studies were of moderate quality. Overall pooled prevalence of antenatal depression was 24.3 % (95% Confidence Interval (CI) 19.03 to 30.47). Studies showed a high degree of heterogeneity (I2=97.66%) and evidence of publication bias (p=0.668). Prevalence rates for India (17.74%, 95% CI 11.19 to 26.96) and Sri Lanka (12.95%, 95% CI 8.29 to 19.68) were lower compared with the overall prevalence, whereas prevalence rates for Pakistan (32.2%, 95% CI 23.11 to 42.87) and Nepal (50%, 95% CI 35.64 to 64.36) were higher.\nConclusions - While robust prevalence studies are sparse in most South Asian countries, available data suggest one in four pregnant women is likely to experience antenatal depression in the region. Findings highlight the need for recognition of the issue in health policy and practice and for resource allocation for capacity building at regional and national levels for prevention, diagnosis and treatment.\nView the Article"
  },
  {
    "objectID": "tidytuesday/2018/Week_31/index.html",
    "href": "tidytuesday/2018/Week_31/index.html",
    "title": "Week 31 : R and Package Downloads",
    "section": "",
    "text": "# load the packages\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(lubridate)\nlibrary(ggthemr)\nlibrary(gridExtra)\nlibrary(magrittr)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(readr)\n\n# load the data\nr_downloads_year &lt;- read_csv(\"r_downloads_year.csv\", \n     col_types = cols(X1 = col_skip(), date = col_date(format = \"%Y-%m-%d\"), \n                      time = col_time(format = \"%H:%M:%S\")))\nr_downloads &lt;- read_csv(\"r-downloads.csv\", \n     col_types = cols(X1 = col_skip(), date = col_date(format = \"%Y-%m-%d\"), \n                      time = col_time(format = \"%H:%M:%S\")))\n\n\n\n{{% tweet \"1069506256761745409\" %}}\n\n\nIntroduction\nTidy Tuesday is a very good move to improve R programming for anyone who is interested in statistics. Data-sets are uploaded every Tuesday, and plots are published under the #tidytuesday. This is just me presenting a few accumulated plots for the data-set r-downloads.csv and r_downloads_year.csv.\nI shall be focusing on the data set provided on 2018 October 30th, Which is R and Package download stats. My main objective is to understand how Sri Lankan users have behaved in this data-set.\nThe packages used in R are readr,ggplot2, lubridate, ggthemr, gridExtra, magrittr, knitr and kableExtra.\nThere are 701 downloads occurred in between the given time limit of 2017 October 20th to 2018 October 20th in Sri Lanka. Similarly, if we look at the downloads on the day of 2018 October 23rd, which is 3 observations. There are 7 variables to be concerned, which are\n\ndate - date of download (y-m-d)\ntime - time of download (in UTC)\nsize - size in bytes\nversion - R release version\nos - Operating System\ncountry - Two letter ISO country code\nip_id - Anonymized daily ip code(unique identifier)\n\n\n# extracting the observations only if the country is Sri Lanka\nr_downloads_year_LK&lt;-subset.data.frame(r_downloads_year,country==\"LK\")\nr_downloads_LK&lt;-subset.data.frame(r_downloads,country==\"LK\")\n\n# number of observations \n#dim(r_downloads_year_LK)\n#dim(r_downloads_LK)\n\nOperating Systems\nWindows is not a favorable operating system for open source programming was my myth. Well, No longer I shall believe that if it is considering Sri Lankans and R programming.\n\n#checking what type of operating systems are in use\nggthemr(\"flat dark\")\nggplot(r_downloads_year_LK,aes(x=os))+geom_bar()+\n    geom_text(stat='count', aes(label=..count..), vjust=-0.5)+\n    xlab(\"Operating System\")+ylab(\"Frequency\")+\n    scale_y_continuous(breaks=seq(0,675,by=25))+\n    ggtitle(\"Operating system preference of Sri lankans for R\",\n              subtitle = \"2017 October 20th - 2018 October 20th\")\n\n\n\n# frequency table for Operating system\ntab1&lt;-round(prop.table(table(r_downloads_year_LK$os)),4)\ntab1&lt;-as.data.frame(tab1)\nnames(tab1)&lt;-c(\"Operating System\",\"Frequency\")\nkable(tab1) %&gt;% \n  kable_styling(bootstrap_options = \"striped\", full_width = F,position=\"left\") %&gt;%\n  add_header_above(c(\"Frequency table to Operating system\"=2))\n\n\n\n\n\n\n\n\n\nFrequency table to Operating system\n\n\n\nOperating System\nFrequency\n\n\n\n\nosx\n0.0371\n\n\nsrc\n0.0271\n\n\nwin\n0.9358\n\n\n\n\n\nMajority of users have used Windows which is 93.58%, while Mac users are represented with 3.71% and finally 2.71% from the source file. Next, focusing on the R versions downloaded.\nR versions\nVersions are updated regularly for R and a grand update occurred on 2018 April for the version 3.5.0. Further, versions 3.4.3 and 3.4.4 were updated in the time gap considered. There are versions from 3.0.0 and higher for Sri Lankan users. It is crucial to study this where we can understand how far does the user have knowledge about R and updating the software version.\n\n#checking what type of R versions were downloaded\nggplot(r_downloads_year_LK,aes(x=version))+geom_bar()+\n    geom_text(stat='count', aes(label=..count..), vjust=-0.5)+\n    xlab(\"R versions\")+ylab(\"Frequency\")+\n    scale_y_continuous(breaks = seq(0,275,by=25))+\n    ggtitle(\"R versions downloaded of Sri Lankans for R\",\n            subtitle = \"2017 October 20th - 2018 October 20th\")\n\n\n\n\nTable shows that version 3.5.1 represents a 36.52% followed by version 3.4.3 of 27.96% and in the third place version 3.5.0 with 15.98%. Further, all downloads are occurred for versions 3.0.0 or higher than it. People believed in 3.4.3 than 3.5.0, which could only mean that 3.4.3 was more stable for user and package requirements.\n\n# frequency table to R versions\ntab2&lt;-sort(round(prop.table(table(r_downloads_year_LK$version)),4))\ntab2&lt;-as.data.frame(tab2)\nnames(tab2)&lt;-c(\"R Version\",\"Frequency\")\nkable(tab2) %&gt;% \n  kable_styling(bootstrap_options = \"striped\", full_width = F,position=\"left\") %&gt;%\n  add_header_above(c(\"Frequency table to R versions\"=2))\n\n\n\n\n\n\n\n\n\nFrequency table to R versions\n\n\n\nR Version\nFrequency\n\n\n\n\n3.0.0\n0.0014\n\n\n3.2.2\n0.0014\n\n\n3.4.0\n0.0014\n\n\n3.3.3\n0.0029\n\n\n3.4.1\n0.0029\n\n\n3.4.4\n0.0899\n\n\n3.4.2\n0.0956\n\n\n3.5.0\n0.1598\n\n\n3.4.3\n0.2796\n\n\n3.5.1\n0.3652\n\n\n\n\n\nIf we further divide the operating systems bar plot with respective to R version it is clearly seen that only versions 3.5.1, 3.5.0, 3.4.4, 3.4.3 and 3.4.2 have maintained importance for the windows operating system.\n\n# Checking what type of operating system is used with R version\n#setting 10 colors becuase flat dark theme only has four originally\nset_swatch(c(\"white\",\"firebrick1\",\"gold\",\"darkorange\",\"dodgerblue\",\"darkblue\",\n             \"forestgreen\",\"green\",\"grey\",\"grey44\",\"black\"))\nggplot(r_downloads_year_LK,aes(x=os,fill=version))+geom_bar()+\n  geom_text(stat='count',aes(y=..count..,label=..count..),position=\"stack\",vjust=1)+\n    xlab(\"Operating System\")+ylab(\"Frequency\")+\n    scale_y_continuous(breaks=seq(0,675,by=25))+\n    ggtitle(\"Operating system preference of Sri lankans for R\",\n              subtitle = \"2017 October 20th - 2018 October 20th\")\n\n\n\n#Contingency table to R version versus operating System\nkable(round(prop.table(table(r_downloads_year_LK$os,\n                             r_downloads_year_LK$version)),4)) %&gt;%\n  kable_styling(bootstrap_options = \"striped\",full_width = T) %&gt;%\n  add_header_above(c(\"Contingency table for R version vs Operating System\"=11))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContingency table for R version vs Operating System\n\n\n\n\n3.0.0\n3.2.2\n3.3.3\n3.4.0\n3.4.1\n3.4.2\n3.4.3\n3.4.4\n3.5.0\n3.5.1\n\n\n\n\nosx\n0.0000\n0.0000\n0.0029\n0.0000\n0.0000\n0.0100\n0.0086\n0.0000\n0.0071\n0.0086\n\n\nsrc\n0.0014\n0.0014\n0.0000\n0.0000\n0.0029\n0.0043\n0.0057\n0.0000\n0.0014\n0.0100\n\n\nwin\n0.0000\n0.0000\n0.0000\n0.0014\n0.0000\n0.0813\n0.2653\n0.0899\n0.1512\n0.3466\n\n\n\n\n\nWindows operating system percentages indicate that 34.66% of users have chosen version 3.5.1, 26.53% have chosen version 3.4.3 and finally version 3.5.0 with 15.12%. Close to 9% is represented by version 3.4.2 and 3.4.4.\nDate versus Operating System\nDate is a difficult variable in statistics therefore I have disseminated the date into 4 types, which are month(January to December), day(1-31), hour(0-23) and minutes(0-59). Further, I have tried to understand what type of operating systems were used in those time types.\n\n#checking which months the downloads occured inrespecitive to operating system\nggthemr(\"flat dark\")\nggplot(r_downloads_year_LK,aes(x=month(date),fill=os))+\n    geom_bar()+ xlab(\"Months\") +ylab(\"Frequency\")+\n    scale_y_continuous(breaks =seq(0,140,10))+\n    scale_x_continuous(breaks=1:12) +\n    ggtitle(\"Operating systems used in the months of R download\",\n              subtitle = \"2017 October 20th - 2018 October 20th\")\n\n\n\n\nSo, the first sub part of time is months. Here, we are considering 12 months of an year and months August and April reflects no Operating System is better than Windows property. While, August holding the least amount of downloads with slightly above frequency 20. Highest frequency occurs to October with count higher than 130 and significantly osx and src types of files also have higher amount than any-other month. Except August only the month of December has counts higher than 100.\n\n#checking which days the downloads occured inrespecitive to operating system\nggplot(r_downloads_year_LK,aes(x=day(date),fill=os))+\n    geom_bar()+xlab(\"Days\") +ylab(\"Frequency\")+\n    scale_y_continuous(breaks =seq(0,80,10))+\n    scale_x_continuous(breaks=1:31)+\n    ggtitle(\"Operating systems used in the days of R download\",\n              subtitle = \"2017 October 20th - 2018 October 20th\")\n\n\n\n\nNext, focusing on the days it is clear that 10th and 11th have most downloads respectively reaching more than 60 and 70 in counts, while in other days it is mostly less than 30. Further, clearly on the 31st it includes least frequency of 10 because 31st is not a common day of all 12 months. It would be very tiring to focus on operating systems individually, but to be fair there is clear sign of few days with only the use of windows, and a few days with combination of other operating systems with windows.\n\n#checking which hour the downloads occured inrespecitive to operating system\nggplot(r_downloads_year_LK,aes(x=hour(time),fill=os))+\n    geom_bar()+xlab(\"Hour\") +ylab(\"Frequency\")+\n    scale_y_continuous(breaks = seq(0,100,5))+\n    scale_x_continuous(breaks=0:23)+\n    ggtitle(\"Operating systems used in the Hours of the day of R download\",\n              subtitle = \"2017 October 20th - 2018 October 20th\")\n\n\n\n\nI have been curious at this part because of wanting to know at which hour of the day did our Sri Lankan users download R and packages. Yet it should be noted that the hour of time could be local time of Sri Lanka or otherwise. Still according to the bar chart the hours 4th and 5th have most downloads with counts of above 80 and above 90 respectively. Where in the 21st hour it reaches the least amount of less than 5 counts. Most of the frequencies are in the range of 10 and 35.\n\n#checking which minute the downloads occured  inrespecitive to operating system\nggplot(r_downloads_year_LK,aes(x=minute(time),fill=os))+\n    geom_bar()+xlab(\"Minute\") +ylab(\"Frequency\")+\n    scale_y_continuous(breaks = 0:25)+\n    scale_x_continuous(breaks=0:59)+\n    ggtitle(\"Operating systems used in the minutes of the day of R download\",\n              subtitle = \"2017 October 20th - 2018 October 20th\")+\n    coord_flip()\n\n\n\n\nLooking at the minutes it is very spread out. Focusing on special occasions only four minutes which are 59th, 46th, 12th and 0th have counts more than 20. While 51st minute has a count of 2. Rather than this nothing more significant occurs here. I think considering these counts in perspective of specific operating systems is tedious amount of work and waste of time.\nDownload Size and IP ID\nPackages were downloaded but none of their names were given in this data-set. Therefore we cannot know which package were downloaded. Yet we can identify the package sizes which were downloaded most. According to the table an R package with 82375220 bytes has most downloads of 50, while second place goes to to a size of 82375219 bytes and finally in third place is for 82375216 bytes with 39 counts.\n\n# table of frequency for sizes of download\ntab3&lt;-as.data.frame(sort(table(r_downloads_year_LK$size))%&gt;% tail(5))\nnames(tab3)&lt;-c(\"Size\",\"Frequency\")\nkable(tab3)  %&gt;% \n  kable_styling(bootstrap_options = \"striped\", full_width = F,position=\"left\") %&gt;%\n  add_header_above(c(\"Frequency table to Download size\"=2))\n\n\n\n\n\n\n\n\n\nFrequency table to Download size\n\n\n\nSize\nFrequency\n\n\n\n\n82877772\n18\n\n\n78171328\n22\n\n\n82375216\n39\n\n\n82375219\n48\n\n\n82375220\n50\n\n\n\n\n\nLooking at the IP ID it is clear that 334 has the highest downloads of 55, while second place goes to 1060 with 46 downloads. Finally, ID number 1286 has 16 downloads with third place.\n\n# table of frequency to IP ID\ntab4&lt;-as.data.frame(sort(table(r_downloads_year_LK$ip_id))%&gt;% tail(5))\nnames(tab4)&lt;-c(\"IP ID\",\"Frequency\")\nkable(tab4)  %&gt;% \n  kable_styling(bootstrap_options = \"striped\", full_width = F,position=\"left\") %&gt;%\n  add_header_above(c(\"Frequency table to IP ID\"=2))\n\n\n\n\n\n\n\n\n\nFrequency table to IP ID\n\n\n\nIP ID\nFrequency\n\n\n\n\n623\n9\n\n\n157\n12\n\n\n1286\n16\n\n\n1060\n46\n\n\n334\n55\n\n\n\n\n\nConclusion\nI shall conclude my findings in point form\n\nMost of Sri lankans (93.58%) use windows as a OS for R downloads\nTop three R versions are 3.5.1, 3.4.3 and 3.5.0 with percentages respectively 36.52% 27.96% and 15.98%.\nWindows users use versions 3.5.1, 3.4.3 and 3.5.0 with percentages 34.56%, 26.53% and 3.5.0.\nMost of the downloads occur in the months October and December, while days are 10th and 11th, while hours are 3rd and 4th and minutes of 59th, 46th, 12th and 0th.\nDownload size of 82375220 bytes happens with the highest count of 50, while the IP ID of 334 has most downloads of 55.\nFurther Analysis\n\nWe can do similar analysis for other countries and compare them.\nUsing Size it should be possible to understand what is being downloaded.\n\nPlease see that\nThis is my Second post on the internet so please be kind to tolerate my mistakes in grammar and spellings. I intend to post more statistics related materials in the future and learn accordingly. Thank you for reading.\nTHANK YOU"
  },
  {
    "objectID": "tidytuesday/2018/Week_34/index.html",
    "href": "tidytuesday/2018/Week_34/index.html",
    "title": "Week 34 : Thanksgiving",
    "section": "",
    "text": "# Load the packges\nlibrary(ggplot2)\nlibrary(ggthemr)\nlibrary(stringr)\nlibrary(gridExtra)\nlibrary(tidyverse)\nlibrary(tweenr)\nlibrary(gganimate)\nlibrary(kableExtra)\nlibrary(magrittr)\nlibrary(knitr)\nlibrary(readr)\n\n#load the data\nThanksgiving&lt;-read_csv(\"thanksgiving_meals.csv\")\n\n# apply the theme grape\nggthemr(\"grape\")\n\n#subset the people who said yes for celebrating thanksgiving\nThanksgiving_Yes&lt;-subset(Thanksgiving,celebrate==\"Yes\")\n\n#subset the people who said no for celebrating thanksgiving\nThanksgiving_No&lt;-subset(Thanksgiving,celebrate==\"No\")\nData set was provided on week 34 for TidyTuesday analysis. As it is Thanksgiving week this is understandable. You can receive the data set here. There are more than 65 variables and 1058 observations. The data was acquired buy a survey conducted online and information about them are here.\n{{% tweet \"1065462771817607168\" %}}"
  },
  {
    "objectID": "tidytuesday/2018/Week_34/index.html#age-distribution",
    "href": "tidytuesday/2018/Week_34/index.html#age-distribution",
    "title": "Week 34 : Thanksgiving",
    "section": "Age Distribution",
    "text": "Age Distribution\nFirst the age distribution has only 4 groups, where people who celebrate Thanksgiving in the age category of 18-29 is the very least. Highest count goes to the age category of 45-59 with 269. There are 33 missing observations and they were removed.\nConsidering the people who do not celebrate Thanksgiving the least count of 6 goes to category of 60+, but here the category of 18-29 has the highest count of 31. No missing observations were recorded here.\nBelow is an animated bar plot where the counts change for their respective 4 categories. As 90% of respondents have answered Yes for celebrating Thanksgiving and rest have answered No we can clearly see the count differences\n\nattach(Thanksgiving_Yes)\nattach(Thanksgiving_No)\n# people who do not celebrate \ndont_age&lt;-as.data.frame(summary.factor(Thanksgiving_No$age))\n# people who do celebrate\ndo_age&lt;-as.data.frame(summary.factor(na.omit(Thanksgiving_Yes$age)))\n# people who do celebrate\ndata_do_age&lt;-data.frame(group=c(\"18-29\",\"30-44\",\"45-59\",\"60+\"),\n                      values=do_age$`summary.factor(na.omit(Thanksgiving_Yes$age))`,\n                        frame=rep(\"Do Celebrate\",4))\n# people who do not celebrate\ndata_dont_age&lt;-data.frame(group=c(\"18-29\",\"30-44\",\"45-59\",\"60+\"),\n                          values=dont_age$`summary.factor(Thanksgiving_No$age)`,\n                          frame=rep(\"Do not Celebrate\",4))\n# combining both\ndata_age&lt;-rbind(data_do_age,data_dont_age)\n\n# animated bar plot for people who do celebrate and who do not celebrate \nggplot(data_age,aes(x=factor(group),values))+\n  geom_bar(stat = 'identity',position = \"identity\")+\n  ylab(\"Frequency\")+xlab(\"Age Group\")+\n  ggtitle(\"Animated plot how Do and Do not people prefer \\naccording to Age\")+\n  geom_text(aes(label=values), vjust=1)+\n  transition_states(frame,transition_length = 2,state_length = 3)+\n  enter_fade()+\n  exit_shrink()+\n  ease_aes('cubic-in-out')\n\n\n\n\n\n\ndetach(Thanksgiving_Yes)\ndetach(Thanksgiving_No)"
  },
  {
    "objectID": "tidytuesday/2018/Week_34/index.html#age-with-other-factors",
    "href": "tidytuesday/2018/Week_34/index.html#age-with-other-factors",
    "title": "Week 34 : Thanksgiving",
    "section": "Age with Other Factors",
    "text": "Age with Other Factors\nFirst table is Age vs Gender for people who celebrate Thanksgiving. All age categories have a percentage range in between 19 and 29. Highest percentage of 28.4055 is for Age category 45 - 59. Female have a higher percentage of 54.3823.\nFemale who are 60+ have the highest percentage of 15.2059, while lowest percentage of 8.7645 is for male in the age category of 18-29.\n\nattach(Thanksgiving_Yes)\n\n#kable(addmargins(table(age,gender))) %&gt;% \n#  kable_styling(\"striped\",full_width = F) %&gt;%\n#  column_spec(4,bold = T,color = \"red\") %&gt;%\n#  row_spec(5,bold = T,color = \"red\")\n\n# table of percentages for people who do celebrate\nkable(addmargins(round(prop.table(table(age,gender)),6)*100),\"html\") %&gt;% \n  kable_styling(\"striped\",full_width = F) %&gt;%\n  column_spec(4,bold = T,color = \"red\") %&gt;%\n  row_spec(5,bold = T,color = \"red\") %&gt;%\n  row_spec(0,bold = T,align = 'center')\n\n\n\n\nFemale\nMale\nSum\n\n\n\n18 - 29\n10.7709\n8.7645\n19.5354\n\n\n30 - 44\n13.4108\n11.4044\n24.8152\n\n\n45 - 59\n14.9947\n13.4108\n28.4055\n\n\n60+\n15.2059\n12.0380\n27.2439\n\n\nSum\n54.3823\n45.6177\n100.0000\n\n\n\n\ndetach(Thanksgiving_Yes)\n\nWhen considering the people who do not celebrate Thanksgiving, highest percentage of 62.8205 is for Male, while age category of 18-29 have the highest percentage of 39.7436.\nMale who are in between 18 and 29 have the highest percentage of 25.6410, while Female who are above 60 have the lowest percentage of 2.5641.\n\nattach(Thanksgiving_No)\n\n#kable(addmargins(table(age,gender))) %&gt;% \n#  kable_styling(\"striped\",full_width = F) %&gt;%\n#  column_spec(4,bold = T,color = \"red\") %&gt;%\n#  row_spec(5,bold = T,color = \"red\")\n\n# table of percentages for people who do not celebrate\nkable(addmargins(round(prop.table(table(age,gender)),6)*100),\"html\") %&gt;% \n  kable_styling(\"striped\",full_width = F) %&gt;%\n  column_spec(4,bold = T,color = \"red\") %&gt;%\n  row_spec(5,bold = T,color = \"red\") %&gt;%\n  row_spec(0,bold = T,align = 'center')\n\n\n\n\nFemale\nMale\nSum\n\n\n\n18 - 29\n14.1026\n25.6410\n39.7436\n\n\n30 - 44\n11.5385\n19.2308\n30.7693\n\n\n45 - 59\n8.9744\n12.8205\n21.7949\n\n\n60+\n2.5641\n5.1282\n7.6923\n\n\nSum\n37.1796\n62.8205\n100.0001\n\n\n\n\ndetach(Thanksgiving_No)\n\nWith relative to people who celebrate Thanksgiving in the Family Income category highest percentage goes to USD 25,000 to 49,999.\nPeople who have Family Income USD 25,000 to 49,999 and age above 60 have the highest percentage of 4.96, while lowest percentage of 0.11 is for people who have Family Income in between USD 175,000 to 199,999 of age category of 18-29.\n\nattach(Thanksgiving_Yes)\n\n#kable(addmargins(table(age,family_income)),\"html\") %&gt;% \n#  kable_styling(\"striped\",full_width = F) %&gt;%\n#  column_spec(13,bold = T,color = \"red\") %&gt;%\n#  row_spec(5,bold = T,color = \"red\")\n\n# table of percentages for people who do celebrate\nkable(addmargins(round(prop.table(table(age,family_income)),6)*100),\"html\") %&gt;% \n  kable_styling(\"striped\",full_width = T,font_size = 9) %&gt;%\n  column_spec(13,bold = T,color = \"red\") %&gt;%\n  row_spec(5,bold = T,color = \"red\") %&gt;%\n  row_spec(0,bold = T,angle = 270,align = 'center')\n\n\n\n\n$0 to $9,999\n$10,000 to $24,999\n$100,000 to $124,999\n$125,000 to $149,999\n$150,000 to $174,999\n$175,000 to $199,999\n$200,000 and up\n$25,000 to $49,999\n$50,000 to $74,999\n$75,000 to $99,999\nPrefer not to answer\nSum\n\n\n\n18 - 29\n3.4847\n2.1119\n0.9504\n0.2112\n0.5280\n0.1056\n0.6336\n3.6959\n2.1119\n2.0063\n3.6959\n19.5354\n\n\n30 - 44\n1.3728\n1.5839\n2.5343\n0.8448\n0.6336\n0.3168\n1.2672\n4.8574\n4.0127\n4.2239\n3.1679\n24.8153\n\n\n45 - 59\n0.3168\n1.3728\n4.1183\n2.5343\n2.0063\n1.1616\n3.1679\n4.0127\n3.1679\n3.6959\n2.8511\n28.4056\n\n\n60+\n0.3168\n1.2672\n3.9071\n1.4784\n0.8448\n1.1616\n2.9567\n4.9630\n4.1183\n3.4847\n2.7455\n27.2441\n\n\nSum\n5.4911\n6.3358\n11.5101\n5.0687\n4.0127\n2.7456\n8.0254\n17.5290\n13.4108\n13.4108\n12.4604\n100.0004\n\n\n\n\ndetach(Thanksgiving_Yes)\n\nOf people who do not celebrate Thanksgiving the Family Income category has the highest percentage which goes to People who prefer not to answer.\n15 cells in this table are zero which is the lowest percentage that can occur, while highest percentage goes to people who are in the age category 18 -29 while Family Income is USD 0 to 9,999 and prefer not to answer.\n\nattach(Thanksgiving_No) \n\n#kable(addmargins(table(age,family_income))) %&gt;% \n#  kable_styling(\"striped\",full_width = F) %&gt;%\n#  column_spec(13,bold = T,color = \"red\") %&gt;%\n#  row_spec(5,bold = T,color = \"red\")\n\n# table of percentages for people who do not celebrate\nkable(addmargins(round(prop.table(table(age,family_income)),6)*100),\"html\") %&gt;% \n  kable_styling(\"striped\",full_width = T,font_size = 9) %&gt;%\n  column_spec(13,bold = T,color = \"red\") %&gt;%\n  row_spec(5,bold = T,color = \"red\") %&gt;%\n  row_spec(0,bold = T,angle = 270,align = 'center') \n\n\n\n\n$0 to $9,999\n$10,000 to $24,999\n$100,000 to $124,999\n$125,000 to $149,999\n$150,000 to $174,999\n$175,000 to $199,999\n$200,000 and up\n$25,000 to $49,999\n$50,000 to $74,999\n$75,000 to $99,999\nPrefer not to answer\nSum\n\n\n\n18 - 29\n12.8205\n2.5641\n1.2821\n0.0000\n0.0000\n0.0000\n0.0000\n2.5641\n5.1282\n2.5641\n12.8205\n39.7436\n\n\n30 - 44\n2.5641\n3.8462\n1.2821\n0.0000\n1.2821\n0.0000\n2.5641\n8.9744\n3.8462\n1.2821\n5.1282\n30.7695\n\n\n45 - 59\n2.5641\n2.5641\n0.0000\n1.2821\n1.2821\n1.2821\n2.5641\n5.1282\n0.0000\n3.8462\n1.2821\n21.7951\n\n\n60+\n0.0000\n1.2821\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n1.2821\n1.2821\n0.0000\n3.8462\n7.6925\n\n\nSum\n17.9487\n10.2565\n2.5642\n1.2821\n2.5642\n1.2821\n5.1282\n17.9488\n10.2565\n7.6924\n23.0770\n100.0007\n\n\n\n\ndetach(Thanksgiving_No)\n\nFor the people who celebrate Thanksgiving highest percentage of 21.80 goes to US region of South Atlantic. While lowest percentage goes to Mountain with 4.41.\nPeople who are from South Atlantic in the age categories of 45-59 and 60+ have the highest percentage of 6.55. While the lowest percentage of 0.64 goes to people who are in the age category of 18-29 and from East South Central.\n\nattach(Thanksgiving_Yes)\n\n#kable(addmargins(table(age,us_region))) %&gt;% \n#  kable_styling(\"striped\",full_width = F) %&gt;%\n#  column_spec(11,bold = T,color = \"red\") %&gt;%\n#  row_spec(5,bold = T,color = \"red\")\n\n# table of percentages for people who do celebrate\nkable(addmargins(round(prop.table(table(age,us_region)),4)*100),\"html\")  %&gt;% \n  kable_styling(\"striped\",full_width = T,font_size = 9) %&gt;%\n  column_spec(11,bold = T,color = \"red\") %&gt;%\n  row_spec(5,bold = T,color = \"red\") %&gt;%\n  row_spec(0,bold = T,align = 'center')\n\n\n\n\nEast North Central\nEast South Central\nMiddle Atlantic\nMountain\nNew England\nPacific\nSouth Atlantic\nWest North Central\nWest South Central\nSum\n\n\n\n18 - 29\n2.79\n0.64\n2.26\n0.75\n0.97\n3.11\n3.76\n2.26\n2.47\n19.01\n\n\n30 - 44\n3.44\n1.18\n4.51\n0.97\n1.61\n3.87\n4.94\n1.72\n2.47\n24.71\n\n\n45 - 59\n4.40\n2.15\n4.94\n1.40\n1.72\n3.22\n6.55\n1.72\n2.69\n28.79\n\n\n60+\n4.94\n2.04\n3.87\n1.29\n1.61\n3.76\n6.55\n1.93\n1.50\n27.49\n\n\nSum\n15.57\n6.01\n15.58\n4.41\n5.91\n13.96\n21.80\n7.63\n9.13\n100.00\n\n\n\n\ndetach(Thanksgiving_Yes)\n\nOf people who do not celebrate Thanksgiving 23.5294% are from Pacific, while lowest percentage is for people who are from New England and West North Central with 4.4118.\n10 cells have zero values which is the lowest percentage value. While highest percentage of 11.7647 occurs to people from Pacific and in the age category 30-44.\n\nattach(Thanksgiving_No)\n\n#kable(addmargins(table(age,us_region)))  %&gt;% \n#  kable_styling(\"striped\",full_width = F) %&gt;%\n#  column_spec(11,bold = T,color = \"red\") %&gt;%\n#  row_spec(5,bold = T,color = \"red\")\n\n# table of percentages for people who do not celebrate\nkable(addmargins(round(prop.table(table(age,us_region)),6)*100),\"html\")  %&gt;% \n  kable_styling(\"striped\",full_width = T,font_size = 9) %&gt;%\n  column_spec(11,bold = T,color = \"red\") %&gt;%\n  row_spec(5,bold = T,color = \"red\") %&gt;%\n  column_spec(1,bold = T,width = '2cm') %&gt;%\n  row_spec(0,bold = T,align = 'center')\n\n\n\n\nEast North Central\nEast South Central\nMiddle Atlantic\nMountain\nNew England\nPacific\nSouth Atlantic\nWest North Central\nWest South Central\nSum\n\n\n\n18 - 29\n1.4706\n2.9412\n8.8235\n4.4118\n0.0000\n7.3529\n4.4118\n2.9412\n5.8824\n38.2354\n\n\n30 - 44\n2.9412\n0.0000\n5.8824\n1.4706\n1.4706\n11.7647\n5.8824\n0.0000\n2.9412\n32.3531\n\n\n45 - 59\n2.9412\n2.9412\n4.4118\n2.9412\n2.9412\n1.4706\n4.4118\n0.0000\n0.0000\n22.0590\n\n\n60+\n0.0000\n0.0000\n1.4706\n0.0000\n0.0000\n2.9412\n1.4706\n1.4706\n0.0000\n7.3530\n\n\nSum\n7.3530\n5.8824\n20.5883\n8.8236\n4.4118\n23.5294\n16.1766\n4.4118\n8.8236\n100.0005\n\n\n\n\ndetach(Thanksgiving_No)"
  },
  {
    "objectID": "tidytuesday/2018/Week_34/index.html#gender-distribution",
    "href": "tidytuesday/2018/Week_34/index.html#gender-distribution",
    "title": "Week 34 : Thanksgiving",
    "section": "Gender Distribution",
    "text": "Gender Distribution\nWe have two types of gender categories in this data set which are male and female. According to the people who celebrate Thanksgiving 515 are Female, while only 432 are male. Here also there are 33 missing observations and they have been removed.\nBut this is not the case for those who do not celebrate Thanksgiving. Female have a count of only 29, where males have a count of 49. There were no missing observations.\n\nattach(Thanksgiving_Yes)\nattach(Thanksgiving_No)\n# people who do not celebrate\ndont_sex&lt;-as.data.frame(summary.factor(Thanksgiving_No$gender))\n# people who do celebrate\ndo_sex&lt;-as.data.frame(summary.factor(na.omit(Thanksgiving_Yes$gender)))\n# people who do celebrate\ndata_do_sex&lt;-data.frame(group=c(\"Female\",\"Male\"),\n                  values=do_sex$`summary.factor(na.omit(Thanksgiving_Yes$gender))`,\n                  frame=rep(\"Do Celebrate\",2))\n# people who do not celebrate\ndata_dont_sex&lt;-data.frame(group=c(\"Female\",\"Male\"),\n                          values=dont_sex$`summary.factor(Thanksgiving_No$gender)`,\n                          frame=rep(\"Do not Celebrate\",2))\n# combining both \ndata_sex&lt;-rbind(data_do_sex,data_dont_sex)\n\n# animated plot for people who do celebrate and who do not celebrate\nggplot(data_sex,aes(group,values))+\n  geom_bar(stat = 'identity',position = \"identity\")+\n  ylab(\"Frequency\")+xlab(\"Gender\")+\n  ggtitle(\"Animated plot how Do and Do not people prefer \\naccording to Gender\")+\n  scale_y_continuous(labels= seq(0,520,10),breaks = seq(0,520,10))+\n  geom_text(aes(label=values), vjust=1)+\n  transition_states(frame,transition_length = 2,state_length = 3)+\n  enter_fade()+\n  exit_shrink()+\n  ease_aes('elastic-in-out')\n\n\n\n\n\n\ndetach(Thanksgiving_Yes)\ndetach(Thanksgiving_No)"
  },
  {
    "objectID": "tidytuesday/2018/Week_34/index.html#gender-with-other-factors",
    "href": "tidytuesday/2018/Week_34/index.html#gender-with-other-factors",
    "title": "Week 34 : Thanksgiving",
    "section": "Gender with Other Factors",
    "text": "Gender with Other Factors\nOf people who do celebrate Thanksgiving highest percentage of 10.14 goes to Females where Family Income is USD 25,000 to 49,999. While lowest percentage of 1.27 goes to Males of Family Income category USD 175,000 to 199,999.\n\nattach(Thanksgiving_Yes)\n\n#kable(addmargins(table(gender,family_income)))  %&gt;% \n#  kable_styling(\"striped\",full_width = F) %&gt;%\n#  column_spec(13,bold = T,color = \"red\") %&gt;%\n#  row_spec(3,bold = T,color = \"red\")\n\n# table of percentages for people who do celebrate\nkable(addmargins(round(prop.table(table(gender,family_income)),4)*100),\"html\")  %&gt;% \n  kable_styling(\"striped\",full_width = T,font_size = 8.75) %&gt;%\n  column_spec(13,bold = T,color = \"red\") %&gt;%\n  row_spec(3,bold = T,color = \"red\") %&gt;%\n  column_spec(1,bold = T,width = '2cm') %&gt;%\n  row_spec(0,bold = T,angle = 270,align = 'center')\n\n\n\n\n$0 to $9,999\n$10,000 to $24,999\n$100,000 to $124,999\n$125,000 to $149,999\n$150,000 to $174,999\n$175,000 to $199,999\n$200,000 and up\n$25,000 to $49,999\n$50,000 to $74,999\n$75,000 to $99,999\nPrefer not to answer\nSum\n\n\n\nFemale\n2.64\n3.80\n5.39\n2.11\n2.11\n1.48\n4.44\n10.14\n7.92\n6.97\n7.39\n54.39\n\n\nMale\n2.85\n2.53\n6.12\n2.96\n1.90\n1.27\n3.59\n7.39\n5.49\n6.44\n5.07\n45.61\n\n\nSum\n5.49\n6.33\n11.51\n5.07\n4.01\n2.75\n8.03\n17.53\n13.41\n13.41\n12.46\n100.00\n\n\n\n\ndetach(Thanksgiving_Yes)\n\n3 cells in the below table are zero values, which is the lowest percentage value. Highest percentage of 14.1026 goes to Males who chose not to answer regarding Family Income where they do not celebrate Thanksgiving.\n\nattach(Thanksgiving_No)\n\n#kable(addmargins(table(gender,family_income)))  %&gt;% \n#  kable_styling(\"striped\",full_width = F) %&gt;%\n#  column_spec(13,bold = T,color = \"red\") %&gt;%\n#  row_spec(3,bold = T,color = \"red\")\n\n# table of percentages for people who do not celebrate\nkable(addmargins(round(prop.table(table(gender,family_income)),6)*100),\"html\")  %&gt;% \n  kable_styling(\"striped\",full_width = F,font_size = 10) %&gt;%\n  column_spec(13,bold = T,color = \"red\") %&gt;%\n  row_spec(3,bold = T,color = \"red\") %&gt;%\n  column_spec(1,bold = T,width = '1.5cm') %&gt;%\n  row_spec(0,bold = T,angle = 270,align = 'center')\n\n\n\n\n$0 to $9,999\n$10,000 to $24,999\n$100,000 to $124,999\n$125,000 to $149,999\n$150,000 to $174,999\n$175,000 to $199,999\n$200,000 and up\n$25,000 to $49,999\n$50,000 to $74,999\n$75,000 to $99,999\nPrefer not to answer\nSum\n\n\n\nFemale\n6.4103\n3.8462\n1.2821\n1.2821\n0.0000\n0.0000\n1.2821\n5.1282\n3.8462\n5.1282\n8.9744\n37.1798\n\n\nMale\n11.5385\n6.4103\n1.2821\n0.0000\n2.5641\n1.2821\n3.8462\n12.8205\n6.4103\n2.5641\n14.1026\n62.8208\n\n\nSum\n17.9488\n10.2565\n2.5642\n1.2821\n2.5641\n1.2821\n5.1283\n17.9487\n10.2565\n7.6923\n23.0770\n100.0006\n\n\n\n\ndetach(Thanksgiving_No)\n\nFemale from South Atlantic who celebrate Thanksgiving have a highest percentage of 12.14. Where respondents from Mountain region and Males have the lowest percentage of 1.29.\n\nattach(Thanksgiving_Yes)\n\n#kable(addmargins(table(gender,us_region)))  %&gt;% \n#  kable_styling(\"striped\",full_width = F) %&gt;%\n#  column_spec(11,bold = T,color = \"red\") %&gt;%\n#  row_spec(3,bold = T,color = \"red\")\n\n# table of percentages for people who do celebrate\nkable(addmargins(round(prop.table(table(gender,us_region)),4)*100),\"html\")  %&gt;% \n  kable_styling(\"striped\",full_width = F,font_size = 10) %&gt;%\n  column_spec(11,bold = T,color = \"red\") %&gt;%\n  row_spec(3,bold = T,color = \"red\") %&gt;%\n  column_spec(1,bold = T,width = '1.5cm') %&gt;%\n  row_spec(0,bold = T,align = 'center')\n\n\n\n\nEast North Central\nEast South Central\nMiddle Atlantic\nMountain\nNew England\nPacific\nSouth Atlantic\nWest North Central\nWest South Central\nSum\n\n\n\nFemale\n8.16\n3.33\n8.59\n3.11\n3.33\n7.30\n12.14\n4.19\n4.40\n54.55\n\n\nMale\n7.41\n2.69\n6.98\n1.29\n2.58\n6.66\n9.67\n3.44\n4.73\n45.45\n\n\nSum\n15.57\n6.02\n15.57\n4.40\n5.91\n13.96\n21.81\n7.63\n9.13\n100.00\n\n\n\n\ndetach(Thanksgiving_Yes)\n\nMale respondents who do not celebrate Thanksgiving where they are from Middle Atlantic have a highest percentage of 16.1765. Even though Females of West South Central have the lowest percentage of 1.4706 and Males from West North Central also have the same percentage value.\n\nattach(Thanksgiving_No)\n\n#kable(addmargins(table(gender,us_region)))  %&gt;% \n#  kable_styling(\"striped\",full_width = F) %&gt;%\n#  column_spec(11,bold = T,color = \"red\") %&gt;%\n#  row_spec(3,bold = T,color = \"red\")\n\n# table of percentages for people who do not celebrate\nkable(addmargins(round(prop.table(table(gender,us_region)),6)*100),\"html\")  %&gt;% \n  kable_styling(\"striped\",full_width = F,font_size = 10) %&gt;%\n  column_spec(11,bold = T,color = \"red\") %&gt;%\n  row_spec(3,bold = T,color = \"red\") %&gt;%\n  column_spec(1,bold = T,width = '1.5cm') %&gt;%\n  row_spec(0,bold = T,align = 'center')\n\n\n\n\nEast North Central\nEast South Central\nMiddle Atlantic\nMountain\nNew England\nPacific\nSouth Atlantic\nWest North Central\nWest South Central\nSum\n\n\n\nFemale\n4.4118\n2.9412\n4.4118\n4.4118\n1.4706\n8.8235\n7.3529\n2.9412\n1.4706\n38.2354\n\n\nMale\n2.9412\n2.9412\n16.1765\n4.4118\n2.9412\n14.7059\n8.8235\n1.4706\n7.3529\n61.7648\n\n\nSum\n7.3530\n5.8824\n20.5883\n8.8236\n4.4118\n23.5294\n16.1764\n4.4118\n8.8235\n100.0002\n\n\n\n\ndetach(Thanksgiving_No)"
  },
  {
    "objectID": "tidytuesday/2018/Week_34/index.html#family-income-distribution",
    "href": "tidytuesday/2018/Week_34/index.html#family-income-distribution",
    "title": "Week 34 : Thanksgiving",
    "section": "Family Income Distribution",
    "text": "Family Income Distribution\nThere are 11 categories when it comes to Family Income. The option of Prefer Not to answer is given and has been chosen by people who celebrate and people who do not celebrate Thanksgiving.\nConsidering the people the who celebrate Thanksgiving, highest count of 166 goes to the category of 25,000 to 49,999 USD. While least count goes to 175,000 to 199,999 USD and the count is 26. Further, 118 people have chosen not to answer this question. 33 Missing observations were removed.\nWhere as in people who do not celebrate Thanksgiving, second highest count goes to the categories of 0 to 9,999 USD and 25,000 to 49,999 USD, where the count is 14. Similarly, for the least count of 1 also there are two Family Income categories, which are 125,000 to 149,999 USD and 175,000 to 199,999 USD. Prefer not to answer is the choice of 18 respondents who participated in this survey. No missing observations were recorded.\nAs before here also an animated bar plot is used to explain this.\n\nattach(Thanksgiving_Yes)\nattach(Thanksgiving_No)\n# people who do not celebrate\ndont_FI&lt;-as.data.frame(summary.factor(Thanksgiving_No$family_income))\n# people who do celebrate\ndo_FI&lt;-as.data.frame(summary.factor(na.omit(Thanksgiving_Yes$family_income)))\n# people who do celebrate\ndata_do_FI&lt;-data.frame(group=c(\"0-9,999\",\"10,000-24,999\",\"25,000-49,999\",\n                               \"50,000-74,999\",\"75,000-99,999\",\"100,000-124,999\",\n                               \"125,000-149,999\",\"150,000-174,999\",\n                               \"175,000-199,999\",\"200,000 and up\",\"Not to answer\"),\n                       values=c(52,60,166,127,127,109,48,38,26,76,118),\n                       frame=rep(\"Do Celebrate\",11))\n# people who do not celebrate\ndata_dont_FI&lt;-data.frame(group=c(\"0-9,999\",\"10,000-24,999\",\"25,000-49,999\",\n                                 \"50,000-74,999\",\"75,000-99,999\",\"100,000-124,999\",\n                               \"125,000-149,999\",\"150,000-174,999\",\"175,000-199,999\",\n                               \"200,000 and up\",\"Not to answer\"),\n                         values=c(14,8,14,8,6,2,1,2,1,4,18),\n                         frame=rep(\"Do not Celebrate\",11))\n# combine the dataset\ndata_FI&lt;-rbind(data_do_FI,data_dont_FI)\n\n# animated plot for people who do celebrate and who do not celebrate\nggplot(data_FI,aes(group,values))+\n  geom_bar(stat = 'identity',position = \"identity\")+\n  ylab(\"Frequency\")+xlab(\"Family Income in dollars\")+\n  ggtitle(\"Animated plot how Do and Do not people prefer \\naccording to Family Income\")+\n  scale_y_continuous(labels= seq(0,170,10),breaks = seq(0,170,10))+\n  geom_text(aes(label=values), vjust=1)+coord_flip()+\n  transition_states(frame,transition_length = 2,state_length = 3)+\n  enter_fade()+\n  exit_shrink()+\n  ease_aes('bounce-in')\n\n\n\n\n\n\ndetach(Thanksgiving_Yes)\ndetach(Thanksgiving_No)"
  },
  {
    "objectID": "tidytuesday/2018/Week_34/index.html#family-income-with-other-factors",
    "href": "tidytuesday/2018/Week_34/index.html#family-income-with-other-factors",
    "title": "Week 34 : Thanksgiving",
    "section": "Family Income with Other Factors",
    "text": "Family Income with Other Factors\nOf people who do celebrate Thanksgiving the lowest percentage of zero is for people from West North Central with Family Income USD 175,000 to 199,999, people from West South Central with Family Income USD 175,000 to 199,999, people from Mountain with Family Income USD 150,000 to 174,999 and people from Mountain with Family Income USD 175,000 to 199,999. Highest percentage of 4.9409 goes to people from South Atlantic with Family Income USD 25,000 to 49,999.\n\nattach(Thanksgiving_Yes)\n\n#kable(addmargins(table(family_income,us_region)))  %&gt;% \n#  kable_styling(\"striped\",full_width = F) %&gt;%\n#  column_spec(11,bold = T,color = \"red\") %&gt;%\n#  row_spec(12,bold = T,color = \"red\")\n\n# table of percentages for people who do celebrate\nkable(addmargins(round(prop.table(table(family_income,us_region)),6)*100),\"html\")%&gt;% \n  kable_styling(\"striped\",full_width = F,font_size = 10) %&gt;%\n  column_spec(11,bold = T,color = \"red\") %&gt;%\n  row_spec(12,bold = T,color = \"red\") %&gt;%\n  column_spec(1,bold = T,width = '2.5cm') %&gt;%\n  row_spec(0,bold = T,align = 'center')\n\n\n\n\nEast North Central\nEast South Central\nMiddle Atlantic\nMountain\nNew England\nPacific\nSouth Atlantic\nWest North Central\nWest South Central\nSum\n\n\n\n$0 to $9,999\n0.6445\n0.2148\n0.6445\n0.1074\n0.2148\n1.0741\n0.8593\n0.2148\n0.9667\n4.9409\n\n\n$10,000 to $24,999\n0.8593\n0.6445\n0.5371\n0.1074\n0.3222\n1.5038\n1.3963\n0.5371\n0.3222\n6.2299\n\n\n$100,000 to $124,999\n2.7927\n0.8593\n1.3963\n0.4296\n0.5371\n1.6112\n2.2556\n0.3222\n1.5038\n11.7078\n\n\n$125,000 to $149,999\n0.5371\n0.1074\n0.6445\n0.3222\n0.3222\n0.6445\n1.8260\n0.2148\n0.5371\n5.1558\n\n\n$150,000 to $174,999\n0.2148\n0.2148\n0.5371\n0.0000\n0.2148\n0.9667\n0.8593\n0.3222\n0.7519\n4.0816\n\n\n$175,000 to $199,999\n0.6445\n0.2148\n1.0741\n0.0000\n0.2148\n0.3222\n0.3222\n0.0000\n0.0000\n2.7926\n\n\n$200,000 and up\n0.7519\n0.4296\n2.0408\n0.6445\n0.7519\n1.0741\n1.1815\n0.5371\n0.6445\n8.0559\n\n\n$25,000 to $49,999\n2.2556\n1.1815\n2.5779\n0.6445\n1.0741\n2.1482\n4.9409\n1.5038\n1.0741\n17.4006\n\n\n$50,000 to $74,999\n2.5779\n0.9667\n2.0408\n0.5371\n0.5371\n1.8260\n2.7927\n1.3963\n0.8593\n13.5339\n\n\n$75,000 to $99,999\n2.7927\n0.8593\n1.9334\n0.8593\n0.5371\n1.2889\n2.4705\n1.2889\n1.5038\n13.5339\n\n\nPrefer not to answer\n1.5038\n0.3222\n2.1482\n0.7519\n1.1815\n1.5038\n2.9001\n1.2889\n0.9667\n12.5671\n\n\nSum\n15.5748\n6.0149\n15.5747\n4.4039\n5.9076\n13.9635\n21.8044\n7.6261\n9.1301\n100.0000\n\n\n\n\ndetach(Thanksgiving_Yes)\n\nThere are a lot of cell values which have zero therefore I am not going to state them. Further, Highest percentage value of 5.8824 is from people of Middle Atlantic and Family Income categories of USD 0 to 9,999 and USD 25,000 to 49,999.\n\nattach(Thanksgiving_No)\n\n#kable(addmargins(table(family_income,us_region)))  %&gt;% \n#  kable_styling(\"striped\",full_width = F) %&gt;%\n#  column_spec(11,bold = T,color = \"red\") %&gt;%\n#  row_spec(12,bold = T,color = \"red\")\n\n# table of percentages for people who do not celebrate\nkable(addmargins(round(prop.table(table(family_income,us_region)),6)*100),\"html\")%&gt;% \n  kable_styling(\"striped\",full_width = F,font_size = 10) %&gt;%\n  column_spec(11,bold = T,color = \"red\") %&gt;%\n  row_spec(12,bold = T,color = \"red\")%&gt;%\n  column_spec(1,bold = T,width = '2.5cm') %&gt;%\n  row_spec(0,bold = T,align = 'center')\n\n\n\n\nEast North Central\nEast South Central\nMiddle Atlantic\nMountain\nNew England\nPacific\nSouth Atlantic\nWest North Central\nWest South Central\nSum\n\n\n\n$0 to $9,999\n0.0000\n0.0000\n5.8824\n4.4118\n0.0000\n2.9412\n2.9412\n0.0000\n1.4706\n17.6472\n\n\n$10,000 to $24,999\n0.0000\n1.4706\n0.0000\n1.4706\n0.0000\n1.4706\n1.4706\n1.4706\n1.4706\n8.8236\n\n\n$100,000 to $124,999\n0.0000\n0.0000\n1.4706\n0.0000\n0.0000\n1.4706\n0.0000\n0.0000\n0.0000\n2.9412\n\n\n$125,000 to $149,999\n1.4706\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n1.4706\n\n\n$150,000 to $174,999\n0.0000\n0.0000\n2.9412\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n2.9412\n\n\n$175,000 to $199,999\n0.0000\n1.4706\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n1.4706\n\n\n$200,000 and up\n1.4706\n0.0000\n0.0000\n0.0000\n1.4706\n0.0000\n2.9412\n0.0000\n0.0000\n5.8824\n\n\n$25,000 to $49,999\n2.9412\n0.0000\n5.8824\n1.4706\n1.4706\n4.4118\n2.9412\n0.0000\n0.0000\n19.1178\n\n\n$50,000 to $74,999\n1.4706\n1.4706\n1.4706\n0.0000\n0.0000\n4.4118\n0.0000\n1.4706\n1.4706\n11.7648\n\n\n$75,000 to $99,999\n0.0000\n0.0000\n0.0000\n1.4706\n1.4706\n4.4118\n1.4706\n0.0000\n0.0000\n8.8236\n\n\nPrefer not to answer\n0.0000\n1.4706\n2.9412\n0.0000\n0.0000\n4.4118\n4.4118\n1.4706\n4.4118\n19.1178\n\n\nSum\n7.3530\n5.8824\n20.5884\n8.8236\n4.4118\n23.5296\n16.1766\n4.4118\n8.8236\n100.0008\n\n\n\n\ndetach(Thanksgiving_No)"
  },
  {
    "objectID": "tidytuesday/2018/Week_34/index.html#us-region-distribution",
    "href": "tidytuesday/2018/Week_34/index.html#us-region-distribution",
    "title": "Week 34 : Thanksgiving",
    "section": "US Region Distribution",
    "text": "US Region Distribution\nThere are 9 regions in both sides, and also both sides have missing values. People who do celebrate Thanksgiving have 49 missing values, while only 10 are missing values for people who do not celebrate Thanksgiving.\n\nattach(Thanksgiving_Yes)\nattach(Thanksgiving_No)\n# people who do not celebrate\ndont_USR&lt;-as.data.frame(summary.factor(na.omit(Thanksgiving_No$us_region)))\n# people who do celebrate\ndo_USR&lt;-as.data.frame(summary.factor(na.omit(Thanksgiving_Yes$us_region)))\n# people who do celebrate\ndata_do_USR&lt;-data.frame(group=c(\"East North Central\", \"East South Central\",\n                                \"West South Central\", \"West North Central\",\n                                \"Middle Atlantic\",\"South Atlantic\", \"Mountain\", \n                                \"New England\", \"Pacific\"),\n                       values=c(145,56,85,71,145,203,41,55,130),\n                       frame=rep(\"Do Celebrate\",9))\n# people who do not celebrate\ndata_dont_USR&lt;-data.frame(group=c(\"East North Central\", \"East South Central\",\n                                  \"West South Central\", \"West North Central\",\n                                \"Middle Atlantic\",\"South Atlantic\", \"Mountain\", \n                                \"New England\", \"Pacific\"),\n                         values=c(5,4,6,3,14,11,6,3,16),\n                         frame=rep(\"Do not Celebrate\",9))\n# combine both datasets\ndata_USR&lt;-rbind(data_do_USR,data_dont_USR)\n\n# animated plot for people who do celebrate and who do not celebrate\nggplot(data_USR,aes(x=str_wrap(group,7),values))+\n  geom_bar(stat = 'identity',position = \"identity\")+\n  ylab(\"Frequency\")+xlab(\"US Regions\")+\n  ggtitle(\"Animated plot how Do and Do not people prefer \\naccording to US Regions\")+\n  scale_y_continuous(labels= seq(0,210,10),breaks = seq(0,210,10))+\n  geom_text(aes(label=values), vjust=1)+\n  transition_states(frame,transition_length = 2,state_length = 3)+\n  enter_fade()+\n  exit_shrink()+\n  ease_aes('cubic-in-out')\n\n\n\n\n\n\ndetach(Thanksgiving_Yes)\ndetach(Thanksgiving_No)"
  },
  {
    "objectID": "tidytuesday/2018/Week_36/index.html",
    "href": "tidytuesday/2018/Week_36/index.html",
    "title": "Week 36: Medium Posts",
    "section": "",
    "text": "#loading packages\n#load data\nlibrary(readr)\n#manipulate data\nlibrary(dplyr)\nlibrary(magrittr)\n# format table with expense\nlibrary(formattable)\n# knitting the document\nlibrary(knitr)\n# another type of table\nlibrary(kableExtra)\n# playing with strings\nlibrary(stringr)\n# combining two plots\nlibrary(grid)\nlibrary(gridExtra)\n# that theme you wanted\nlibrary(ggthemr)\n# text analysis\nlibrary(tm)\nlibrary(SnowballC)\nlibrary(RColorBrewer)\nlibrary(wordcloud)\n\n# adding theme called fresh for plots\nggthemr(\"fresh\")\n#loading the data\nmedium &lt;- read_csv(\"medium_datasci.csv\")\nattach(medium)\n\nFocusing on Claps with Authors and publications, where does writing alot of posts will get you with popularity and claps. The code will focus on Top 10 Authors with most of the posts and Claps they have received. Further, does having an image in the post matter ?. Finally, word clouds for Top 10 authors and Top 5 publications with their titles.\n\n\n{{% tweet \"1071369742672429056\" %}}\n\n\nGitHub Code\nClaps\nTable indicates that 25,729 posts have 0 claps, while 7,093 posts with only one clap, and finally 3044 posts with 2 claps. The only odd one is posts with 50 claps where the count is 970.\n\n# extracting the top 15 with most claps\nclaps_table&lt;-table(claps) %&gt;%\n                     sort() %&gt;%\n                     tail(15)\n\n# table it up \nkable(t(claps_table),\"html\") %&gt;%\n  kable_styling(bootstrap_options = \"striped\",full_width = T) %&gt;%\n  row_spec(0,bold = T,font_size = 13,color = \"grey\")\n\n\n\n13\n12\n9\n8\n11\n7\n50\n10\n6\n5\n4\n3\n2\n1\n0\n\n\n602\n634\n661\n721\n759\n864\n970\n989\n1124\n1389\n1402\n2150\n3044\n7093\n25729\n\n\n\n\nTop 10 Authors and Claps for their posts\nThere are only two posts which do not have Images in their content. The highest number of claps is 60,000, a post written by Sophia Ciocca under the title “How Does Spotify Know You So Well?”. Second Place is for the article “Blockchain is not only crappy technology but a bad vision for the future” which was written by Kai Stinchombe with 53,000 claps. De Xun is the only author who has two articles which are in this list on the places 8 and 9 with claps respectively 37,000 and 36,000.\n\n# seperate medium with author, titles, claps and image\nclaps_A_I&lt;-medium[,c(\"author\",\"title\",\"subtitle\",\"claps\",\"image\")] %&gt;%\n              arrange(claps) %&gt;%\n              tail(10)\nnames(claps_A_I)&lt;-c(\"Author\",\"Title\",\"Subtitle\",\"Claps\",\"Image\")\n\n# table it\nformattable(claps_A_I[,-3],align=c(\"l\",\"l\",\"r\",\"c\"),\n            list(\n              Claps=color_tile(\"lightblue\",\"blue\"),\n              Image=color_tile(\"red\",\"green\")\n            ))\n\n\n\n\n\nAuthor\n\n\nTitle\n\n\nClaps\n\n\nImage\n\n\n\n\n\nVishal Maini\n\n\nA Beginners Guide to AI/ML\n\n\n36000\n\n\n1\n\n\n\n\nDe Xun\n\n\nSWIPE Bi-Weekly Update, 16th-27th July\n\n\n36000\n\n\n1\n\n\n\n\nDe Xun\n\n\n[PARTNERSHIP] SWIPE-Bluzelle: Building SWIPEs decentralized database\n\n\n37000\n\n\n1\n\n\n\n\nAndrej Karpathy\n\n\nSoftware 2.0\n\n\n38000\n\n\n0\n\n\n\n\nRadu Raicea\n\n\nWant to know how Deep Learning works? Heres a quick guide for everyone.\n\n\n39000\n\n\n1\n\n\n\n\nAnything App\n\n\nFast-forward twenty years with Anything App.\n\n\n42000\n\n\n0\n\n\n\n\nMichael Jordan\n\n\nArtificial IntelligenceThe Revolution Hasnt Happened Yet\n\n\n46000\n\n\n1\n\n\n\n\nXiaohan Zeng\n\n\nI interviewed at five top companies in Silicon Valley in five days, and luckily got five job offers\n\n\n49000\n\n\n1\n\n\n\n\nKai Stinchcombe\n\n\nBlockchain is not only crappy technology but a bad vision for the future\n\n\n53000\n\n\n1\n\n\n\n\nSophia Ciocca\n\n\nHow Does Spotify Know You So Well?\n\n\n60000\n\n\n1\n\n\n\n\n\n\nTop 10 Authors with most posts\nYves Mulkers has most amount of posts with 487 but only 3,779 claps. This is not the case for Corsairs publishing where for 156 posts the number of claps are 111,501. Even looking at other authors names we can see that writing alot of posts does not create claps.\n\n# finding who are the top 10 authors with claps\n# summary.factor(medium$author) %&gt;%\n#  sort() %&gt;%\n#  tail(11)\n\n# extracting posts only from the top 10 authors with most posts\nTop10_author&lt;-subset(medium,\n       author ==\"C Gavilanes\" | author == \"Jae Duk Seo\" |\n       author == \"Corsair's Publishing\" | author==\"Alibaba Cloud\" |\n       author == \"Ilexa Yardley\" | author == \"Peter Marshall\" |\n       author == \"AI Hawk\" | author == \"DEEP AERO DRONES\" |\n       author == \"Synced\" | author == \"Yves Mulkers\")\n\n# plotting the top 10 authors\ng1_Top10_a&lt;-ggplot(Top10_author,aes(x=author))+\n            coord_flip()+ geom_bar()+\n            xlab(\"Author\")+ylab(\"Frequency\")+\n            ggtitle(\"Top 10 Authors and number of posts they have written\")+\n            geom_text(stat = 'count',aes(label=..count..),hjust=0.5)\n\n# plotting the top 10 authors and their claps\ng2_Top10_a&lt;-Top10_author[,c(\"author\",\"claps\")] %&gt;%\n            group_by(author) %&gt;%\n            summarise_each(funs(sum)) %&gt;%\n            ggplot(.,aes(x=author,y=claps))+ geom_bar(stat = \"identity\")+\n            ggtitle(\"Total number of Claps they got\")+\n            xlab(\"Author\")+ylab(\"Claps\")+\n            coord_flip()+geom_text(aes(label=claps),hjust =0.5 )\n\n# plotting two plots at same grid\ngrid.arrange(g1_Top10_a,g2_Top10_a,ncol=1)\n\n\n\n\nTop 10 Authors who have posts with Images\nExtracting the top 10 authors with posts which have images it is clear most of the posts do have Images and they do generate claps. This is true for Corsairs’s Publishing. With 154 posts it generates 109,906 claps. There are authors who have written more posts than totally received claps . It should be noted that two Authors did not add any Images for their post and they are Peter Marshall and C Gavilanes.\n\n# plotting top 10 authors with Image\nI1_g1_Top10_a&lt;-subset(Top10_author[,c(\"author\",\"image\")],image==1) %&gt;%\n            ggplot(.,aes(x=author))+ geom_bar()+coord_flip()+\n            xlab(\"Author\")+ylab(\"Frequency\")+  \n            ggtitle(\"Top 10 Authors and posts which has Images\")+\n            geom_text(stat='count',aes(label=..count..),hjust =0.5 )\n\n# plotting the claps for top 10 authors with Image \nI1_g2_Top10_a&lt;-subset(Top10_author[,c(\"author\",\"claps\",\"image\")],image==1) %&gt;%\n               group_by(author,image) %&gt;%\n               summarise_each(funs(sum)) %&gt;%\n               ggplot(.,aes(x=author,y=claps))+ geom_bar(stat = \"identity\")+\n               coord_flip()+\n               xlab(\"Author\")+ylab(\"Claps\")+\n               ggtitle( \"Total number of Claps they got\")+\n               geom_text(aes(label=claps),hjust =0.5 )    \n\n# top plots one grid\ngrid.arrange(I1_g1_Top10_a,I1_g2_Top10_a,ncol=1)\n\n\n\n\nTop 10 Authors who have posts without Images\nPosts without images have very low amount of total claps. To be specific 14 posts by Jae Duk Seo have 1833 claps but 2 posts by Corsair’s publishing has 1595 claps. That is very Impressive. Further there are even posts which have claps less than 10 where the number of posts is less than 5.\n\n# plotting top 10 authors with No Image\nI0_g1_Top10_a&lt;-subset(Top10_author[,c(\"author\",\"image\")],image==0) %&gt;%\n               ggplot(.,aes(x=author))+ geom_bar()+coord_flip()+\n               xlab(\"Author\")+ylab(\"Frequency\")+  \n               ggtitle(\"Top 10 Authors and posts without Images\")+\n               geom_text(stat='count',aes(label=..count..),hjust =0.5 )\n\n# plotting the claps for top 10 authors with No Image \nI0_g2_Top10_a&lt;-subset(Top10_author[,c(\"author\",\"claps\",\"image\")],image==0) %&gt;%\n               group_by(author,image) %&gt;%\n               summarise_each(funs(sum)) %&gt;%\n               ggplot(.,aes(x=author,y=claps))+ geom_bar(stat = \"identity\")+\n               coord_flip()+\n               xlab(\"Author\")+ylab(\"Claps\")+               \n               ggtitle(\"Total number of claps they got\")+\n               geom_text(aes(label=claps),hjust =0.5 )\n\n# top plots one grid\ngrid.arrange(I0_g1_Top10_a,I0_g2_Top10_a,ncol=1)\n\n\n\n\nTop 10 Authors and Reading time for their posts\nReading in minutes, does it has anything to do with number of posts?. Looking at the plot it is clear that posts from Synced has more total reading time than Yves Mulkers with highest number posts. The difference between posts is close 150, while difference between reading times is above 150 for Yves Mulkers and Synced.\n\n# plotting top 10 authors with reading times\nRT_g1_Top10_a&lt;-Top10_author[,c(\"author\",\"reading_time\")] %&gt;%\n            group_by(author) %&gt;%\n            summarise_each(funs(sum)) %&gt;%\n            ggplot(.,aes(x=author,y=reading_time))+ geom_bar(stat = \"identity\")+\n            ggtitle(\"Reading Time\")+\n            xlab(\"Author\")+ylab(\"Reading Time in minutes\")+  \n            coord_flip()+geom_text(aes(label=reading_time),hjust =0.5 )\n\n# printing the above plot with number of posts \ngrid.arrange(g1_Top10_a,RT_g1_Top10_a,ncol=1)\n\n\n\n\nTop 5 Publications with most posts\nPublications with most number of posts has the highest number of claps and order achieved\nin the “Top 5 publications and number of posts they have written” plot is maintained in the “Total number of Claps they got” plot as well. This simply refers, when you write alot of posts under a publication you will receive alot of claps because of the foundation these specific publications holds in Medium.\n\n# finding who are the top 5 publications with claps\n# summary.factor(medium$publication) %&gt;%\n#  sort() %&gt;%\n#  tail(11)\n\n# extracting posts only from the top 5 publications with most posts\nTop5_pub&lt;-subset(medium,\n       publication ==\"Towards Data Science\" | \n       publication == \"Hacker Noon\" |\n       publication == \"Becoming Human: Artificial Intelligence Magazine\" |\n       publication ==\"Chatbots Life\" |\n       publication == \"Data Driven Investor\" )\n\n# plotting the top 5 publications\ng1_Top5_p&lt;-ggplot(Top5_pub,aes(x=str_wrap(publication,15)))+\n            coord_flip()+ geom_bar()+\n            xlab(\"Publication\")+ylab(\"Frequency\")+  \n            ggtitle(\"Top 5 Publications and number of posts they have written\")+\n            geom_text(stat = 'count',aes(label=..count..),hjust=0.5)\n\n# plotting the top 5 publications and their claps\ng2_Top5_p&lt;-Top5_pub[,c(\"publication\",\"claps\")] %&gt;%\n            group_by(publication) %&gt;%\n            summarise_each(funs(sum)) %&gt;%\n            ggplot(.,aes(x=str_wrap(publication,15),y=claps))+\n            geom_bar(stat=\"identity\")+\n            xlab(\"Publication\")+ylab(\"Claps\")+  \n            ggtitle(\"Total number of Claps they got\")+\n            coord_flip()+geom_text(aes(label=claps),hjust =0.5 )\n\n# plotting two plots at same grid\ngrid.arrange(g1_Top5_p,g2_Top5_p,ncol=1)\n\n\n\n\nWord Cloud for the Titles from Top 10 Authors\nWord cloud from the titles of the posts by Top 10 authors of most number of posts is below. The words thing, read, data, drone and new are with highest mentions. Where words such as big, telecom and tech are next in the line with higher amount of posts. In restrictions I have considered that this word cloud will have 1500 words and only if a word atleast holds the frequency of 10.\nWell, I could clearly see that there cannot be 1500 words here.\n\n#convert into data frame\nTop10_author&lt;-data.frame(Top10_author)\n\n# Calculate Corpus\nTop10_author.Corpus&lt;-Corpus(VectorSource(Top10_author$title))\n\n# clean the data\nTop10_author.Clean&lt;-tm_map(Top10_author.Corpus,PlainTextDocument)\nTop10_author.Clean&lt;-tm_map(Top10_author.Corpus,tolower)\nTop10_author.Clean&lt;-tm_map(Top10_author.Clean,removeNumbers)\nTop10_author.Clean&lt;-tm_map(Top10_author.Clean,removeWords,stopwords(\"english\"))\nTop10_author.Clean&lt;-tm_map(Top10_author.Clean,removePunctuation)\nTop10_author.Clean&lt;-tm_map(Top10_author.Clean,stripWhitespace)\nTop10_author.Clean&lt;-tm_map(Top10_author.Clean,stemDocument)\n\n# save as png\n#png(filename = \"wordcloud1.png\",width = 1024,height = 768)\n# plot the word cloud\nwordcloud(Top10_author.Clean,max.words = 1500,min.freq = 10,\n          colors = brewer.pal(11,\"Spectral\"),random.color = FALSE,\n          random.order = TRUE)\n\n\n\n\nWord Cloud for the Titles from Top 5 publications\nThis word cloud also has similar restrictions for number of words and minimum frequency for a word. Words such as data, learn, use, machin, network, deep, scienc and artifici have most amount of frequency. Further, words such as neural, intellig, chatbot, part and python are also with significant amount of frequency. Here we can see clearly see there can be more than 1000 words.\n\n#convert into data frame\nTop5_pub&lt;-data.frame(Top5_pub)\n\n# Calculate Corpus\nTop5_pub.Corpus&lt;-Corpus(VectorSource(Top5_pub$title))\n\n#clean the data\nTop5_pub.Clean&lt;-tm_map(Top5_pub.Corpus,PlainTextDocument)\nTop5_pub.Clean&lt;-tm_map(Top5_pub.Corpus,tolower)\nTop5_pub.Clean&lt;-tm_map(Top5_pub.Clean,removeNumbers)\nTop5_pub.Clean&lt;-tm_map(Top5_pub.Clean,removeWords,stopwords(\"english\"))\nTop5_pub.Clean&lt;-tm_map(Top5_pub.Clean,removePunctuation)\nTop5_pub.Clean&lt;-tm_map(Top5_pub.Clean,stripWhitespace)\nTop5_pub.Clean&lt;-tm_map(Top5_pub.Clean,stemDocument)\n\n# save as png\n#png(filename = \"wordcloud2.png\",width = 1024,height = 768)\n# plot the word cloud\nwordcloud(Top5_pub.Clean,max.words = 1500,min.freq = 10,\n          colors = brewer.pal(11,\"Spectral\"),random.color = FALSE,\n          random.order = TRUE)\n\n\n\n\nConclusion\nMy conclusion of the above plots and tables in point form\n\nUsing dplyr to manipulate the data-set was useful when there is complication.\ngrid and gridExtra packages provide a safe way of combining multiple plots into one plot.\nformattable and kableExtra were crucial in generating tables which are informative.\nWord cloud or analyzing text is very useful and flexible when we use the above packages.\nFurther Analysis\n\nSimilarly we can do the above analysis for Top 5 publications and other variables.\nWord clouds for subtitles also will be interesting to see, specially focusing on authors and publications.\n\nPlease see that\nThis is my sixth post on the internet so my mistakes in grammar and spellings should be very less than previous posts. I intend to post more statistics related materials in the future and learn accordingly. Thank you for reading.\nTHANK YOU"
  },
  {
    "objectID": "tidytuesday/2018/Week_38/index.html",
    "href": "tidytuesday/2018/Week_38/index.html",
    "title": "Week 38: Sea Creatures",
    "section": "",
    "text": "Over the weeks I have only done blog posts for TidyTuesday. Today for week 38, I am going to present my blog post in a presentation. This presentation does not include plots, but the code only. Therefore, I am putting the plots here.\n{{% tweet \"1074965237231697921\" %}}"
  },
  {
    "objectID": "tidytuesday/2018/Week_38/index.html#ios-slide-presentation",
    "href": "tidytuesday/2018/Week_38/index.html#ios-slide-presentation",
    "title": "Week 38: Sea Creatures",
    "section": "IOS slide Presentation",
    "text": "IOS slide Presentation\n\nRmarkdown for the same presentation\n\nBelow is the content from the presentation, but I have included the plots.\n\n#load the packages\nlibrary(readr)\nlibrary(lubridate)\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(ggthemr)\nlibrary(stringr)"
  },
  {
    "objectID": "tidytuesday/2018/Week_38/index.html#introduction",
    "href": "tidytuesday/2018/Week_38/index.html#introduction",
    "title": "Week 38: Sea Creatures",
    "section": "Introduction",
    "text": "Introduction\n\n2194 observations.\n21 variables.\nData : Cetacean Data\n\nRead : The Pudding Article\n\nAbout : Big Fish in the Sea."
  },
  {
    "objectID": "tidytuesday/2018/Week_38/index.html#packages-used",
    "href": "tidytuesday/2018/Week_38/index.html#packages-used",
    "title": "Week 38: Sea Creatures",
    "section": "Packages Used",
    "text": "Packages Used\n\nreadr\nlubridate\ntidyverse\nmagrittr\nggthemr\nstringr\n\n\n#load the data\nSeaCreature &lt;- read_csv(\"allCetaceanData.csv\", \n                         col_types = cols(X1 = col_skip()))\nattach(SeaCreature)\n\n# loading theme\nggthemr(\"flat dark\")"
  },
  {
    "objectID": "tidytuesday/2018/Week_38/index.html#species-vs-sex-vs-birthyear-code",
    "href": "tidytuesday/2018/Week_38/index.html#species-vs-sex-vs-birthyear-code",
    "title": "Week 38: Sea Creatures",
    "section": "Species vs Sex vs BirthYear (code)",
    "text": "Species vs Sex vs BirthYear (code)\n\nPlot1&lt;-ggplot(SeaCreature,aes(x=species,y=birthYear,color=sex))+\n       geom_jitter()+\n       coord_flip()+\n       theme(axis.text.x =element_text(angle = 90, hjust = 1))+\n       ggtitle(\"Species and Sex over their BirthYear\")+\n       ylab(\"Birth Year\")+\n       xlab(\"Species\")+ \n       legend_bottom()  \n\nPlot1\n\n\n\n\n\n\n#ggsave(\"Plot_1.png\",width = 12,height = 12)"
  },
  {
    "objectID": "tidytuesday/2018/Week_38/index.html#species-vs-sex-vs-birthyear-plot",
    "href": "tidytuesday/2018/Week_38/index.html#species-vs-sex-vs-birthyear-plot",
    "title": "Week 38: Sea Creatures",
    "section": "Species vs Sex vs BirthYear (plot)",
    "text": "Species vs Sex vs BirthYear (plot)\n\nPlot 1\nAlot of Bottle-nose type species from early years.\nMore missing values for Birth Year.\nSecond most goes to Killer Whale Orca.\nThird place is in with Beluga type Species.\nHere and there few of them without knowledge of Gender."
  },
  {
    "objectID": "tidytuesday/2018/Week_38/index.html#status-vs-sex-vs-birthyear-code",
    "href": "tidytuesday/2018/Week_38/index.html#status-vs-sex-vs-birthyear-code",
    "title": "Week 38: Sea Creatures",
    "section": "Status vs Sex vs BirthYear (code)",
    "text": "Status vs Sex vs BirthYear (code)\n\nPlot2&lt;-ggplot(SeaCreature,aes(x=str_wrap(status,8),\n                              y=birthYear,color=sex))+\n       geom_jitter()+\n       coord_flip()+\n       theme(axis.text.x =element_text(angle = 90, hjust = 1))+\n       ggtitle(\"Status and Sex over their BirthYear\")+\n       ylab(\"Birth Year\")+\n       xlab(\"Status\")+ \n       legend_bottom()  \nPlot2\n\n\n\n\n\n\n#ggsave(\"Plot_2.png\",width = 12,height = 12)"
  },
  {
    "objectID": "tidytuesday/2018/Week_38/index.html#status-vs-sex-vs-birthyear-plot",
    "href": "tidytuesday/2018/Week_38/index.html#status-vs-sex-vs-birthyear-plot",
    "title": "Week 38: Sea Creatures",
    "section": "Status vs Sex vs BirthYear (plot)",
    "text": "Status vs Sex vs BirthYear (plot)\n\nPlot 2\nDead Sea Creatures from the beginning of time itself.\nMostly dead, but from 1960 alot of them are alive.\nBirth Year unknown for most of the Dead and few of the Released.\nQuite a few with status unknown.\nOnly one escaped and it is a male in 1981."
  },
  {
    "objectID": "tidytuesday/2018/Week_38/index.html#species-vs-sex-vs-status-code",
    "href": "tidytuesday/2018/Week_38/index.html#species-vs-sex-vs-status-code",
    "title": "Week 38: Sea Creatures",
    "section": "Species vs Sex vs Status (code)",
    "text": "Species vs Sex vs Status (code)\n\nPlot3&lt;-ggplot(SeaCreature,aes(x=str_wrap(status,8),\n                              y=str_wrap(species,12),color=sex))+\n       geom_jitter()+\n       coord_flip()+\n       theme(axis.text.x =element_text(angle = 90, hjust = 1))+\n       ggtitle(\"Species and Sex over their status\")+\n       ylab(\"Species\")+\n       xlab(\"Status\")+ \n       legend_bottom()  \nPlot3\n\n\n\n\n\n\n#ggsave(\"Plot_3.png\",width = 14,height = 12)"
  },
  {
    "objectID": "tidytuesday/2018/Week_38/index.html#species-vs-sex-vs-status-plot",
    "href": "tidytuesday/2018/Week_38/index.html#species-vs-sex-vs-status-plot",
    "title": "Week 38: Sea Creatures",
    "section": "Species vs Sex vs Status (plot)",
    "text": "Species vs Sex vs Status (plot)\n\nPlot 3\nOne male Bottle-nose species escaped.\nMore Killer whale orca’s and White-sided Pacific Species are dead than alive\nAround 15 Species have dead creatures and non alive.\nOne male Bottle-nose species Escaped but found dead.\nThere are 4 miscarriaged Bottle-nose species and three are female."
  },
  {
    "objectID": "tidytuesday/2018/Week_38/index.html#birth-year-and-sex-of-the-acquisitioned-code",
    "href": "tidytuesday/2018/Week_38/index.html#birth-year-and-sex-of-the-acquisitioned-code",
    "title": "Week 38: Sea Creatures",
    "section": "Birth Year and Sex of the Acquisitioned (code)",
    "text": "Birth Year and Sex of the Acquisitioned (code)\n\nPlot4&lt;-ggplot(SeaCreature,aes(x=acquisition,\n                              y=birthYear,color=sex))+\n       geom_jitter()+\n       theme(axis.text.x =element_text(angle = 90, hjust = 1))+\n       ggtitle(\"Acquisitioned ones with their and BirthYear\")+\n       ylab(\"Birth Year\")+\n       xlab(\"Acquisition\")+ \n       legend_bottom()  \nPlot4\n\n\n\n\n\n\n#ggsave(\"Plot_4.png\",width = 12,height = 12)"
  },
  {
    "objectID": "tidytuesday/2018/Week_38/index.html#birth-year-and-sex-of-the-acquisitioned-plot",
    "href": "tidytuesday/2018/Week_38/index.html#birth-year-and-sex-of-the-acquisitioned-plot",
    "title": "Week 38: Sea Creatures",
    "section": "Birth Year and Sex of the Acquisitioned (plot)",
    "text": "Birth Year and Sex of the Acquisitioned (plot)\n\nPlot 4\nWith early Birth Year to until 1990 the creatures were captured.\nFrom Birth Year 1971 to 2017 only the creatures are born.\nAfter 1965 around 30 creatures have been rescued.\nClose to 40 creatures with unknown status with Birth Year known.\nMost of the rescued ones are of Male gender."
  },
  {
    "objectID": "tidytuesday/2018/Week_38/index.html#species-and-their-sex-over-current-location-code",
    "href": "tidytuesday/2018/Week_38/index.html#species-and-their-sex-over-current-location-code",
    "title": "Week 38: Sea Creatures",
    "section": "Species and their sex over current location (code)",
    "text": "Species and their sex over current location (code)\n\nPlot5&lt;-ggplot(SeaCreature,aes(x=str_wrap(species,12),\n                              y=currently,color=sex))+\n       geom_jitter()+\n       theme(axis.text.x =element_text(angle = 90, hjust = 1))+\n       ggtitle(\"Species and Sex over their Current Location\")+\n       ylab(\"Current Location\")+\n       xlab(\"Species\")+ \n       legend_bottom()  \nPlot5\n\n\n\n\n\n\n#ggsave(\"Plot_5.png\",width = 14,height = 14)"
  },
  {
    "objectID": "tidytuesday/2018/Week_38/index.html#species-and-their-sex-over-current-location-plot",
    "href": "tidytuesday/2018/Week_38/index.html#species-and-their-sex-over-current-location-plot",
    "title": "Week 38: Sea Creatures",
    "section": "Species and their sex over current location (plot)",
    "text": "Species and their sex over current location (plot)\n\nPlot 5\nClose to 50 current locations.\nThere are few locations with only one type of species.\nBottle-nose creatures in most of these locations.\nSea Life park in Hawaii has a diverse amount of Species.\nSea world in San Diego is second when it comes to diversity."
  },
  {
    "objectID": "tidytuesday/2018/Week_38/index.html#acquisitioned-ones-and-thier-sex-with-status-code",
    "href": "tidytuesday/2018/Week_38/index.html#acquisitioned-ones-and-thier-sex-with-status-code",
    "title": "Week 38: Sea Creatures",
    "section": "Acquisitioned ones and thier Sex with Status (code)",
    "text": "Acquisitioned ones and thier Sex with Status (code)\n\nPlot6&lt;-ggplot(SeaCreature,aes(x=status,y=acquisition,color=sex))+\n       geom_jitter()+\n       ggtitle(\"Acquisitioned with Sex and Status\")+\n       xlab(\"Status\")+\n       ylab(\"Acquisition\")+ \n       legend_bottom()  \nPlot6\n\n\n\n\n\n\n#ggsave(\"Plot_6.png\",width = 12,height = 12)"
  },
  {
    "objectID": "tidytuesday/2018/Week_38/index.html#acquisitioned-ones-and-thier-sex-with-status-plot",
    "href": "tidytuesday/2018/Week_38/index.html#acquisitioned-ones-and-thier-sex-with-status-plot",
    "title": "Week 38: Sea Creatures",
    "section": "Acquisitioned ones and thier Sex with Status (plot)",
    "text": "Acquisitioned ones and thier Sex with Status (plot)\n\nPlot 6\nMost of the Captured creatures are Dead, but few of them Released.\nMost of the Rescued creatures are Dead, few alive and some Released.\nIn Unknown acquisition-ed type alot of them are Dead.\nOne rescued creature with unknown status.\n6 creatures which were born have been released and 50% are male."
  },
  {
    "objectID": "tidytuesday/2018/Week_38/index.html#conclusion",
    "href": "tidytuesday/2018/Week_38/index.html#conclusion",
    "title": "Week 38: Sea Creatures",
    "section": "Conclusion",
    "text": "Conclusion\n\nIos slides are NICE.\nJitter plots useful for categorical data.\nPlots are too complex when using Location, Currently and Birth Year, but manageable.\nBottle-nose species is holding a special place in this data-set.\nAlot of unknown data points when it comes to Birth Year.\n\nTHANK YOU"
  },
  {
    "objectID": "tidytuesday/2019/Week_10/index.html",
    "href": "tidytuesday/2019/Week_10/index.html",
    "title": "Week 10: Women in Workforce",
    "section": "",
    "text": "GitHub Code\n{{% tweet \"1103226092352139264\" %}}\n#load the packages\nlibrary(tidyverse)\nlibrary(gganimate)\nlibrary(readr)\nlibrary(ggthemr)\n\n# load the theme\nggthemr(\"fresh\")\n\n# load the data\nearnings_female &lt;- read_csv(\"earnings_female.csv\")\nemployed_gender &lt;- read_csv(\"employed_gender.csv\")\njobs_gender &lt;- read_csv(\"jobs_gender.csv\")"
  },
  {
    "objectID": "tidytuesday/2019/Week_10/index.html#all-other-age-groups",
    "href": "tidytuesday/2019/Week_10/index.html#all-other-age-groups",
    "title": "Week 10: Women in Workforce",
    "section": "All other age groups",
    "text": "All other age groups\n\naverage&lt;-earnings_female %&gt;%\n         filter(group==\"Total, 16 years and older\") %&gt;%\n         mutate(year=cut(Year,breaks = c(1978,1989,1999,2011),\n                  labels =c(\"1980s\",\"1990s\",\"2000s\") ) ) %&gt;%\n        group_by(year) %&gt;%\n        summarize(Mean=mean(percent))\n\nearnings_female %&gt;%\n  filter(group!=\"Total, 16 years and older\") %&gt;%\nggplot(.,aes(Year,percent,color=group))+\n  geom_point()+geom_line()+\n  theme(legend.position = \"bottom\")+\n  geom_hline(yintercept = average$Mean,color=c(\"red\",\"maroon\",\"brown\"),size=1.2)+\n  annotate(\"text\",x=2010,y=68,label=\"1980s Average\")+\n  annotate(\"text\",x=1983,y=76,label=\"1990s Average\")+\n  annotate(\"text\",x=2008,y=80.5,label=\"2000s Average\")"
  },
  {
    "objectID": "tidytuesday/2019/Week_10/index.html#comparing-full-time-with-part-time",
    "href": "tidytuesday/2019/Week_10/index.html#comparing-full-time-with-part-time",
    "title": "Week 10: Women in Workforce",
    "section": "Comparing Full Time with Part Time",
    "text": "Comparing Full Time with Part Time\n\nemployed_gender %&gt;%\n  select(year,total_full_time,total_part_time) %&gt;%\n  gather(Type,percent,c(total_full_time,total_part_time)) %&gt;%\nggplot(.,aes(year,percent,fill=Type,label=round(percent)))+\n  geom_col()+theme(legend.position = \"bottom\")+\n  geom_text(nudge_y = -.75,color=\"white\")+xlab(\"Year\")+\n  ylab(\"Percentage\")+\n  ggtitle(\"Total Work Force Full Time and Part Time\")"
  },
  {
    "objectID": "tidytuesday/2019/Week_10/index.html#male-occupants-with-full-time-and-part-time-work",
    "href": "tidytuesday/2019/Week_10/index.html#male-occupants-with-full-time-and-part-time-work",
    "title": "Week 10: Women in Workforce",
    "section": "Male Occupants with Full Time and Part Time Work",
    "text": "Male Occupants with Full Time and Part Time Work\n\nemployed_gender %&gt;%\n  select(year,full_time_male,part_time_male) %&gt;%\n  gather(Type,percent,c(full_time_male,part_time_male)) %&gt;%\nggplot(.,aes(year,percent,fill=Type,label=round(percent)))+\n  geom_col()+theme(legend.position = \"bottom\")+\n  geom_text(nudge_y = -.75,color=\"white\")+xlab(\"Year\")+\n  ylab(\"Percentage\")+\n  ggtitle(\"Male Work Force Full Time and Part Time\")"
  },
  {
    "objectID": "tidytuesday/2019/Week_10/index.html#female-occupants-with-full-time-and-part-time-work",
    "href": "tidytuesday/2019/Week_10/index.html#female-occupants-with-full-time-and-part-time-work",
    "title": "Week 10: Women in Workforce",
    "section": "Female Occupants with Full Time and Part Time Work",
    "text": "Female Occupants with Full Time and Part Time Work\n\nemployed_gender %&gt;%\n  select(year,full_time_female,part_time_female) %&gt;%\n  gather(Type,percent,c(full_time_female,part_time_female)) %&gt;%\nggplot(.,aes(year,percent,fill=Type,label=round(percent)))+\n  geom_col()+theme(legend.position = \"bottom\")+\n  geom_text(nudge_y = -.75,color=\"white\")+xlab(\"Year\")+\n  ylab(\"Percentage\")+\n  ggtitle(\"Female Work Force Full Time and Part Time\")"
  },
  {
    "objectID": "tidytuesday/2019/Week_10/index.html#major-category",
    "href": "tidytuesday/2019/Week_10/index.html#major-category",
    "title": "Week 10: Women in Workforce",
    "section": "Major Category",
    "text": "Major Category\nMajor Category and Total Workers\n\njg_avg&lt;-jobs_gender %&gt;%\n        select(-one_of(c(\"occupation\",\"minor_category\"))) %&gt;%\n        group_by(year,major_category) %&gt;%\n        group_by(year) %&gt;%\n        summarize_if(is.numeric,funs(mean),na.rm=TRUE)\n\np&lt;-jobs_gender %&gt;%\n     select(-one_of(c(\"occupation\",\"minor_category\"))) %&gt;%\n     group_by(year,major_category) %&gt;%\n     summarise_all(funs(mean),na.rm=TRUE) %&gt;%\nggplot(.,aes(str_wrap(major_category,12),total_workers,label=round(total_workers,2)))+\n  geom_col()+transition_time(year)+ease_aes(\"linear\")+  \n  ggtitle(\"Total Workers changing Over time\",subtitle = \"Year :{frame_time}\")+\n  xlab(\"Major Category\")+ylab(\"Total Workers\")+geom_text(vjust=-1)\n\nanimate(p,nframes=4,fps=1)\n\n\n\n\nMajor Category and Male Workers\n\np&lt;-jobs_gender %&gt;%\n     select(-one_of(c(\"occupation\",\"minor_category\"))) %&gt;%\n     group_by(year,major_category) %&gt;%\n     summarise_all(funs(mean),na.rm=TRUE) %&gt;%\nggplot(.,aes(str_wrap(major_category,12),workers_male,label=round(workers_male,2)))+\n  geom_col()+transition_time(year)+ease_aes(\"linear\")+  \n  ggtitle(\"Male Workers changing Over time\",subtitle = \"Year :{frame_time}\")+\n  xlab(\"Major Category\")+ylab(\"Male Workers\")+geom_text(vjust=-1)\n\nanimate(p,nframes=4,fps=1)\n\n\n\n\nMajor Category and Female Workers\n\np&lt;-jobs_gender %&gt;%\n     select(-one_of(c(\"occupation\",\"minor_category\"))) %&gt;%\n     group_by(year,major_category) %&gt;%\n     summarise_all(funs(mean),na.rm=TRUE) %&gt;%\nggplot(.,aes(str_wrap(major_category,12),workers_female,label=round(workers_female,2)))+\n  geom_col()+transition_time(year)+ease_aes(\"linear\")+  \n  ggtitle(\"Female Workers changing Over time\",subtitle = \"Year :{frame_time}\")+\n  xlab(\"Major Category\")+ylab(\"Female Workers\")+geom_text(vjust=-1)\n\nanimate(p,nframes=4,fps=1)\n\n\n\n\nMajor Category and Total Earnings Male Wage\n\np&lt;-jobs_gender %&gt;%\n     select(-one_of(c(\"occupation\",\"minor_category\"))) %&gt;%\n     group_by(year,major_category) %&gt;%\n     summarise_all(funs(mean),na.rm=TRUE) %&gt;%\nggplot(.,aes(str_wrap(major_category,12),total_earnings_male,label=round(total_earnings_male,2)))+\n  geom_col()+transition_time(year)+ease_aes(\"linear\") +\n  ggtitle(\"Total Earnings Male Wage changing Over time\",subtitle = \"Year :{frame_time}\")+\n  xlab(\"Major Category\")+ylab(\"Total Earnings Male\")+geom_text(vjust=-1)\n\nanimate(p,nframes=4,fps=1)\n\n\n\n\nMajor Category and Total Earnings Female Wage\n\np&lt;-jobs_gender %&gt;%\n     select(-one_of(c(\"occupation\",\"minor_category\"))) %&gt;%\n     group_by(year,major_category) %&gt;%\n     summarise_all(funs(mean),na.rm=TRUE) %&gt;%\nggplot(.,aes(str_wrap(major_category,12),total_earnings_female,label=round(total_earnings_female)))+\n  geom_col()+transition_time(year)+ease_aes(\"linear\")+\n  ggtitle(\"Total Earnings Female Wage changing Over time\",subtitle = \"Year :{frame_time}\")+\n  xlab(\"Major Category\")+ylab(\"Total Earnings Female\")+geom_text(vjust=-1)\n\nanimate(p,nframes=4,fps=1)\n\n\n\n\nMajor Category and Wage Percent for Female relative to Male\n\np&lt;-jobs_gender %&gt;%\n     select(-one_of(c(\"occupation\",\"minor_category\"))) %&gt;%\n     group_by(year,major_category) %&gt;%\n     summarise_all(funs(mean),na.rm=TRUE) %&gt;%\nggplot(.,aes(str_wrap(major_category,12),wage_percent_of_male,\n             label=round(wage_percent_of_male,2)))+\n  geom_col()+transition_time(year)+ease_aes(\"linear\")+\n  ggtitle(\"Wage Percent of Female relative to Male changing Over time\",\n          subtitle = \"Year :{frame_time}\")+\n  xlab(\"Major Category\")+ylab(\"Relative Percentage\")+geom_text(vjust=-1)\n\nanimate(p,nframes=4,fps=1)"
  },
  {
    "objectID": "tidytuesday/2019/Week_10/index.html#minor-category",
    "href": "tidytuesday/2019/Week_10/index.html#minor-category",
    "title": "Week 10: Women in Workforce",
    "section": "Minor Category",
    "text": "Minor Category\nMinor Category and Total Workers\n\np&lt;-jobs_gender %&gt;%\n     select(-one_of(c(\"occupation\",\"major_category\"))) %&gt;%\n     group_by(year,minor_category) %&gt;%\n     summarise_all(funs(mean),na.rm=TRUE) %&gt;%\nggplot(.,aes(str_wrap(minor_category,20),total_workers,label=round(total_workers,2)))+\n  geom_col()+transition_time(year)+ease_aes(\"linear\")+coord_flip()+\n  ggtitle(\"Total Workers changing Over time\",subtitle = \"Year :{frame_time}\")+\n  xlab(\"Minor Category\")+ylab(\"Total Workers\")+geom_text(hjust=\"inward\")\n\nanimate(p,nframes=4,fps=1)\n\n\n\n\nMinor Category and Male Workers\n\np&lt;-jobs_gender %&gt;%\n     select(-one_of(c(\"occupation\",\"major_category\"))) %&gt;%\n     group_by(year,minor_category) %&gt;%\n     summarise_all(funs(mean),na.rm=TRUE) %&gt;%\nggplot(.,aes(str_wrap(minor_category,20),workers_male,label=round(workers_male,2)))+\n  geom_col()+transition_time(year)+ease_aes(\"linear\")+coord_flip()+\n  ggtitle(\"Male Workers changing Over time\",subtitle = \"Year :{frame_time}\")+\n  xlab(\"Minor Category\")+ylab(\"Male Workers\")+geom_text(hjust=\"inward\")\n\nanimate(p,nframes=4,fps=1)\n\n\n\n\nMinor Category and Female Workers\n\np&lt;-jobs_gender %&gt;%\n     select(-one_of(c(\"occupation\",\"major_category\"))) %&gt;%\n     group_by(year,minor_category) %&gt;%\n     summarise_all(funs(mean),na.rm=TRUE) %&gt;%\nggplot(.,aes(str_wrap(minor_category,20),workers_female,label=round(workers_female,2)))+\n  geom_col()+transition_time(year)+ease_aes(\"linear\")+coord_flip()+\n  ggtitle(\"Female Workers changing Over time\",subtitle = \"Year :{frame_time}\")+\n  xlab(\"Minor Category\")+ylab(\"Female Workers\")+geom_text(hjust=\"inward\")\n\nanimate(p,nframes=4,fps=1)\n\n\n\n\nMinor Category and Total Earnings Male Wage\n\np&lt;-jobs_gender %&gt;%\n     select(-one_of(c(\"occupation\",\"major_category\"))) %&gt;%\n     group_by(year,minor_category) %&gt;%\n     summarise_all(funs(mean),na.rm=TRUE) %&gt;%\nggplot(.,aes(str_wrap(minor_category,20),total_earnings_male,label=round(total_earnings_male,2)))+\n  geom_col()+transition_time(year)+ease_aes(\"linear\")+coord_flip()+\n  ggtitle(\"Total Earnings Male Wage changing Over time\",subtitle = \"Year :{frame_time}\")+\n  xlab(\"Minor Category\")+ylab(\"Total earnings Male\")+geom_text(hjust=\"inward\")\n\nanimate(p,nframes=4,fps=1)\n\n\n\n\nMinor Category and Total Earnings Female Wage\n\np&lt;-jobs_gender %&gt;%\n     select(-one_of(c(\"occupation\",\"major_category\"))) %&gt;%\n     group_by(year,minor_category) %&gt;%\n     summarise_all(funs(mean),na.rm=TRUE) %&gt;%\nggplot(.,aes(str_wrap(minor_category,20),total_earnings_female,label=round(total_earnings_female,2)))+\n  geom_col()+transition_time(year)+ease_aes(\"linear\")+coord_flip()+\n  ggtitle(\"Total Earnings Female Wage changing Over time\",subtitle = \"Year :{frame_time}\")+\n  xlab(\"Minor Category\")+ylab(\"Total earnings Female\")+geom_text(hjust=\"inward\")\n\nanimate(p,nframes=4,fps=1)\n\n\n\n\nMinor Category and Wage Percent for Female relative to Male\n\np&lt;-jobs_gender %&gt;%\n     select(-one_of(c(\"occupation\",\"major_category\"))) %&gt;%\n     group_by(year,minor_category) %&gt;%\n     summarise_all(funs(mean),na.rm=TRUE) %&gt;%\nggplot(.,aes(str_wrap(minor_category,18),wage_percent_of_male,\n             label=round(wage_percent_of_male,2)))+\n  geom_col()+transition_time(year)+ease_aes(\"linear\")+\n  ggtitle(\"Wage Percent of Female relative to Male changing Over time\",\n          subtitle = \"Year :{frame_time}\")+coord_flip()+\n  xlab(\"Minor Category\")+ylab(\"Relative Percentage\")+geom_text(hjust=1)\n\nanimate(p,nframes=4,fps=1)\n\n\n\n\nTHANK YOU"
  },
  {
    "objectID": "tidytuesday/2019/Week_13/index.html",
    "href": "tidytuesday/2019/Week_13/index.html",
    "title": "Week 13 : Pets In Seattle",
    "section": "",
    "text": "library(readr)\nlibrary(tidyverse)\nlibrary(gganimate)\nlibrary(splitstackshape)\nlibrary(forcats)\nlibrary(ggthemr)\n\nggthemr(\"flat dark\")\nseattle_pets &lt;- read_csv(\"seattle_pets.csv\")\n{{% tweet \"1111090496372391937\" %}}\nGitHub Code"
  },
  {
    "objectID": "tidytuesday/2019/Week_13/index.html#zipcode-counts-over-the-years",
    "href": "tidytuesday/2019/Week_13/index.html#zipcode-counts-over-the-years",
    "title": "Week 13 : Pets In Seattle",
    "section": "Zipcode Counts Over the Years",
    "text": "Zipcode Counts Over the Years\n\np&lt;-seattle_pets %&gt;%\n   cSplit(\"license_issue_date\",sep = \" \") %&gt;%\n   rename(Month =license_issue_date_1) %&gt;%\n   rename(Day = license_issue_date_2) %&gt;%\n   rename(Year = license_issue_date_3) %&gt;%\n   select(zip_code,Year) %&gt;%\n   group_by(Year) %&gt;%\n   count(zip_code) %&gt;%\n   remove_missing() %&gt;%\n   subset(Year &gt;=2010) %&gt;%\n   top_n(25) %&gt;%\nggplot(.,aes(x= fct_infreq(zip_code),y=n,fill=factor(Year)))+\n  geom_col()+transition_time(Year)+ease_aes(\"linear\") +\n  coord_flip()+xlab(\"Zip code\")+ylab(\" Count\")+\n  labs(fill=\"Year\")+\n  scale_y_continuous(breaks=seq(0,3000,250),labels = seq(0,3000,250))+\n  ggtitle(\"Zip Code Over the Years\",subtitle=\"Year : {frame_time}\")\n\nanimate(p,fps=1,nframes=7)"
  },
  {
    "objectID": "tidytuesday/2019/Week_13/index.html#species-counts-over-the-years",
    "href": "tidytuesday/2019/Week_13/index.html#species-counts-over-the-years",
    "title": "Week 13 : Pets In Seattle",
    "section": "Species Counts Over the Years",
    "text": "Species Counts Over the Years\n\nseattle_pets %&gt;%\n   cSplit(\"license_issue_date\",sep = \" \") %&gt;%\n   rename(Month =license_issue_date_1) %&gt;%\n   rename(Day = license_issue_date_2) %&gt;%\n   rename(Year = license_issue_date_3) %&gt;%\n   select(species,Year) %&gt;%\n   subset(Year &gt;=2016) %&gt;%\n   group_by(Year) %&gt;%\n   count(species) %&gt;%\nggplot(.,aes(x= species,y=n,fill=factor(Year),label=n))+\n  geom_col()+geom_text()+\n  transition_states(Year,transition_length = 2,state_length = 2)+\n  enter_fade()+exit_shrink()+ease_aes(\"back-in\")+\n  xlab(\"Species\")+ylab(\"Count\")+ labs(fill=\"Year\")+\n  scale_y_continuous(breaks=seq(0,23000,1000),labels=seq(0,23000,1000))+\n  ggtitle(\"Species Over the Years\",subtitle = \"Year: {closest_state}\")"
  },
  {
    "objectID": "tidytuesday/2019/Week_13/index.html#primary-breed-over-the-years",
    "href": "tidytuesday/2019/Week_13/index.html#primary-breed-over-the-years",
    "title": "Week 13 : Pets In Seattle",
    "section": "Primary Breed Over the Years",
    "text": "Primary Breed Over the Years\n\np&lt;-seattle_pets %&gt;%\n   cSplit(\"license_issue_date\",sep = \" \") %&gt;%\n   rename(Month =license_issue_date_1) %&gt;%\n   rename(Day = license_issue_date_2) %&gt;%\n   rename(Year = license_issue_date_3) %&gt;%\n   select(primary_breed,Year) %&gt;%\n   group_by(Year) %&gt;%\n   count(primary_breed) %&gt;%\n   remove_missing() %&gt;%\n   subset(Year &gt;=2010) %&gt;%\n   top_n(15) %&gt;%\nggplot(.,aes(x= str_wrap(primary_breed,20),y=n,label=n,fill=factor(Year)))+\n  geom_col()+transition_time(Year)+ease_aes(\"linear\") +\n  coord_flip()+geom_text()+labs(fill=\"Year\")+\n  xlab(\"Primary Breed\")+ylab(\"Count\")+\n  scale_y_continuous(breaks=seq(0,6000,500),labels=seq(0,6000,500))+\n  ggtitle(\"Primary Breed Over the Years\",subtitle = \"Year : {frame_time}\")\n\nanimate(p,fps=1,nframes=8)"
  },
  {
    "objectID": "tidytuesday/2019/Week_13/index.html#animals-name-over-the-years",
    "href": "tidytuesday/2019/Week_13/index.html#animals-name-over-the-years",
    "title": "Week 13 : Pets In Seattle",
    "section": "Animals Name Over the Years",
    "text": "Animals Name Over the Years\n\np&lt;-seattle_pets %&gt;%\n   cSplit(\"license_issue_date\",sep = \" \") %&gt;%\n   rename(Month =license_issue_date_1) %&gt;%\n   rename(Day = license_issue_date_2) %&gt;%\n   rename(Year = license_issue_date_3) %&gt;%\n   select(animals_name,Year) %&gt;%\n   group_by(Year) %&gt;%\n   count(animals_name) %&gt;%\n   remove_missing() %&gt;%\n   subset(Year &gt;=2010) %&gt;%\n   top_n(5) %&gt;%\nggplot(.,aes(x= fct_infreq(animals_name),y=n,fill=factor(Year),label=n))+\n  geom_col()+transition_time(Year)+ease_aes(\"linear\") +\n  coord_flip()+geom_text()+labs(fill=\"Year\")+\n  xlab(\"Animals Name\")+ylab(\"Count\")+\n  scale_y_continuous(breaks = seq(0,275,25),labels=seq(0,275,25))+\n  ggtitle(\"Animales N ame Over the Years\",\n          subtitle = \"Year : {frame_time}\")\n\nanimate(p,fps=1,nframes=8)\n\n\n\n\nTHANK YOU"
  },
  {
    "objectID": "tidytuesday/2019/Week_15/index.html",
    "href": "tidytuesday/2019/Week_15/index.html",
    "title": "Week 15 : Tennis Tournaments",
    "section": "",
    "text": "library(readr)\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(gganimate)\n\nplayer_dob &lt;- read_csv(\"player_dob.csv\", \n                        col_types = cols(date_of_birth = col_date(format = \"%Y-%m-%d\"), \n                        date_of_first_title = col_date(format = \"%Y-%m-%d\")))\ngrand_slams &lt;- read_csv(\"grand_slams.csv\", \n                        col_types = cols(gender = col_factor(levels = c(\"Female\",\"Male\")), \n                        rolling_win_count = col_integer(), \n                        tournament_date = col_date(format = \"%Y-%m-%d\"), \n                        year = col_integer()))\ngrand_slam_timeline &lt;- read_csv(\"grand_slam_timeline.csv\", \n                                col_types = cols(gender = col_factor(levels = c(\"Female\",\"Male\")), \n                                year = col_integer()))\n{{% tweet \"1115652786862153729\" %}}\nGitHub Code"
  },
  {
    "objectID": "tidytuesday/2019/Week_15/index.html#decade-of-birth-vs-first-grand-slam-title-won",
    "href": "tidytuesday/2019/Week_15/index.html#decade-of-birth-vs-first-grand-slam-title-won",
    "title": "Week 15 : Tennis Tournaments",
    "section": "Decade of Birth vs First Grand Slam Title Won",
    "text": "Decade of Birth vs First Grand Slam Title Won\n\nplayer_dob %&gt;% \n   remove_missing() %&gt;%\n   mutate(grand_slam=recode_factor(grand_slam,\n                                  'Wimbledon'=\"Wimbledon\",\n                                  'US Open'=\"US Open\",\n                                  'French Open'=\"French Open\",\n                                  'Australian Open'=\"Aus Open\",\n                                  'Australian Open (January)'=\"Aus Open\",\n                                  'Australian Open (Jan)'=\"Aus Open\",       \n                                  'Australian Open (December)'=\"Aus Open\",  \n                                  'Australian Open (Jan.)'=\"Aus Open\"                                  \n                                  )) %&gt;%\n  mutate(Birth=year(date_of_birth)) %&gt;%\n  mutate(Birth=cut(Birth,breaks = c(1929,1939,1949,1959,1969,1979,1989,1999),\n                   labels = c(1930,1940,1950,1960,1970,1980,1990)\n                   )) %&gt;%\n  group_by(Birth,grand_slam) %&gt;%\n  count()  %&gt;%\nggplot(.,aes(x=factor(Birth),y=n,fill=grand_slam))+\n      geom_col(position = \"dodge\")+\n      scale_y_continuous(breaks=seq(1,11,1),labels=seq(1,11,1))+\n      labs(fill=\"Grand Slam\")+\n      geom_text(aes(label=n),position = position_dodge(width = 1),vjust=1)+\n      xlab(\"Decade of Birth\")+ylab(\"Count\")+\n      ggtitle(\"How Decade of Birth and First Win of Grand Slam changes\")"
  },
  {
    "objectID": "tidytuesday/2019/Week_15/index.html#decade-of-birth-vs-first-grand-slam-with-age",
    "href": "tidytuesday/2019/Week_15/index.html#decade-of-birth-vs-first-grand-slam-with-age",
    "title": "Week 15 : Tennis Tournaments",
    "section": "Decade of Birth vs First Grand Slam with Age",
    "text": "Decade of Birth vs First Grand Slam with Age\n\nplayer_dob %&gt;% \n  remove_missing() %&gt;%\n  mutate(grand_slam=recode_factor(grand_slam,\n                                  'Wimbledon'=\"Wimbledon\",\n                                  'US Open'=\"US Open\",\n                                  'French Open'=\"French Open\",\n                                  'Australian Open'=\"Aus Open\",\n                                  'Australian Open (January)'=\"Aus Open\",\n                                  'Australian Open (Jan)'=\"Aus Open\",       \n                                  'Australian Open (December)'=\"Aus Open\",  \n                                  'Australian Open (Jan.)'=\"Aus Open\"                                  \n                                  )) %&gt;%\n  mutate(Birth=year(date_of_birth)) %&gt;%\n  mutate(Birth=cut(Birth,breaks = c(1929,1939,1949,1959,1969,1979,1989,1999),\n                   labels = c(1930,1940,1950,1960,1970,1980,1990)\n                   )) %&gt;%\nggplot(.,aes(x=grand_slam,size=round(age/365),y=Birth))+\n       geom_jitter()+\n       xlab(\"Grand Slam\")+ylab(\"Decade of Birth\")+\n       labs(color=\"Age\",size=\"Age in Years\")+\n       ggtitle(\"Birth Decade vs First Grand Slam with Age\")"
  },
  {
    "objectID": "tidytuesday/2019/Week_15/index.html#gender-vs-grand-slam-with-name",
    "href": "tidytuesday/2019/Week_15/index.html#gender-vs-grand-slam-with-name",
    "title": "Week 15 : Tennis Tournaments",
    "section": "Gender vs Grand Slam with Name",
    "text": "Gender vs Grand Slam with Name\n\ngrand_slams %&gt;%\n      group_by(name,gender) %&gt;%\n      count(sort = TRUE) %&gt;%\n      head(25) %&gt;%\n      ggplot(.,aes(x=fct_inorder(name),y=n,\n                   fill=gender,label=n))+\n      geom_col()+xlab(\"Name\")+ylab(\"Count\")+\n      labs(fill=\"Gender\")+\n      coord_flip()+geom_text(hjust =1)+\n      ggtitle(\"Who won most with Gender\")"
  },
  {
    "objectID": "tidytuesday/2019/Week_15/index.html#gender-vs-grand-slam-with-year",
    "href": "tidytuesday/2019/Week_15/index.html#gender-vs-grand-slam-with-year",
    "title": "Week 15 : Tennis Tournaments",
    "section": "Gender vs Grand Slam with Year",
    "text": "Gender vs Grand Slam with Year\n\np&lt;-grand_slams %&gt;%\n      ggplot(.,aes(x=name,y=rolling_win_count,\n                   shape=gender,color=grand_slam))+\n      geom_point()+\n      xlab(\"Name\")+ylab(\"Cumulative Count\")+\n      labs(color=\"Grand Slam\",shape=\"Gender\")+\n      transition_time(tournament_date)+ease_aes(\"linear\")+\n      coord_flip()+shadow_mark()+\n      ggtitle(\"Cumulative progress with Year: {year(frame_time)}\")\n\nanimate(p,nframes = 52,fps=1)"
  },
  {
    "objectID": "tidytuesday/2019/Week_15/index.html#top-10-players-and-their-outcomes",
    "href": "tidytuesday/2019/Week_15/index.html#top-10-players-and-their-outcomes",
    "title": "Week 15 : Tennis Tournaments",
    "section": "Top 10 players and their Outcomes",
    "text": "Top 10 players and their Outcomes\n\ntop10&lt;-grand_slams %&gt;%\n       group_by(name) %&gt;%\n       count(sort = TRUE) %&gt;%\n       head(10) %&gt;%\n       select(name)\n\ngrand_slam_timeline %&gt;%\n      rename(name=\"player\") %&gt;%\n      inner_join(top10,\"name\") %&gt;%\n      group_by(name,outcome) %&gt;%\n      count() %&gt;%\nggplot(.,aes(name,n,fill=outcome))+\n      geom_col(position=position_dodge(width = 0.95))+\n      geom_text(aes(label=n),position = position_dodge(width = 0.95),hjust=1)+\n      coord_flip()+labs(fill=\"Outcome\")+\n      xlab(\"Name\")+ylab(\"Count\")+\n      ggtitle(\"Top 10 Tennis Players and their Outcomes\")"
  },
  {
    "objectID": "tidytuesday/2019/Week_15/index.html#top-10-players-and-tournament-with-outcomes-of-activeness",
    "href": "tidytuesday/2019/Week_15/index.html#top-10-players-and-tournament-with-outcomes-of-activeness",
    "title": "Week 15 : Tennis Tournaments",
    "section": "Top 10 Players and Tournament with outcomes of activeness",
    "text": "Top 10 Players and Tournament with outcomes of activeness\n\ngrand_slam_timeline %&gt;%\n      rename(name=\"player\") %&gt;%\n      inner_join(top10,\"name\") %&gt;%\n      group_by(tournament,outcome) %&gt;%\n      count() %&gt;%\nggplot(.,aes(x=tournament,y=n,fill=outcome))+\n      geom_col(position =position_dodge(width=0.95))+\n      geom_text(aes(label=n),position =position_dodge(width=0.95),vjust=1)+\n      labs(fill=\"Outcome\")+\n      xlab(\"Tournament\")+ylab(\"Count\")+\n      ggtitle(\"Tournament perspective of Top 10 Tennis Players and their Outcomes\")"
  },
  {
    "objectID": "tidytuesday/2019/Week_15/index.html#top-10-players-winning-based-on-tournament",
    "href": "tidytuesday/2019/Week_15/index.html#top-10-players-winning-based-on-tournament",
    "title": "Week 15 : Tennis Tournaments",
    "section": "Top 10 Players Winning based on Tournament",
    "text": "Top 10 Players Winning based on Tournament\n\ngrand_slam_timeline %&gt;%\n      rename(name=\"player\") %&gt;%\n      inner_join(top10,\"name\") %&gt;%\n      subset(outcome==\"Won\") %&gt;%\n      group_by(name,tournament) %&gt;%\n      count() %&gt;%\nggplot(.,aes(x=name,y=n,fill=tournament))+\n      geom_col(position =position_dodge(width=0.95))+\n      geom_text(aes(label=n),position =position_dodge(width=0.95),hjust=1)+\n      coord_flip()+\n      labs(fill=\"Tournament\")+\n      xlab(\"Name\")+ylab(\"Count\")+\n      ggtitle(\"Winning, Top 10 Tennis Players with related to Tournament\")"
  },
  {
    "objectID": "tidytuesday/2019/Week_15/index.html#top-10-players-finalist-based-on-tournament",
    "href": "tidytuesday/2019/Week_15/index.html#top-10-players-finalist-based-on-tournament",
    "title": "Week 15 : Tennis Tournaments",
    "section": "Top 10 Players Finalist based on Tournament",
    "text": "Top 10 Players Finalist based on Tournament\n\ngrand_slam_timeline %&gt;%\n      rename(name=\"player\") %&gt;%\n      inner_join(top10,\"name\") %&gt;%\n      subset(outcome==\"Finalist\") %&gt;%\n      group_by(name,tournament) %&gt;%\n      count() %&gt;%\nggplot(.,aes(x=name,y=n,fill=tournament))+\n      geom_col(position =position_dodge(width=0.95))+\n      geom_text(aes(label=n),position =position_dodge(width=0.95),hjust=1)+\n      coord_flip()+\n      labs(fill=\"Tournament\")+\n      xlab(\"Name\")+ylab(\"Count\")+\n      ggtitle(\"Finalist, Top 10 Tennis Players with related to Tournament\")"
  },
  {
    "objectID": "tidytuesday/2019/Week_15/index.html#top-10-players-semi-finalist-based-on-tournament",
    "href": "tidytuesday/2019/Week_15/index.html#top-10-players-semi-finalist-based-on-tournament",
    "title": "Week 15 : Tennis Tournaments",
    "section": "Top 10 Players Semi-Finalist based on Tournament",
    "text": "Top 10 Players Semi-Finalist based on Tournament\n\ngrand_slam_timeline %&gt;%\n      rename(name=\"player\") %&gt;%\n      inner_join(top10,\"name\") %&gt;%\n      subset(outcome==\"Semi-finalist\") %&gt;%\n      group_by(name,tournament) %&gt;%\n      count() %&gt;%\nggplot(.,aes(x=name,y=n,fill=tournament))+\n      geom_col(position =position_dodge(width=0.95))+\n      geom_text(aes(label=n),position =position_dodge(width=0.95),hjust=1)+\n      coord_flip()+\n      labs(fill=\"Tournament\")+\n      xlab(\"Name\")+ylab(\"Count\")+\n      ggtitle(\"Semi-Finalist, Top 10 Tennis Players with related to Tournament\")"
  },
  {
    "objectID": "tidytuesday/2019/Week_15/index.html#top-10-players-retired-based-on-tournament",
    "href": "tidytuesday/2019/Week_15/index.html#top-10-players-retired-based-on-tournament",
    "title": "Week 15 : Tennis Tournaments",
    "section": "Top 10 Players Retired based on Tournament",
    "text": "Top 10 Players Retired based on Tournament\n\ngrand_slam_timeline %&gt;%\n      rename(name=\"player\") %&gt;%\n      inner_join(top10,\"name\") %&gt;%\n      subset(outcome==\"Retired\") %&gt;%\n      group_by(name,tournament) %&gt;%\n      count() %&gt;%\nggplot(.,aes(x=name,y=n,fill=tournament))+\n      geom_col(position =position_dodge(width=0.95))+\n      geom_text(aes(label=n),position =position_dodge(width=0.95),hjust=1)+\n      coord_flip()+\n      labs(fill=\"Tournament\")+\n      xlab(\"Name\")+ylab(\"Count\")+\n      ggtitle(\"Retired, Top 10 Tennis Players with related to Tournament\")\n\n\n\n\nTHANK YOU"
  },
  {
    "objectID": "tidytuesday/2019/Week_23/index.html",
    "href": "tidytuesday/2019/Week_23/index.html",
    "title": "Week 23 : Ramen Ratings",
    "section": "",
    "text": "ramen_ratings &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-06-04/ramen_ratings.csv\")\n\nlibrary(tidyverse)\nlibrary(gganimate)\nlibrary(ggthemes)\nI have not posted regarding #TidyTuesday in a while, so here it is. It is all about manipulating the dataset and generating necessary plots.\nData GitHub Code\n{{% tweet \"1136303490978988033\" %}}\nramen_ratings %&gt;%\n  count(stars,sort = TRUE) %&gt;%\nggplot(.,aes(fct_inorder(as_factor(stars)),n,label=n))+\n      geom_col(fill=blues9[5])+\n      geom_text(vjust=-0.5,color=blues9[9])+ylab(\"Count / Frequency\")+\n      xlab(\"Stars\")+ggtitle(\"Stars Distribution\")+\n      theme_economist()+\n      theme(axis.text.x = element_text(angle = 45))"
  },
  {
    "objectID": "tidytuesday/2019/Week_23/index.html#brand-versus-stars",
    "href": "tidytuesday/2019/Week_23/index.html#brand-versus-stars",
    "title": "Week 23 : Ramen Ratings",
    "section": "Brand versus Stars",
    "text": "Brand versus Stars\nRating where stars is 4 or above 4\n\nramen_ratings %&gt;%\n  subset(stars &gt;=4)%&gt;%\n  count(brand,sort = TRUE) %&gt;%\n  top_n(25) %&gt;%\nggplot(.,aes(x=fct_inorder(brand),y=n,label=n))+geom_col(fill=blues9[5])+\n      ylab(\"Count / Frequency\")+xlab(\"Brand\")+\n      geom_text(hjust=-0.5,color=blues9[9])+\n      ggtitle(\"Brands Distribution by Stars\",\n              subtitle = \"Top 25\")+\n      theme_economist()+ coord_flip()\n\n\n\n\nRating where stars is below 2\n\nramen_ratings %&gt;%\n  subset(stars &lt; 2)%&gt;%\n  count(brand,sort = TRUE) %&gt;%\n  top_n(25) %&gt;%\nggplot(.,aes(x=fct_inorder(brand),y=n,label=n))+geom_col(fill=blues9[5])+\n      ylab(\"Count / Frequency\")+xlab(\"Brand\")+\n      geom_text(hjust=-0.5,color=blues9[9])+\n      ggtitle(\"Brands Distribution by Stars\",\n              subtitle = \"Top 25\")+\n      theme_economist()+ coord_flip()"
  },
  {
    "objectID": "tidytuesday/2019/Week_23/index.html#style-versus-stars",
    "href": "tidytuesday/2019/Week_23/index.html#style-versus-stars",
    "title": "Week 23 : Ramen Ratings",
    "section": "Style versus Stars",
    "text": "Style versus Stars\nRating where stars is 4 or above 4\n\nramen_ratings %&gt;%\n  subset(stars &gt;=4)%&gt;%\n  count(style,sort = TRUE) %&gt;%\n  top_n(10) %&gt;%\nggplot(.,aes(x=fct_inorder(style),y=n,label=n))+geom_col(fill=blues9[5])+\n      ylab(\"Count / Frequency\")+xlab(\"Style\")+\n      geom_text(hjust=-0.5,color=blues9[9])+\n      ggtitle(\"Styles Distribution by Stars\",\n              subtitle = \"Top 10\")+\n      theme_economist()+ coord_flip()\n\n\n\n\nRating where stars is below 2\n\nramen_ratings %&gt;%\n  subset(stars &lt; 2)%&gt;%\n  count(style,sort = TRUE) %&gt;%\n  top_n(5) %&gt;%\nggplot(.,aes(x=fct_inorder(style),y=n,label=n))+geom_col(fill=blues9[5])+\n      ylab(\"Count / Frequency\")+xlab(\"Style\")+\n      geom_text(hjust=-0.5,color=blues9[9])+\n      ggtitle(\"Styles Distribution by Stars\",\n              subtitle = \"Top 5\")+\n      theme_economist()+ coord_flip()"
  },
  {
    "objectID": "tidytuesday/2019/Week_23/index.html#country-versus-stars",
    "href": "tidytuesday/2019/Week_23/index.html#country-versus-stars",
    "title": "Week 23 : Ramen Ratings",
    "section": "Country versus Stars",
    "text": "Country versus Stars\nRating where stars is 4 or above 4\n\nramen_ratings %&gt;%\n  subset(stars &gt;=4)%&gt;%\n  count(country,sort = TRUE) %&gt;%\n  top_n(25) %&gt;%\nggplot(.,aes(x=fct_inorder(country),y=n,label=n))+geom_col(fill=blues9[5])+\n      ylab(\"Count / Frequency\")+xlab(\"Country\")+\n      geom_text(hjust=-0.5,color=blues9[9])+\n      ggtitle(\"Countries Distribution by Stars\",\n              subtitle = \"Top 25\")+\n      theme_economist()+ coord_flip()\n\n\n\n\nRating where stars is below 2\n\nramen_ratings %&gt;%\n  subset(stars &lt; 2)%&gt;%\n  count(country,sort = TRUE) %&gt;%\n  top_n(25) %&gt;%\nggplot(.,aes(x=fct_inorder(country),y=n,label=n))+geom_col(fill=blues9[5])+\n      ylab(\"Count / Frequency\")+xlab(\"Country\")+\n      geom_text(hjust=-0.5,color=blues9[9])+\n      ggtitle(\"Countries Distribution by Stars\",\n              subtitle = \"Top 20\")+\n      theme_economist()+ coord_flip()"
  },
  {
    "objectID": "tidytuesday/2019/Week_23/index.html#brand-and-style-versus-stars",
    "href": "tidytuesday/2019/Week_23/index.html#brand-and-style-versus-stars",
    "title": "Week 23 : Ramen Ratings",
    "section": "Brand and Style versus Stars",
    "text": "Brand and Style versus Stars\nRating where stars is 4 or above 4\n\nramen_ratings %&gt;%\n  subset(stars &gt;=4)%&gt;%\n  count(brand,style,sort = TRUE) %&gt;%\n  top_n(25) %&gt;%\nggplot(.,aes(x=fct_inorder(brand),y=n,label=n,fill=style))+\n      geom_col(position = \"dodge\")+\n      ylab(\"Count / Frequency\")+xlab(\"Brand\")+\n      geom_text(hjust=-0.25,color=blues9[9],\n                position = position_dodge(width = 1))+\n      ggtitle(\"Brand Distribution by Stars but for Styles\",\n              subtitle = \"Top 25\")+\n      theme_economist()+ coord_flip()\n\n\n\n\nRating where stars is below 2\n\nramen_ratings %&gt;%\n  subset(stars &lt;2)%&gt;%\n  count(brand,style,sort = TRUE) %&gt;%\n  top_n(25) %&gt;%\nggplot(.,aes(x=fct_inorder(brand),y=n,label=n,fill=style))+\n      geom_col(position = \"dodge\")+\n      ylab(\"Count / Frequency\")+xlab(\"Brand\")+\n      geom_text(hjust=-0.25,color=blues9[9],\n                position = position_dodge(width = 1))+\n      ggtitle(\"Brand Distribution by Stars but for Styles\",\n              subtitle = \"Top 25\")+\n      theme_economist()+ coord_flip()"
  },
  {
    "objectID": "tidytuesday/2019/Week_23/index.html#brand-and-country-versus-stars",
    "href": "tidytuesday/2019/Week_23/index.html#brand-and-country-versus-stars",
    "title": "Week 23 : Ramen Ratings",
    "section": "Brand and Country versus Stars",
    "text": "Brand and Country versus Stars\nRating where stars is 4 or above 4\n\nramen_ratings %&gt;%\n  subset(stars &gt;=4)%&gt;%\n  count(brand,country,sort = TRUE) %&gt;%\n  top_n(25) %&gt;%\nggplot(.,aes(x=fct_inorder(brand),y=n,label=n,fill=country))+\n      geom_col(position = \"dodge\")+\n      ylab(\"Count / Frequency\")+xlab(\"Brand\")+\n      geom_text(hjust=-0.25,color=blues9[9],\n                position = position_dodge(width = 1))+\n      ggtitle(\"Brand Distribution by Stars but for Country\",\n              subtitle = \"Top 25\")+\n      theme_economist()+ coord_flip()\n\n\n\n\nRating where stars is below 2\n\nramen_ratings %&gt;%\n  subset(stars &lt;2)%&gt;%\n  count(brand,country,sort = TRUE) %&gt;%\n  top_n(25) %&gt;%\nggplot(.,aes(x=fct_inorder(brand),y=n,label=n,fill=country))+\n      geom_col(position = \"dodge\")+\n      ylab(\"Count / Frequency\")+xlab(\"Brand\")+\n      geom_text(hjust=-0.25,color=blues9[9],\n                position = position_dodge(width = 1))+\n      ggtitle(\"Brand Distribution by Stars but for Country\",\n              subtitle = \"Top 25\")+\n      theme_economist()+ coord_flip()"
  },
  {
    "objectID": "tidytuesday/2019/Week_23/index.html#style-and-country-versus-stars",
    "href": "tidytuesday/2019/Week_23/index.html#style-and-country-versus-stars",
    "title": "Week 23 : Ramen Ratings",
    "section": "Style and Country versus Stars",
    "text": "Style and Country versus Stars\nRating where stars is 4 or above 4\n\nramen_ratings %&gt;%\n  subset(stars &gt;=4)%&gt;%\n  count(style,country,sort = TRUE) %&gt;%\n  top_n(25) %&gt;%\nggplot(.,aes(x=fct_inorder(country),y=n,label=n,fill=style))+\n      geom_col(position = \"dodge\")+\n      ylab(\"Count / Frequency\")+xlab(\"Country\")+\n      geom_text(hjust=-0.25,color=blues9[9],\n                position = position_dodge(width = 1))+\n      ggtitle(\"Style Distribution by Stars but for Country\",\n              subtitle = \"Top 25\")+\n      theme_economist()+ coord_flip()\n\n\n\n\nRating where stars is below 2\n\nramen_ratings %&gt;%\n  subset(stars &lt;2)%&gt;%\n  count(style,country,sort = TRUE) %&gt;%\n  top_n(25) %&gt;%\nggplot(.,aes(x=fct_inorder(country),y=n,label=n,fill=style))+\n      geom_col(position = \"dodge\")+\n      ylab(\"Count / Frequency\")+xlab(\"Country\")+\n      geom_text(hjust=-0.25,color=blues9[9],\n                position = position_dodge(width = 1))+\n      ggtitle(\"Style Distribution by Stars but for Country\",\n              subtitle = \"Top 25\")+\n      theme_economist()+ coord_flip()"
  },
  {
    "objectID": "tidytuesday/2019/Week_23/index.html#brand-style-and-country-versus-stars",
    "href": "tidytuesday/2019/Week_23/index.html#brand-style-and-country-versus-stars",
    "title": "Week 23 : Ramen Ratings",
    "section": "Brand, Style and Country versus Stars",
    "text": "Brand, Style and Country versus Stars\nRating where stars is 4 or above 4\n\nramen_ratings %&gt;%\n  subset(stars &gt;=4)%&gt;%\n  count(brand,style,country,sort = TRUE) %&gt;%\n  top_n(25) %&gt;%\nggplot(.,aes(x=country,y=n,label=n,fill=style))+\n      geom_col(position = \"dodge\")+facet_wrap(~brand)+\n      ylab(\"Count / Frequency\")+xlab(\"Country\")+\n      geom_text(hjust=-0.05,color=blues9[9],size=3.5,\n                position = position_dodge(width = 1))+\n      ggtitle(\"Style Distribution by Stars but for Country\",\n              subtitle = \"Top 25\")+\n      theme_economist()+ coord_flip()\n\n\n\n\nRating where stars is below 2\n\nramen_ratings %&gt;%\n  subset(stars &lt;2)%&gt;%\n  count(brand,style,country,sort = TRUE) %&gt;%\n  top_n(25) %&gt;%\nggplot(.,aes(x=country,y=n,label=n,fill=style))+\n      geom_col(position = \"dodge\")+facet_wrap(~brand)+\n      ylab(\"Count / Frequency\")+xlab(\"Country\")+\n      geom_text(hjust=-0.05,color=blues9[9],size=3.5,\n                position = position_dodge(width = 1))+\n      ggtitle(\"Style Distribution by Stars but for Country\",\n              subtitle = \"Top 25\")+\n      theme_economist()+ coord_flip()"
  },
  {
    "objectID": "tidytuesday/2019/Week_25/index.html",
    "href": "tidytuesday/2019/Week_25/index.html",
    "title": "Week 25: Birds, Ohhhh Canada",
    "section": "",
    "text": "bird_counts &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-06-18/bird_counts.csv\")\n\nlibrary(tidyverse)\nlibrary(gganimate)\nlibrary(ggthemr)\n\nggthemr('flat')\nGitHub Code\n{{% tweet \"1141046651815178240\" %}}"
  },
  {
    "objectID": "tidytuesday/2019/Week_25/index.html#year-and-total-hours-by-decade",
    "href": "tidytuesday/2019/Week_25/index.html#year-and-total-hours-by-decade",
    "title": "Week 25: Birds, Ohhhh Canada",
    "section": "Year and Total Hours by decade",
    "text": "Year and Total Hours by decade\n\nbird_counts_new&lt;-bird_counts\nbird_counts_new$year&lt;-cut(bird_counts_new$year,\n                          breaks=c(1920,1929,1939,1949,\n                                   1959,1969,1979,1989,\n                                   1999,2009,2017),\n                          labels=c(\"1920s\",\"1930s\",\"1940s\",\"1950s\",\n                                   \"1960s\",\"1970s\",\"1980s\",\"1990s\",\n                                   \"2000s\",\"2010s\"))\n\n  bird_counts_new %&gt;%\n  replace_na(list(total_hours = 0)) %&gt;%\n  group_by(year) %&gt;%\n  summarise(mean_hours=mean(total_hours),sum_hours=sum(total_hours)) %&gt;%\n  ggplot(.,aes(year,sum_hours))+geom_col()+\n         xlab(\"Year\")+ylab(\"Summation of Total Hours \\n(Mean Total Hours)\")+\n         ggtitle(\"Total Hours by Decade\")+\n         scale_y_continuous(expand=c(0,25000))+\n         geom_text(aes(label=sum_hours),size=3.5,vjust=-0.5)+\n         geom_text(aes(label=round(mean_hours,2)),vjust=1)"
  },
  {
    "objectID": "tidytuesday/2019/Week_25/index.html#year-and-how-many-counted-by-decade",
    "href": "tidytuesday/2019/Week_25/index.html#year-and-how-many-counted-by-decade",
    "title": "Week 25: Birds, Ohhhh Canada",
    "section": "Year and How many Counted by decade",
    "text": "Year and How many Counted by decade\n\nbird_counts_new&lt;-bird_counts\nbird_counts_new$year&lt;-cut(bird_counts_new$year,\n                          breaks=c(1920,1929,1939,1949,\n                                   1959,1969,1979,1989,\n                                   1999,2009,2017),\n                          labels=c(\"1920s\",\"1930s\",\"1940s\",\"1950s\",\n                                   \"1960s\",\"1970s\",\"1980s\",\"1990s\",\n                                   \"2000s\",\"2010s\"))\n\n  bird_counts_new %&gt;%\n  replace_na(list(how_many_counted = 0)) %&gt;%\n  group_by(year) %&gt;%\n  summarise(mean_counted=mean(how_many_counted),\n            sum_counted=sum(how_many_counted)) %&gt;%\n  ggplot(.,aes(year,sum_counted))+geom_col()+\n         xlab(\"Year\")+ylab(\"Summation of How Many Counted \\n(Mean of How many counted)\")+\n         ggtitle(\"Total of How many counted\")+\n         scale_y_continuous(expand=c(0,75000))+\n         geom_text(aes(label=sum_counted),size=3.5,vjust=-0.5)+\n         geom_text(aes(label=round(mean_counted,2)),vjust=1)"
  },
  {
    "objectID": "tidytuesday/2019/Week_25/index.html#year-and-how-many-hours-counted-by-hour-for-decade",
    "href": "tidytuesday/2019/Week_25/index.html#year-and-how-many-hours-counted-by-hour-for-decade",
    "title": "Week 25: Birds, Ohhhh Canada",
    "section": "Year and How many Hours Counted by Hour for decade",
    "text": "Year and How many Hours Counted by Hour for decade\n\nbird_counts_new&lt;-bird_counts\nbird_counts_new$year&lt;-cut(bird_counts_new$year,\n                          breaks=c(1920,1929,1939,1949,\n                                   1959,1969,1979,1989,\n                                   1999,2009,2017),\n                          labels=c(\"1920s\",\"1930s\",\"1940s\",\"1950s\",\n                                   \"1960s\",\"1970s\",\"1980s\",\"1990s\",\n                                   \"2000s\",\"2010s\"))\n\n  bird_counts_new %&gt;%\n  replace_na(list(how_many_counted_by_hour = 0)) %&gt;%\n  group_by(year) %&gt;%\n  summarise(mean_counted_hour=mean(how_many_counted_by_hour),\n            sum_counted_hour=sum(how_many_counted_by_hour)) %&gt;%\n  ggplot(.,aes(year,sum_counted_hour))+geom_col()+\n         xlab(\"Year\")+ylab(\"Summation of How Many Counted by hour \\n(Mean of How many counted by hour)\")+\n         ggtitle(\"Total of How many counted by hour\")+\n         scale_y_continuous(expand=c(0,750))+\n         geom_text(aes(label=round(sum_counted_hour,2)),size=3.5,vjust=-0.5)+\n         geom_text(aes(label=round(mean_counted_hour,2)),vjust=1)"
  },
  {
    "objectID": "tidytuesday/2019/Week_25/index.html#top-10-species-with-how-many-counted-by-year",
    "href": "tidytuesday/2019/Week_25/index.html#top-10-species-with-how-many-counted-by-year",
    "title": "Week 25: Birds, Ohhhh Canada",
    "section": "Top 10 Species with how many counted by year",
    "text": "Top 10 Species with how many counted by year\n\np1&lt;-bird_counts%&gt;%\n    subset(species==\"American Crow\" | species==\"Canada Goose\" | \n           species==\"Common Goldeneye\" | species==\"Common Merganser\" |\n           species==\"European Starling\" | species==\"Greater Scaup\" | \n           species==\"Herring Gull\" | species==\"House Sparrow\" |\n           species==\"Long-tailed Duck\" | species==\"Mallard\") %&gt;%\n  replace_na(list(how_many_counted = 0)) %&gt;%\n  group_by(year,species) %&gt;%\n  ggplot(.,aes(str_wrap(species,12),how_many_counted))+geom_col()+\n         xlab(\"Year\")+ylab(\"How Many Counted\")+\n         ggtitle(\"How many counted for Top 10 Species\",\n                 subtitle =\"Year:{round(frame_time)}\" )+\n         transition_time(year)+ease_aes(\"linear\")+\n         geom_text(aes(label=how_many_counted),size=3.5,vjust=0.5)\n\nanimate(p1,nframes=94,fps=1)"
  },
  {
    "objectID": "tidytuesday/2019/Week_25/index.html#top-10-species-with-how-many-counted-by-decade",
    "href": "tidytuesday/2019/Week_25/index.html#top-10-species-with-how-many-counted-by-decade",
    "title": "Week 25: Birds, Ohhhh Canada",
    "section": "Top 10 Species with how many counted by decade",
    "text": "Top 10 Species with how many counted by decade\n\nbird_counts_new%&gt;%\n    subset(species==\"American Crow\" | species==\"Canada Goose\" | \n           species==\"Common Goldeneye\" | species==\"Common Merganser\" |\n           species==\"European Starling\" | species==\"Greater Scaup\" | \n           species==\"Herring Gull\" | species==\"House Sparrow\" |\n           species==\"Long-tailed Duck\" | species==\"Mallard\") %&gt;%\n  replace_na(list(how_many_counted = 0)) %&gt;%\n  group_by(year,species) %&gt;%\n  summarise(hmc=sum(how_many_counted)) %&gt;%\n  ggplot(.,aes(str_wrap(species,12),hmc))+geom_col()+\n         xlab(\"Year\")+ylab(\"How Many Counted\")+\n         ggtitle(\"How many counted for Top 10 Species\",\n                 subtitle =\"Decade:{closest_state}\" )+\n         transition_states(year)+ease_aes(\"linear\")+\n         geom_text(aes(label=hmc),size=3.5,vjust=0.5)"
  },
  {
    "objectID": "tidytuesday/2019/Week_25/index.html#top-10-species-with-how-many-counted-by-hours-for-year",
    "href": "tidytuesday/2019/Week_25/index.html#top-10-species-with-how-many-counted-by-hours-for-year",
    "title": "Week 25: Birds, Ohhhh Canada",
    "section": "Top 10 Species with how many counted by hours for year",
    "text": "Top 10 Species with how many counted by hours for year\n\np1&lt;-bird_counts%&gt;%\n    subset(species==\"American Crow\" | species==\"Canada Goose\" | \n           species==\"Common Goldeneye\" | species==\"Common Merganser\" |\n           species==\"European Starling\" | species==\"Greater Scaup\" | \n           species==\"Herring Gull\" | species==\"House Sparrow\" |\n           species==\"Long-tailed Duck\" | species==\"Mallard\") %&gt;%\n  replace_na(list(how_many_counted_by_hour = 0)) %&gt;%\n  group_by(year,species) %&gt;%\n  ggplot(.,aes(str_wrap(species,12),how_many_counted_by_hour))+geom_col()+\n         xlab(\"Year\")+ylab(\"How Many Counted by Hour\")+\n         ggtitle(\"How many counted by Hour for Top 10 Species\",\n                 subtitle =\"Year:{round(frame_time)}\" )+\n         transition_time(year)+ease_aes(\"linear\")+\n         geom_text(aes(label=round(how_many_counted_by_hour,2)),size=3.5,vjust=0.5)\n\nanimate(p1,nframes=94,fps=1)"
  },
  {
    "objectID": "tidytuesday/2019/Week_25/index.html#top-10-species-with-how-many-counted-by-hours-for-decade",
    "href": "tidytuesday/2019/Week_25/index.html#top-10-species-with-how-many-counted-by-hours-for-decade",
    "title": "Week 25: Birds, Ohhhh Canada",
    "section": "Top 10 Species with how many counted by hours for decade",
    "text": "Top 10 Species with how many counted by hours for decade\n\nbird_counts_new%&gt;%\n    subset(species==\"American Crow\" | species==\"Canada Goose\" | \n           species==\"Common Goldeneye\" | species==\"Common Merganser\" |\n           species==\"European Starling\" | species==\"Greater Scaup\" | \n           species==\"Herring Gull\" | species==\"House Sparrow\" |\n           species==\"Long-tailed Duck\" | species==\"Mallard\") %&gt;%\n  replace_na(list(how_many_counted_by_hour = 0)) %&gt;%\n  group_by(year,species) %&gt;%\n  summarise(hmc=sum(how_many_counted)) %&gt;%\n  ggplot(.,aes(str_wrap(species,12),hmc))+geom_col()+\n         xlab(\"Year\")+ylab(\"How Many Counted\")+\n         ggtitle(\"How many counted for Top 10 Species\",\n                 subtitle =\"Decade:{closest_state}\" )+\n         transition_states(year)+ease_aes(\"linear\")+\n         geom_text(aes(label=round(hmc,2)),size=3.5,vjust=0.5)\n\n\n\n\nTHANK YOU"
  },
  {
    "objectID": "tidytuesday/2019/Week_27/index.html",
    "href": "tidytuesday/2019/Week_27/index.html",
    "title": "Week 27: Media Franchise Powerhouses",
    "section": "",
    "text": "media_franchises &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-07-02/media_franchises.csv\")\n\nlibrary(tidyverse)\nlibrary(cowplot)\nlibrary(rayshader)\nlibrary(ggmosaic)\nlibrary(gganimate)\nlibrary(viridis)\n\n\n\n{{% tweet \"1145985415326945280\" %}}\n\n\nGitHub Code\nRevenue Category and Original Media by Cow Plot\n\np1&lt;-media_franchises %&gt;%\n    group_by(revenue_category) %&gt;%\n    summarise(revenue_total=sum(revenue),revenue_avg=mean(revenue)) %&gt;%\nggplot(.,aes(x=str_wrap(revenue_category,15),y=revenue_total,fill=revenue_avg))+geom_col()+\n       xlab(\"Revenue Category\")+ylab(\"Revenue Total by $\")+\n       ggtitle(\"Revenue Category vs Revenue Total\",\n               subtitle = \"By Billions of Dollars\")+\n       theme(axis.text.x = element_text(size=10.5,angle = 90))+\n       labs(fill=\"Revenue\\nAverage\\nIn Billions\")+\n       scale_y_continuous(expand = c(0,150))+\n       geom_text(aes(label=revenue_total),vjust=-0.5)\n  \np2&lt;-media_franchises %&gt;%\n    group_by(original_media) %&gt;%\n    summarise(revenue_total=sum(revenue),revenue_avg=mean(revenue)) %&gt;%\nggplot(.,aes(x=str_wrap(original_media,15),y=revenue_total,fill=revenue_avg))+geom_col()+\n       xlab(\"Original Media\")+\n       ylab(\"Original media by $\")+\n       ggtitle(\"Original Media vs Revenue Total\",\n               subtitle = \"By Billions of Dollars\")+\n       labs(fill=\"Revenue\\nAverage\\nIn Billions\")+\n       scale_y_continuous(expand = c(0,50))+\n       geom_text(aes(label=revenue_total),hjust=-0.15)+coord_flip()\n\nggdraw()+\n    draw_plot(p2+theme(legend.justification = \"top\"),0,0,1,1)+\n    draw_plot(p1+scale_color_viridis_d()+theme(legend.justification = \"bottom\"), 0.45, 0.08, 0.55, 0.55)+\n    draw_plot_label(c(\"A\", \"B\"), c(0, 0.55), c(1, 0.45), size = 5.5)\n\n\n\n\nRevenue Category and Original Media by Animations\n\np1&lt;-media_franchises %&gt;%\n    group_by(year_created,revenue_category) %&gt;%\n    summarise(revenue_total=sum(revenue),revenue_avg=mean(revenue)) %&gt;% \nggplot(.,aes(x=str_wrap(revenue_category,15),y=revenue_total,\n             fill=revenue_avg))+ geom_col()+\n       xlab(\"Revenue Category\")+ ylab(\"Revenue Total by $\")+\n       transition_time(year_created)+ease_aes(\"linear\")+\n       ggtitle(\"Revenue Category vs Revenue Total\",\n               subtitle = \"By Billions of Dollars of Year : {round(frame_time)}\")+\n       labs(fill=\"Revenue\\nAverage\\nIn Billions\")+\n       geom_text(aes(label=revenue_total),vjust=-0.5)\n\nanimate(p1,nframes=52,fps=1)\n\n\n\n\n\np1&lt;-media_franchises %&gt;%\n    group_by(year_created,original_media) %&gt;%\n    summarise(revenue_total=sum(revenue),revenue_avg=mean(revenue)) %&gt;% \nggplot(.,aes(x=str_wrap(original_media,15),y=revenue_total,\n             fill=revenue_avg))+ geom_col()+coord_flip()+\n       xlab(\"Original Media\")+ ylab(\"Revenue Total by $\")+\n       transition_time(year_created)+ease_aes(\"linear\")+\n       ggtitle(\"Original Media vs Revenue Total\",\n               subtitle = \"By Billions of Dollars of Year : {round(frame_time)}\")+\n       labs(fill=\"Revenue\\nAverage\\nIn Billions\")+\n       geom_text(aes(label=revenue_total),hjust=-0.05)\n\nanimate(p1,nframes=52,fps=1)\n\n\n\n\nRevenue Category and Original Media by rayshader\n\np1&lt;-media_franchises %&gt;%\n    group_by(revenue_category) %&gt;%\n    summarise(revenue_total=sum(revenue),revenue_avg=mean(revenue)) %&gt;%\n    ggplot(.,aes(x=str_wrap(revenue_category,10),y=revenue_total,fill=revenue_avg))+geom_col()+\n       xlab(\"Revenue Category\")+ylab(\"Revenue Total by $\")+\n       ggtitle(\"Revenue Category vs Revenue Total\",\n               subtitle = \"By Billions of Dollars\")+theme_minimal()+\n       theme(axis.text = element_text(size=7,angle = 90),\n             title = element_text(size=10))+\n       labs(fill=\"Revenue\\nAverage\\nIn Billions\")+\n       scale_y_continuous(expand = c(0,150))\n\nplot_gg(p1,width=3.5,multicore = TRUE, windowsize = c(1000, 1000), \n        zoom = 0.85, phi = 60, theta = 30, sunangle = 270, soliddepth = -60)\nrender_snapshot(clear=TRUE)\n\n\n\n\n\np1&lt;-media_franchises %&gt;%\n    group_by(original_media) %&gt;%\n    summarise(revenue_total=sum(revenue),revenue_avg=mean(revenue)) %&gt;%\n    ggplot(.,aes(x=original_media,y=revenue_total,fill=revenue_avg))+geom_col()+\n       xlab(\"Original Media\")+ylab(\"Revenue Total by $\")+\n       ggtitle(\"Original Media vs Revenue Total\",\n               subtitle = \"By Billions of Dollars\")+theme_minimal()+\n       theme(axis.text = element_text(size=7,angle = 90))+\n       labs(fill=\"Revenue\\nAverage\\nIn Billions\")\n\nplot_gg(p1,width=3.5,multicore = TRUE, windowsize = c(1000, 800), \n        zoom = 0.85, phi = 40, theta = 50, sunangle = 270, soliddepth = -30)\nrender_snapshot(clear=TRUE)\n\n\n\n\nRevenue Category In Decades\n\nmedia_franchises %&gt;%\n    mutate(years=cut(year_created,\n                     breaks=c(1920,1929,1939,1949,\n                              1959,1969,1979,1989,\n                              1999,2009,2017),\n                     labels=c(\"1920s\",\"1930s\",\"1940s\",\"1950s\",\n                              \"1960s\",\"1970s\",\"1980s\",\"1990s\",\n                              \"2000s\",\"2010s\"))) %&gt;%\n    group_by(years,revenue_category) %&gt;%\nggplot(.) + \n    geom_mosaic(aes(x=product(years),fill=revenue_category))+\n    xlab(\"Years by Decade\")+ylab(\"Revenue Category\")+\n    theme(axis.text.x = element_text(angle=90))+\n    labs(fill=\"Revenue\\nCategory\")+\n    ggtitle(\"Mosaic Graph for Decades of Year vs Revenue of Category \")\n\n\n\n\nRevenue Category In Decades but with Total Revenue\n\nmedia_franchises %&gt;%\n    mutate(years=cut(year_created,\n                     breaks=c(1920,1929,1939,1949,\n                              1959,1969,1979,1989,\n                              1999,2009,2017),\n                     labels=c(\"1920s\",\"1930s\",\"1940s\",\"1950s\",\n                              \"1960s\",\"1970s\",\"1980s\",\"1990s\",\n                              \"2000s\",\"2010s\"))) %&gt;%\n    group_by(years,revenue_category) %&gt;%\n    summarise(revenue_total=sum(revenue),revenue_avg=mean(revenue)) %&gt;%\nggplot(.,aes(x=str_wrap(revenue_category,10),y=revenue_total,fill=revenue_avg))+\n    xlab(\"Revenue Category\")+ylab(\"Revenue Total\")+geom_col()+\n    transition_states(years)+ease_aes(\"linear\")+\n    labs(fill=\"Revenue\\nAverage\")+\n    geom_text(aes(label=revenue_total),vjust=-1)+\n    ggtitle(\"Revenue Total in Billion $ for Revenue Category\",\n            subtitle = \"Year : {closest_state}\")\n\n\n\n\nTHANK YOU"
  },
  {
    "objectID": "tidytuesday/2019/Week_29/index.html",
    "href": "tidytuesday/2019/Week_29/index.html",
    "title": "Week 29: R4DS Users",
    "section": "",
    "text": "r4ds_members &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-07-16/r4ds_members.csv\")\n\n# load the packages\nlibrary(tidyverse)\nlibrary(ggstatsplot)\nlibrary(lubridate)\nGitHub Code\n{{% tweet \"1151089852663963650\" %}}"
  },
  {
    "objectID": "tidytuesday/2019/Week_29/index.html#timely-change-for-full-members",
    "href": "tidytuesday/2019/Week_29/index.html#timely-change-for-full-members",
    "title": "Week 29: R4DS Users",
    "section": "Timely Change for Full Members",
    "text": "Timely Change for Full Members\n\nggplot(r4ds_members,aes(date,full_members))+geom_point(color=blues9[7])+\n      ggthemes::theme_stata()+\n      xlab(\"Date\")+ylab(\"Full Members\")+\n      scale_x_date(date_labels = \"%y %b %d\",breaks = '1 month')+\n      theme(axis.text.x = element_text(angle = 60,hjust=1))+\n      ggtitle(\"How R4DS Members have involved over the Years\")"
  },
  {
    "objectID": "tidytuesday/2019/Week_33/index.html",
    "href": "tidytuesday/2019/Week_33/index.html",
    "title": "Week 33: Roman Emperors",
    "section": "",
    "text": "This is a very small data frame, which is with 68 observations and 16 variables. Several plots were generated but main focus was on using the ‘ggpol’ package.\n# load the data\nemperors &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-08-13/emperors.csv\")\n\n# load packages\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(ggpol)\n\n# load theme package\nlibrary(ggthemr)\nggthemr(\"flat\")\nGitHub Code\n{{% tweet \"1161214486012993536\" %}}"
  },
  {
    "objectID": "tidytuesday/2019/Week_33/index.html#era",
    "href": "tidytuesday/2019/Week_33/index.html#era",
    "title": "Week 33: Roman Emperors",
    "section": "Era",
    "text": "Era\n\n# character to factor type\nemperors$era&lt;-factor(emperors$era)\n\nemperors %&gt;%\n  count(era) %&gt;%\nggplot(.) + \n  geom_parliament(aes(seats = n, fill = era)) + \n  scale_fill_manual(values=c(\"red\",\"yellow\"),labels = unique(emperors$era))+\n  ggtitle(\"Count based on Era\")+\n  coord_fixed() + theme_void()"
  },
  {
    "objectID": "tidytuesday/2019/Week_33/index.html#risen-to-power",
    "href": "tidytuesday/2019/Week_33/index.html#risen-to-power",
    "title": "Week 33: Roman Emperors",
    "section": "Risen to Power",
    "text": "Risen to Power\n\n# character to factor type\nemperors$rise&lt;-factor(emperors$rise)\n\nemperors %&gt;%\n  count(rise) %&gt;%\nggplot(.) + \n  geom_parliament(aes(seats = n, fill = rise)) + \n  scale_fill_manual(values=blues9,labels = unique(emperors$rise))+\n  coord_fixed() + theme_void()"
  },
  {
    "objectID": "tidytuesday/2019/Week_33/index.html#cause-of-death",
    "href": "tidytuesday/2019/Week_33/index.html#cause-of-death",
    "title": "Week 33: Roman Emperors",
    "section": "Cause of Death",
    "text": "Cause of Death\n\n# character to factor type\nemperors$cause&lt;-factor(emperors$cause)\n\nemperors %&gt;%\n  count(cause) %&gt;%\nggplot(.) + \n  geom_parliament(aes(seats = n, fill = cause)) + \n  scale_fill_manual(values=blues9,labels = unique(emperors$cause))+\n  coord_fixed() + theme_void()"
  },
  {
    "objectID": "tidytuesday/2019/Week_33/index.html#killer",
    "href": "tidytuesday/2019/Week_33/index.html#killer",
    "title": "Week 33: Roman Emperors",
    "section": "Killer",
    "text": "Killer\n\n# character to factor type\nemperors$killer&lt;-factor(emperors$killer)\n\nemperors %&gt;%\n  count(killer) %&gt;%\nggplot(.) + \n  geom_parliament(aes(seats = n, fill = killer)) + \n  scale_fill_manual(values=c(blues9,\"red\",\"yellow\",\"green\",\"grey\",\"black\"),\n                    labels = unique(emperors$killer))+\n  coord_fixed() + theme_void()"
  },
  {
    "objectID": "tidytuesday/2019/Week_33/index.html#dynasty",
    "href": "tidytuesday/2019/Week_33/index.html#dynasty",
    "title": "Week 33: Roman Emperors",
    "section": "Dynasty",
    "text": "Dynasty\n\n# character to factor type\nemperors$dynasty&lt;-factor(emperors$dynasty)\n\nemperors %&gt;%\n  count(dynasty) %&gt;%\nggplot(.) + \n  geom_parliament(aes(seats = n, fill = dynasty)) + \n  scale_fill_manual(values=blues9,labels = unique(emperors$dynasty))+\n  coord_fixed() + theme_void()\n\n\n\n\nThe ‘ggpol’ plots seem fine, to be fair they would be much more cool if we have a proper color palette. Also it would make more sense if the number of observations are higher also but not above 500. Rather than that it looks fine.\nTHANK YOU"
  },
  {
    "objectID": "tidytuesday/2019/Week_4/index.html",
    "href": "tidytuesday/2019/Week_4/index.html",
    "title": "Week 4: Prison Data",
    "section": "",
    "text": "# load the packages\nlibrary(readr)\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(gganimate)\nlibrary(ggthemr)\n\n# load the theme\nggthemr(\"flat dark\")\n\n# load the data\npretrial_summary &lt;- read_csv(\"pretrial_summary.csv\")\n\nprison_summary &lt;- read_csv(\"prison_summary.csv\")\n\nincarceration_trends&lt;-read_csv(\"incarceration_trends.csv\")\nTidyTuesday Week 4 of 2019 is focused on prison data. You can find the data here. There are 5 csv files, clearly 2 files are a summary of the main data, which are Prison Summary and Pretrial Summary.\n{{% tweet \"1087702502240407552\" %}}\nI have mainly focused on these two data-sets here. Further in order of curiosity I did take a peak at a main data file, which is incarceration_trends.csv."
  },
  {
    "objectID": "tidytuesday/2019/Week_4/index.html#prison-summary-with-gender",
    "href": "tidytuesday/2019/Week_4/index.html#prison-summary-with-gender",
    "title": "Week 4: Prison Data",
    "section": "Prison Summary With Gender",
    "text": "Prison Summary With Gender\nThe population increase has an effect on it according to the below plot. Urban area has an increase in these prisoners over the years but after mid 1990 there is a decline. This is true for males. Next considering the suburban area this is quite similar as before, only difference is that the decline begins in year 2005.\nConsidering rural area there is a clear increase of prisoners for both genders in the years. There is an anomaly in year 1986 with alot of prisoners for males. Both rural and small/mid areas behave similarly for both genders as the increase rate gets somewhat slower after 2010.\n\nsubset(prison_summary,pop_category==\"Male\" | pop_category==\"Female\") %&gt;%\nggplot(.,aes(x=year,y=rate_per_100000,fill=pop_category))+\n  facet_wrap(~urbanicity)+geom_area()+\n  transition_reveal(year)+ labs(fill=\"Gender\")+\n  scale_x_continuous(breaks=c(1983:2015),labels=c(1983:2015))+\n  theme(axis.text.x = element_text(angle = 90),legend.position = \"bottom\")+\n  scale_y_continuous(breaks = seq(0,1750,250),labels=seq(0,1750,250))+\n  xlab(\"Year\")+ylab(\"Rate per 100,000\")+\n  ggtitle(\"Gender change over the years from 1983-2015\")"
  },
  {
    "objectID": "tidytuesday/2019/Week_4/index.html#prison-summary-with-ethnicity",
    "href": "tidytuesday/2019/Week_4/index.html#prison-summary-with-ethnicity",
    "title": "Week 4: Prison Data",
    "section": "Prison Summary with Ethnicity",
    "text": "Prison Summary with Ethnicity\n5 ethnicity types are considered here which are Asian, Black, Latino, White and Native American. From 1980 only we can see the active prisoners of Latino and Native American ethnicity. Over the years we can the increase of prisoners for African American Community. The increase is very high considering the other ethnicity types.\nAsian ethnicity people have prisoners but it is only negligible considering the other ethnicity types. Except the suburban area others have an increase rate until 2005 and there is a decline followed in the next years. This is not the case for suburban area. Here less change after year 2000 and the decline begins only in year 2010.\n\nsubset(prison_summary,pop_category == \"Asian\" | pop_category==\"Black\" |\n                      pop_category == \"Latino\" |pop_category==\"White\" |\n                      pop_category == \"Native American\" ) %&gt;%\nggplot(.,aes(x=year,y=rate_per_100000,fill=pop_category))+\n  facet_wrap(~urbanicity)+geom_area()+\n  transition_reveal(year)+ labs(fill=\"Ethnicity\")+\n  scale_x_continuous(breaks=c(1983:2015),labels=c(1983:2015))+\n  theme(axis.text.x = element_text(angle = 90),legend.position = \"bottom\")+\n  scale_y_continuous(breaks = seq(0,5250,250),labels=seq(0,5250,250))+\n  xlab(\"Year\")+ylab(\"Rate per 100,000\")+\n  ggtitle(\"Ethnicity change over the years from 1983-2015\")"
  },
  {
    "objectID": "tidytuesday/2019/Week_4/index.html#prison-summary-with-other-and-total",
    "href": "tidytuesday/2019/Week_4/index.html#prison-summary-with-other-and-total",
    "title": "Week 4: Prison Data",
    "section": "Prison Summary with Other and Total",
    "text": "Prison Summary with Other and Total\nThe Other category is no longer active since 1989, but before that we can see different anomalies in all four areas. Clearly urban area has more prisoners and final place goes to suburban according to the below area plot.\nRural area has an increase in prisoners over the years and there is no decline. This is not the case for small/mid and suburban areas. Urban areas has a sudden decline in between year 1995 to 2000 and again there is a steep decline after year 2005.\n\nsubset(prison_summary,pop_category == \"Other\" | pop_category==\"Total\") %&gt;%\nggplot(.,aes(x=year,y=rate_per_100000,fill=pop_category))+\n    facet_wrap(~urbanicity)+geom_area()+\n    transition_reveal(year)+ labs(fill=\"Category\")+\n    scale_x_continuous(breaks=c(1983:2015),labels=c(1983:2015))+\n    theme(axis.text.x = element_text(angle = 90),legend.position = \"bottom\")+\n    scale_y_continuous(breaks = seq(0,1000,100),labels=seq(0,1000,100))+\n    xlab(\"Year\")+ylab(\"Rate per 100,000\")+\n    ggtitle(\"Total and Other category change over the years from 1983-2015\")"
  },
  {
    "objectID": "tidytuesday/2019/Week_4/index.html#rape-crimes-over-the-years-in-states-of-rural-area",
    "href": "tidytuesday/2019/Week_4/index.html#rape-crimes-over-the-years-in-states-of-rural-area",
    "title": "Week 4: Prison Data",
    "section": "Rape Crimes over the Years in States of Rural Area",
    "text": "Rape Crimes over the Years in States of Rural Area\nI developed these plots to understand a pattern in state wise or region wise, apparently its very difficult but still I am keeping these plots here. Well, frankly I think there could be some other better way to visualize the above selective data.\n\np1&lt;-subset(incarceration_trends,year!=\"1970\" & year!=\"1971\" & year!=\"1972\" &\n                            year!=\"1973\" & year!=\"1974\" & year!=\"1975\" &\n                            year!=\"1976\" & year!=\"2015\" & year!=\"2016\" &\n                            urbanicity == \"rural\",\n       select = c(\"year\",\"state\",\"rape_crime\",\"urbanicity\",\"region\")) %&gt;%\nggplot(.,aes(x=factor(state),y=rape_crime,color=region))+\n      geom_jitter(width = 0.1)+transition_time(year)+\n      coord_flip()+ \n      ylab(\"Rape Crime\")+xlab(\"State\")+\n      theme(legend.position = \"bottom\")+\n      labs(title=\"Rape Crime for Rural Areas on the Year : {round(frame_time)}\")\n\nanimate(p1,fps=1,duration=38)"
  },
  {
    "objectID": "tidytuesday/2019/Week_4/index.html#rape-crimes-over-the-years-in-states-of-small-or-mid-area",
    "href": "tidytuesday/2019/Week_4/index.html#rape-crimes-over-the-years-in-states-of-small-or-mid-area",
    "title": "Week 4: Prison Data",
    "section": "Rape Crimes over the Years in States of Small or Mid Area",
    "text": "Rape Crimes over the Years in States of Small or Mid Area\n\np2&lt;-subset(incarceration_trends,year!=\"1970\" & year!=\"1971\" & year!=\"1972\" &\n                            year!=\"1973\" & year!=\"1974\" & year!=\"1975\" &\n                            year!=\"1976\" & year!=\"2015\" & year!=\"2016\" &\n                            urbanicity == \"small/mid\",\n       select = c(\"year\",\"state\",\"rape_crime\",\"urbanicity\",\"region\")) %&gt;%\nggplot(.,aes(x=factor(state),y=rape_crime,color=region))+\n      geom_jitter(width = 0.1)+transition_time(year)+\n      coord_flip()+ \n      ylab(\"Rape Crime\")+xlab(\"State\")+\n      theme(legend.position = \"bottom\")+\n      labs(title=\"Rape Crime for Small or Mid Areas on the Year: {round(frame_time)}\")\n\nanimate(p2,fps=1,duration=38)"
  },
  {
    "objectID": "tidytuesday/2019/Week_4/index.html#rape-crimes-over-the-years-in-states-of-suburban-area",
    "href": "tidytuesday/2019/Week_4/index.html#rape-crimes-over-the-years-in-states-of-suburban-area",
    "title": "Week 4: Prison Data",
    "section": "Rape Crimes over the Years in States of Suburban Area",
    "text": "Rape Crimes over the Years in States of Suburban Area\n\np3&lt;-subset(incarceration_trends,year!=\"1970\" & year!=\"1971\" & year!=\"1972\" &\n                            year!=\"1973\" & year!=\"1974\" & year!=\"1975\" &\n                            year!=\"1976\" & year!=\"2015\" & year!=\"2016\" &\n                            urbanicity == \"suburban\",\n       select = c(\"year\",\"state\",\"rape_crime\",\"urbanicity\",\"region\")) %&gt;%\nggplot(.,aes(x=factor(state),y=rape_crime,color=region))+\n      geom_jitter(width = 0.1)+transition_time(year)+\n      coord_flip()+ \n      ylab(\"Rape Crime\")+xlab(\"State\")+\n      theme(legend.position = \"bottom\")+\n      labs(title=\"Rape Crime for Suburban Areas on the Year: {round(frame_time)}\")\n\nanimate(p3,fps=1,duration=38)"
  },
  {
    "objectID": "tidytuesday/2019/Week_4/index.html#rape-crimes-over-the-years-in-states-of-urban-area",
    "href": "tidytuesday/2019/Week_4/index.html#rape-crimes-over-the-years-in-states-of-urban-area",
    "title": "Week 4: Prison Data",
    "section": "Rape Crimes over the Years in States of Urban Area",
    "text": "Rape Crimes over the Years in States of Urban Area\n\np4&lt;-subset(incarceration_trends,year!=\"1970\" & year!=\"1971\" & year!=\"1972\" &\n                            year!=\"1973\" & year!=\"1974\" & year!=\"1975\" &\n                            year!=\"1976\" & year!=\"2015\" & year!=\"2016\" &\n                            urbanicity == \"urban\",\n       select = c(\"year\",\"state\",\"rape_crime\",\"urbanicity\",\"region\")) %&gt;%\nggplot(.,aes(x=factor(state),y=rape_crime,color=region))+\n      geom_jitter(width = 0.1)+transition_time(year)+\n      coord_flip()+ \n      ylab(\"Rape Crime\")+xlab(\"State\")+\n      theme(legend.position = \"bottom\")+\n      labs(title=\"Rape Crime for Urban Areas on the Year: {round(frame_time)}\")\n\nanimate(p4,fps=1,duration=38)\n\n\n\n\nTHANK YOU"
  },
  {
    "objectID": "tidytuesday/2019/Week_5/index.html",
    "href": "tidytuesday/2019/Week_5/index.html",
    "title": "Week 5: Dairy Products in USA",
    "section": "",
    "text": "# load the packages\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(ggthemr)\nlibrary(readr)\nlibrary(gganimate)\nlibrary(usmap)\n\n# load the data\nfluid_milk_sales &lt;- read_csv(\"fluid_milk_sales.csv\")\nstate_milk_production &lt;- read_csv(\"state_milk_production.csv\")\nclean_cheese &lt;- read_csv(\"clean_cheese.csv\")\n\n# load the theme\nggthemr(\"flat dark\")"
  },
  {
    "objectID": "tidytuesday/2019/Week_5/index.html#us-states-and-milk-production-over-the-years",
    "href": "tidytuesday/2019/Week_5/index.html#us-states-and-milk-production-over-the-years",
    "title": "Week 5: Dairy Products in USA",
    "section": "US States and Milk Production Over the Years",
    "text": "US States and Milk Production Over the Years\nIn the first half of 1970 to 1993 we can see how a few states are having steady increase over the years.\n\nattach(state_milk_production)\n\n# dividing the milk produced by 10^6\n#summary(state_milk_production$milk_produced)\nstate_milk_production$milk_produced&lt;-state_milk_production$milk_produced/(10^6)\n\n# plot in us map the milk produced by state for years 1970 to 1993\nplot_usmap(data=subset(state_milk_production,year &lt;=\"1993\"),values = \"milk_produced\")+\n          facet_wrap(~factor(year),ncol = 4)+\n          ggtitle(\"Over the years Milk production changing in USA\")+\n          theme(legend.position = \"left\")+\n          labs(fill=\"lbs in 10^6\")\n\n\n\n\nThis is similar to the next half which is from 1994 to 2017 as well. Similar increase occurs for the above same states as I see in the below plot. Well it is not very accurately described in the two plots for us to see.\n\n# plot in us map the milk produced by state for years 1994 to 2017\nplot_usmap(data=subset(state_milk_production,year &gt;\"1993\"),values = \"milk_produced\")+\n          facet_wrap(~factor(year),ncol = 4)+\n          ggtitle(\"Over the years Milk production changing in USA\")+\n          theme(legend.position = \"left\")+\n          labs(fill=\"lbs in 10^6\")\n\n\n\n\nTo understand the above same change over the years clearly I have created a bar plot how the increase occurs. This plot also indicates how much change has occurred over the 48 years for each state who produces milk. Clearly the states California and Wisconsin have higher increase over the years, which is very strong. There are some states which have not produced more amount each year than their previous years.\nThe states Wyoming, Rhode island, Hawaii, Delaware and Alaska have very low amount of milk production over the years.\n\n# over the years how milk production has changed for each year in a bar plot\np&lt;-ggplot(state_milk_production,aes(x=state,y=milk_produced,\n                                    fill=year))+\n      geom_bar(stat=\"identity\")+coord_flip()+\n      transition_time(year)+ease_aes(\"linear\")+shadow_mark()+\n      xlab(\"State\")+ylab(\"Milk Produced (in 10^6)\")+\n      ggtitle(\"States vs Milk Produced in year: {round(frame_time)}\")\n\nanimate(p,nframes = 48,fps=1)"
  },
  {
    "objectID": "tidytuesday/2019/Week_5/index.html#summing-states-of-same-regions-over-the-years",
    "href": "tidytuesday/2019/Week_5/index.html#summing-states-of-same-regions-over-the-years",
    "title": "Week 5: Dairy Products in USA",
    "section": "Summing States of Same Regions Over the Years",
    "text": "Summing States of Same Regions Over the Years\nThere are 50 states but only 10 regions and not all regions have equal amount of states. Therefore I am going to sum up the milk production for all regions over the years and try to understand if there is any pattern.\nIn order to do this I have used the dplyr package and created a function which would sum up the production for each region of each year. Similarly, this function has the ability to get the average production for each region of each year as well.\n\n# manipulating the data by sum and mean\nby_region_sum&lt;-function(i,ch_sum)\n{\n  if(ch_sum==TRUE)\n  {\n    # subsetting by summation over all years for each region\n    temp&lt;-subset(state_milk_production,year==i,select=c(\"region\",\"milk_produced\")) %&gt;%\n                  group_by(region) %&gt;%\n                  summarise_each(funs(sum))\n  \n  output&lt;-data.frame(year=rep(i,10), region=temp$region, milk_produced=temp$milk_produced)\n  }\n  else\n  {\n    # subsetting by Average over all years for each region    \n    temp&lt;-subset(state_milk_production,year==i,select=c(\"region\",\"milk_produced\")) %&gt;%\n                  group_by(region) %&gt;%\n                  summarise_each(funs(mean))\n  \n    output&lt;-data.frame(year=rep(i,10), region=temp$region, milk_produced=temp$milk_produced)\n  }\n  return(output)\n}\n\nUsing the by_region_sum function I am now finding the sum as below.\n\n# subsetting by summation\nmilk_region_data&lt;-rbind.data.frame(by_region_sum(1970,T),by_region_sum(1971,T),\n                                   by_region_sum(1972,T),by_region_sum(1973,T),\n                                   by_region_sum(1974,T),by_region_sum(1975,T),\n                                   by_region_sum(1976,T),by_region_sum(1977,T),\n                                   by_region_sum(1978,T),by_region_sum(1979,T),\n                                   by_region_sum(1980,T),by_region_sum(1981,T),\n                                   by_region_sum(1982,T),by_region_sum(1983,T),                   \n                                   by_region_sum(1984,T),by_region_sum(1985,T),               \n                                   by_region_sum(1986,T),by_region_sum(1987,T),                 \n                                   by_region_sum(1988,T),by_region_sum(1989,T),\n                                   by_region_sum(1990,T),by_region_sum(1991,T), \n                                   by_region_sum(1992,T),by_region_sum(1993,T), \n                                   by_region_sum(1994,T),by_region_sum(1995,T), \n                                   by_region_sum(1996,T),by_region_sum(1997,T), \n                                   by_region_sum(1998,T),by_region_sum(1999,T), \n                                   by_region_sum(2000,T),by_region_sum(2001,T),\n                                   by_region_sum(2002,T),by_region_sum(2003,T),   \n                                   by_region_sum(2004,T),by_region_sum(2005,T),                 \n                                   by_region_sum(2006,T),by_region_sum(2007,T), \n                                   by_region_sum(2008,T),by_region_sum(2009,T),\n                                   by_region_sum(2010,T),by_region_sum(2011,T),\n                                   by_region_sum(2012,T),by_region_sum(2013,T),\n                                   by_region_sum(2014,T),by_region_sum(2015,T),\n                                   by_region_sum(2016,T),by_region_sum(2017,T)                  \n                                   )\n\nIf we consider the summation we can see clearly how centered and very limited variation is there for some regions such as Southeast, Northern Plains, Delta States, Corn Belt and Appalachian. There is some variation in the Northeast region. Clear and highest variation is in for Pacific, Mountain and Lake States regions.\n\n# Region wise total milk production changing  over the year \nggplot(milk_region_data,aes(x=region,y=milk_produced,color=year))+\n        geom_jitter()+ coord_flip()+\n        xlab(\"Region\")+ylab(\"Milk Produced (in 10^6)\")+\n        ggtitle(\"Total Milk Produced by Year in All Regions\")\n\n\n\n\nBelow is the same graph with points animated by year.\n\n# Region wise total milk production changing  over the year animated\np&lt;-ggplot(milk_region_data,aes(x=region,y=milk_produced,color=year))+\n        geom_jitter()+ coord_flip()+\n        xlab(\"Region\")+ylab(\"Milk Produced (in 10^6)\")+\n        ggtitle(\"Total Milk Produced for All Regions for Year: {frame_time}\")+\n        transition_time(year)+ease_aes(\"linear\")+shadow_mark()\n        \nanimate(p,nframes=48,fps=1)"
  },
  {
    "objectID": "tidytuesday/2019/Week_5/index.html#averaging-regions-considering-all-the-states-over-the-years",
    "href": "tidytuesday/2019/Week_5/index.html#averaging-regions-considering-all-the-states-over-the-years",
    "title": "Week 5: Dairy Products in USA",
    "section": "Averaging Regions Considering All the States Over the Years",
    "text": "Averaging Regions Considering All the States Over the Years\nIf we consider the same approach but for the average of each region we can develop the same two plots. Here also we can see the same variation and centering for points for the same regions over the years.\n\n# subsetting by average\nmilk_region_data_new&lt;-rbind.data.frame(by_region_sum(1970,F),by_region_sum(1971,F),\n                                   by_region_sum(1972,F),by_region_sum(1973,F),\n                                   by_region_sum(1974,F),by_region_sum(1975,F),\n                                   by_region_sum(1976,F),by_region_sum(1977,F),\n                                   by_region_sum(1978,F),by_region_sum(1979,F),\n                                   by_region_sum(1980,F),by_region_sum(1981,F),\n                                   by_region_sum(1982,F),by_region_sum(1983,F),                   \n                                   by_region_sum(1984,F),by_region_sum(1985,F),               \n                                   by_region_sum(1986,F),by_region_sum(1987,F),                 \n                                   by_region_sum(1988,F),by_region_sum(1989,F),\n                                   by_region_sum(1990,F),by_region_sum(1991,F), \n                                   by_region_sum(1992,F),by_region_sum(1993,F), \n                                   by_region_sum(1994,F),by_region_sum(1995,F), \n                                   by_region_sum(1996,F),by_region_sum(1997,F), \n                                   by_region_sum(1998,F),by_region_sum(1999,F), \n                                   by_region_sum(2000,F),by_region_sum(2001,F),\n                                   by_region_sum(2002,F),by_region_sum(2003,F),   \n                                   by_region_sum(2004,F),by_region_sum(2005,F),                 \n                                   by_region_sum(2006,F),by_region_sum(2007,F), \n                                   by_region_sum(2008,F),by_region_sum(2009,F),\n                                   by_region_sum(2010,F),by_region_sum(2011,F),\n                                   by_region_sum(2012,F),by_region_sum(2013,F),\n                                   by_region_sum(2014,F),by_region_sum(2015,F),\n                                   by_region_sum(2016,F),by_region_sum(2017,F)                  \n                                   )\n\nBelow is the plot for the average of regions over the years.\n\n# Region wise average milk production changing  over the year \nggplot(milk_region_data_new,aes(x=region,y=milk_produced,color=year))+\n        geom_jitter()+ coord_flip()+ \n        xlab(\"Region\")+ylab(\"Milk Produced (in 10^6)\")+\n        ggtitle(\"Average Milk Produced by Year in All Regions\")\n\n\n\n\nThe same plot is now animated for each year and all regions.\n\n# Region wise average milk production changing  over the year animated\np&lt;-ggplot(milk_region_data_new,aes(x=region,y=milk_produced,color=year))+\n        geom_jitter()+ coord_flip()+\n        transition_time(year)+ease_aes(\"linear\")+ shadow_mark()+       \n        xlab(\"Region\")+ylab(\"Milk Produced (in 10^6)\")+\n        ggtitle(\"Average Milk Produced for All Regions for Year: {frame_time}\")+\n        transition_time(year)+ease_aes(\"linear\")\n        \nanimate(p,nframes=48,fps=1)        \n\n\n\ndetach(state_milk_production)"
  },
  {
    "objectID": "tidytuesday/2019/Week_5/index.html#cheese-with-other",
    "href": "tidytuesday/2019/Week_5/index.html#cheese-with-other",
    "title": "Week 5: Dairy Products in USA",
    "section": "Cheese with Other",
    "text": "Cheese with Other\nI am going to consider the types American Other, Italian Other and Swiss for this plot. Red color indicates to American Other, yellow color refers to Italian other and blue for Swiss. Alot of fluctuation for American other type, but this is not the case for Swiss type cheese. There is steady increase for the Italian other type cheese over the years. All of these are less than 4 lbs per person and it is animated.\n\n# 3 types of cheese change per person over the year in lbs\np&lt;-ggplot(clean_cheese,aes(x=factor(Year)))+ \n          geom_point(aes(y=`American Other`),stat=\"identity\",color=\"red\")+\n          geom_point(aes(y=`Italian other`),stat=\"identity\",color=\"yellow\")+\n          geom_point(aes(y=Swiss),stat=\"identity\",color=\"blue\")+\n          transition_time(Year)+\n          theme(axis.text.x =element_text(angle = 90, hjust = 1))+\n          xlab(\"Year\")+ylab(\"Consumption in lbs per person\")+\n          ggtitle(\"Cheese Consumption Over the Years\")+\n          ease_aes(\"linear\")+shadow_mark()\n\nanimate(p,nframes=48,fps=1)"
  },
  {
    "objectID": "tidytuesday/2019/Week_5/index.html#cheese-with-total",
    "href": "tidytuesday/2019/Week_5/index.html#cheese-with-total",
    "title": "Week 5: Dairy Products in USA",
    "section": "Cheese with Total",
    "text": "Cheese with Total\nThis is also an animated plot but for the cheese types which has the word Total. They are Total American Cheese, Total Italian Cheese, Total Natural Cheese and Total Processed Cheese Products with the colors represented respectively red, yellow, blue and green.\nAll the Consumption units are in between 0 to 40 lbs per person. Clearly Total Natural Cheese has a steady amount of increase from 1970(slightly above 10) to 2017(approximately less than 40). Considering the other three types we can see it is not the same order that it is in 1970 over the years.\n\n# 4 types of cheese change per person over the year in lbs\np&lt;-ggplot(clean_cheese,aes(x=factor(Year)))+ \n          geom_point(aes(y=`Total American Chese`),stat=\"identity\",color=\"red\")+\n          geom_point(aes(y=`Total Italian Cheese`),stat=\"identity\",color=\"yellow\")+\n          geom_point(aes(y=`Total Natural Cheese`),stat=\"identity\",color=\"blue\")+\n          geom_point(aes(y=`Total Processed Cheese Products`),stat=\"identity\",color=\"green\")+\n          xlab(\"Year\")+ylab(\"Consumption in lbs per person\")+\n          theme(axis.text.x =element_text(angle = 90, hjust = 1))+  \n          ggtitle(\"Cheese Consumption Over the Years\")+\n          transition_time(Year)+ \n          ease_aes(\"linear\")+shadow_mark()\n\nanimate(p,nframes=48,fps=1)"
  },
  {
    "objectID": "tidytuesday/2019/Week_5/index.html#cheese-with-known-type-names",
    "href": "tidytuesday/2019/Week_5/index.html#cheese-with-known-type-names",
    "title": "Week 5: Dairy Products in USA",
    "section": "Cheese with known Type Names",
    "text": "Cheese with known Type Names\nNext group of cheese types include Cheddar, Mozzarella, Brick, Processed Cheese and Foods & spreads for the colors representing respectively red, yellow, blue, green and white. Clearly Cheddar and mozzarella type cheese are are mostly consumed by 2017 above 10 lbs per person, but this is not the case in 1970 where consumption is less than 6 lbs per person.\nWell Processed Cheese and Foods & Spreads have changed very small over the years. The consumption is always less than 6 lbs per person. This is not the case for Brick type cheese where the consumption is close to zero over the years from 1970 until 2017.\n\n# 5 types of cheese change per person over the year in lbs\np&lt;-ggplot(clean_cheese,aes(x=factor(Year)))+ \n          geom_point(aes(y=Cheddar),stat=\"identity\",color=\"red\")+\n          geom_point(aes(y=Mozzarella),stat=\"identity\",color=\"yellow\")+\n          geom_point(aes(y=`Brick`),stat=\"identity\",color=\"blue\")+\n          geom_point(aes(y=`Processed Cheese`),stat=\"identity\",color=\"green\")+\n          geom_point(aes(y=`Foods and spreads`),stat=\"identity\",color=\"white\")+\n          xlab(\"Year\")+ylab(\"Consumption in lbs per person\")+\n          theme(axis.text.x =element_text(angle = 90, hjust = 1))+\n          ggtitle(\"Cheese Consumption Over the Years\")+\n          transition_time(Year)+ \n          ease_aes(\"linear\")+shadow_mark()\n\nanimate(p,nframes=48,fps=1)\n\n\n\n\nTHANK YOU"
  },
  {
    "objectID": "tidytuesday/2019/Week_7/index.html",
    "href": "tidytuesday/2019/Week_7/index.html",
    "title": "Week 7: Spending On Science Stuff",
    "section": "",
    "text": "# load the packages \nlibrary(readr)\nlibrary(tidyverse)\nlibrary(gganimate)\nlibrary(dplyr)\nlibrary(magrittr)\n\n# load the data\nclimate &lt;- read_csv(\"climate_spending.csv\")\n\nenergy &lt;- read_csv(\"energy_spending.csv\", \n                    col_types = cols(year = col_integer()))\n\nfederal &lt;- read_csv(\"fed_r_d_spending.csv\")\n\nEven though I can go further and do an investigative plotting from the rest data it is not done here. I was more focused on the scientific notation values in the plotting and scales, which were bothering me a lot.\n3 Data sets are given here, they are\n\nGlobal Climate Change Research Program Spending. - climate\nEnergy Departments Data. - energy\nTotal Federal R & D Spending by Department. - federal\n\nOddly though climate data-set did not have year values, I checked the downloaded csv file and the GitHub upload as well. Well, that did not stop me from doing some tidy plotting.\nYou can obtain the data from here. It should be noted that I am not going to rename the abbreviation of departments with their full names, so below is a screen shot which would come in handy.\n\n\nDepartment Full Names with Abbreviations\n\n\n\n{{% tweet \"1095152521507733505\" %}}\n\n\nGitHub Code\nClimate Change Research\nAs I mentioned earlier for the climate data there are no values in the year column, but according to summary I was able to deduce that we have 18 years of information. When we do plot it is going to be the summation for each department in a bar.\nClearly NASA has the most amount ( above than 2.5 x 10^10 USD) of spending because rockets are expensive, second place goes to NSF (5 x 10^9 USD) and third place to NOAA. Lowest amount of spending is to the Department of Interior (8.47 x 10^8 USD).\n\nggplot(climate,aes(x=fct_inorder(department),y=gcc_spending,fill=department))+\n  geom_bar(stat=\"identity\",show.legend = FALSE)+\n  ggtitle(\"Total GCC Spending for 18 Years\")+\n  scale_y_continuous(labels = scales::scientific,breaks = seq(0,2.75e+10,0.25e+10))+\n  xlab(\"Sub Agency / Department\")+ylab(\"GCC Spending (in USD)\")\n\n\n\n\nEnergy\nSince 1997 to 2018 how Energy Department funding has changed with sub agency/ department is the purpose of the below bar plot. Office of Science R & D and Atomic Energy Defense are competitive over the years and for a short period of time the latter has less funding than the former, this was between 2006 to 2010.\nOther agencies oscillates over the years while reaching new highs and lows.\n\np&lt;-ggplot(energy,aes(x=department,y=energy_spending,fill=year))+\n          geom_bar(stat=\"identity\",position =\"identity\")+\n          transition_time(year)+\n          geom_text(aes(label=scales::scientific(energy_spending)),\n                    vjust = \"inward\", hjust = \"inward\")+\n          ease_aes(\"linear\")+coord_flip()+\n          ylab(\"Energy Spending (in USD)\")+\n          theme(legend.position = \"right\")+\n          xlab(\"Sub Agency / Department\")+\n          scale_fill_continuous(breaks = seq(1997,2018,3))+\n          scale_y_continuous(labels = scales::scientific)+\n          ggtitle(\"Energy Spending Of Year : {frame_time}\")\n\nanimate(p,fps=1,nframes=22)\n\n\n\n\nFederal\nData of Federal funding has four different types to be compared and they are mentioned below in the description image which would make explanation more easier.\n\nExcept rd_budget others have a very clear increase in amount between 1976 to 2018. Further, all four plots have different scales and the limits are widely different for each plot.\n\np&lt;-federal %&gt;%\n    gather(funding,amount,c(rd_budget,total_outlays,discretionary_outlays,gdp)) %&gt;%\n    ggplot(.,aes(x=factor(department),y=amount,color=year))+\n           geom_jitter()+transition_time(year)+\n           ease_aes(\"linear\")+coord_flip()+\n           shadow_mark()+\n           theme(legend.position = \"right\")+\n           ylab(\"Spending in USD\")+xlab(\"Department\")+\n           ggtitle(\"Total Federal R&D for Year : {frame_time}\")+\n           scale_color_continuous(breaks = seq(1976,2018,6),labels=seq(1976,2018,6))+\n           scale_y_continuous(labels = scales::scientific)+\n           facet_wrap(~funding,scales = \"free\")\n\nanimate(p,fps=1,nframes=42)\n\n\n\n\nTHANK YOU"
  },
  {
    "objectID": "tidytuesday/2019/Week_9/index.html",
    "href": "tidytuesday/2019/Week_9/index.html",
    "title": "Week 9 : French Train Delays",
    "section": "",
    "text": "# load the packages\nlibrary(readr)\nlibrary(tidyverse)\nlibrary(gganimate)\nlibrary(ggalluvial)\nlibrary(geomnet)\nlibrary(ggthemr)\n\n# load the theme\nggthemr(\"fresh\")\n\n# load the data\nsmall_trains &lt;- read_csv(\"small_trains.csv\")\nGitHub Code\nData set\n{{% tweet \"1101039210298003456\" %}}"
  },
  {
    "objectID": "tidytuesday/2019/Week_9/index.html#chosen-city-with-total-number-of-trips",
    "href": "tidytuesday/2019/Week_9/index.html#chosen-city-with-total-number-of-trips",
    "title": "Week 9 : French Train Delays",
    "section": "Chosen City with Total Number of Trips",
    "text": "Chosen City with Total Number of Trips\nTotal number of trips from Paris Montparnasse station to other cities is noted here. Four years of data with more accuracy by months is considered in this plot. There is clear variation in this data for cities.\nClose to 800 trips have been recorded towards the Bordeaux St Jean city but not clearly the same pattern for all years or months as well. Further, St Malo city has the lowest amount of trips compared to other cities in all fours but follows a centered pattern around the 100 mark.\n\np&lt;-ggplot(subset(small_trains,departure_station==\"PARIS MONTPARNASSE\"),\n       aes(x=str_wrap(arrival_station,20),y=total_num_trips,color=month))+\n       geom_jitter()+coord_flip()+ labs(color=\"Month\")+\n       transition_time(year)+ease_aes(\"linear\")+\n       scale_y_continuous(breaks = seq(0,800,100),labels=seq(0,800,100))+\n       xlab(\"Arrival Station\")+ylab(\"Total Number of Trips\")+\n       ggtitle(\"Paris Montparnasse and its arrival Stations\" ,subtitle =\"Year :{frame_time}\")\n\nanimate(p,nframes=4,fps=1)"
  },
  {
    "objectID": "tidytuesday/2019/Week_9/index.html#chosen-city-with-average-journey-time",
    "href": "tidytuesday/2019/Week_9/index.html#chosen-city-with-average-journey-time",
    "title": "Week 9 : French Train Delays",
    "section": "Chosen City with Average Journey Time",
    "text": "Chosen City with Average Journey Time\nExcept year 2017 all cities has similar and centered data points. In this exceptional year of 2017 we can see a difference between the first six months and rest. Where most of the Average journey times have been reduced, it is clear according to year 2018 points.\nCity of Toulouse Matabiau has the highest Average Journey Time, while lowest time goes to the city of Le Mans. So what happened after mid of year 2017.\nMaximum Average Journey time before mid of year 2017 is close to 325 but after this period it is centered around 275. The Minimum Average Journey time before mid of year 2017 and after also it is close to 50.\n\np&lt;-ggplot(subset(small_trains,departure_station==\"PARIS MONTPARNASSE\"),\n       aes(x=str_wrap(arrival_station,20),y=journey_time_avg,color=month))+\n       geom_jitter()+coord_flip()+ labs(color=\"Month\")+\n       transition_time(year)+ease_aes(\"linear\")+\n       scale_y_continuous(breaks=seq(0,350,25),labels=seq(0,350,25))+\n       xlab(\"Arrival Station\")+ylab(\"Average Journey Time\")+\n       ggtitle(\"Paris Montparnasse and its arrival Station\" ,subtitle =\"Year :{frame_time}\")\n\nanimate(p,nframes=4,fps=1)"
  },
  {
    "objectID": "tidytuesday/2019/Week_9/index.html#chosen-city-with-average-delay-with-all-departing",
    "href": "tidytuesday/2019/Week_9/index.html#chosen-city-with-average-delay-with-all-departing",
    "title": "Week 9 : French Train Delays",
    "section": "Chosen City with Average Delay with All Departing",
    "text": "Chosen City with Average Delay with All Departing\nThere is no clear pattern in perspective of months or years because data points are spread all over the place. Yet there is an odd occurring of negative values for average delay with all departing for some cities after mid of year 2017.\nWell none of these negative values does not exceed -2.5, where the maximum average delay in all departing is close to 5.5. I believe unit measured is in minutes.\n\np&lt;-ggplot(subset(small_trains,departure_station==\"PARIS MONTPARNASSE\"),\n       aes(x=str_wrap(arrival_station,20),y= avg_delay_all_departing,color=month))+\n       geom_jitter()+coord_flip()+ labs(color=\"Month\")+\n       scale_y_continuous(breaks=seq(-2.5,5.5,0.5),labels=seq(-2.5,5.5,0.5))+\n       transition_time(year)+ease_aes(\"linear\")+\n       geom_hline(yintercept = 0,color=\"red\")+\n       xlab(\"Arrival Station\")+ylab(\"Average Delay All Departing\")+\n       ggtitle(\"Paris Montparnasse and its arrival Station\" ,subtitle =\"Year :{frame_time}\")\n\nanimate(p,nframes=4,fps=1)"
  },
  {
    "objectID": "tidytuesday/2019/Week_9/index.html#chosen-city-with-average-delay-with-all-arriving",
    "href": "tidytuesday/2019/Week_9/index.html#chosen-city-with-average-delay-with-all-arriving",
    "title": "Week 9 : French Train Delays",
    "section": "Chosen City with Average Delay with All Arriving",
    "text": "Chosen City with Average Delay with All Arriving\nHighest delay could occur close to 18 minutes for average delay in Arriving and the lowest is close to -3. only in 2018 we see such negative values. These negative values occurs for the city of St Malo. Also there is no clear pattern in any city with relative to year or months.\n\np&lt;-ggplot(subset(small_trains,departure_station==\"PARIS MONTPARNASSE\"),\n       aes(x=str_wrap(arrival_station,20),y=avg_delay_all_arriving,color=month))+\n       geom_jitter()+coord_flip()+ labs(color=\"Month\")+\n       transition_time(year)+ease_aes(\"linear\")+\n       geom_hline(yintercept = 0,color=\"red\")+\n       scale_y_continuous(breaks=seq(-3,18),labels=seq(-3,18))+\n       xlab(\"Arrival Station\")+ylab(\"Average Delay All Arriving\")+\n       ggtitle(\"Paris Montparnasse and its arrival Station\" ,subtitle =\"Year :{frame_time}\")\n\nanimate(p,nframes=4,fps=1)"
  },
  {
    "objectID": "tidytuesday/2019/Week_9/index.html#chosen-city-with-number-of-late-departures",
    "href": "tidytuesday/2019/Week_9/index.html#chosen-city-with-number-of-late-departures",
    "title": "Week 9 : French Train Delays",
    "section": "Chosen City with Number of Late Departures",
    "text": "Chosen City with Number of Late Departures\nNumber of Late departures over the years increases for all cities. It is more clear for Bordeaux St Jean where the counts go beyond 150 and close to 200 in the year of 2018, but in 2015 the highest point is close to 75 for the same city.\nSt Malo has the lowest number of late departures where it fails to reach the count of 30 in all four years.\n\np&lt;-ggplot(subset(small_trains,departure_station==\"PARIS MONTPARNASSE\"),\n       aes(x=str_wrap(arrival_station,20),y=num_late_at_departure,\n           color=month))+\n       geom_jitter()+coord_flip()+ labs(color=\"Month\")+\n       scale_y_continuous(breaks=seq(0,200,25),labels=seq(0,200,25))+\n       transition_time(year)+ease_aes(\"linear\")+\n       xlab(\"Arrival Station\")+ylab(\"Number of Lates at Departure\")+\n       ggtitle(\"Paris Montparnasse and its arrival Station\" ,subtitle =\"Year :{frame_time}\")\n\nanimate(p,nframes=4,fps=1)"
  },
  {
    "objectID": "tidytuesday/2019/Week_9/index.html#chosen-city-with-number-of-late-arrivals",
    "href": "tidytuesday/2019/Week_9/index.html#chosen-city-with-number-of-late-arrivals",
    "title": "Week 9 : French Train Delays",
    "section": "Chosen City with Number of Late Arrivals",
    "text": "Chosen City with Number of Late Arrivals\nCity of St Malo has the lowest amount of late arrivals for all fours in general. Most amount of highest late arrivals occur in the city of Bordeaux St Jean. In year 2015 most of these data points are centered towards their specific values. In the next few years we can see that is not the case and they are with a lot of variation.\n\np&lt;-ggplot(subset(small_trains,departure_station==\"PARIS MONTPARNASSE\"),\n       aes(x=str_wrap(arrival_station,20),y=num_arriving_late,\n           color=month))+\n       geom_jitter()+coord_flip()+ labs(color=\"Month\")+\n       transition_time(year)+ease_aes(\"linear\")+\n       scale_y_continuous(breaks=seq(0,200,20),labels=seq(0,200,20))+\n       xlab(\"Arrival Station\")+ylab(\"Number of Lates at Arriving\")+\n       ggtitle(\"Paris Montparnasse and its arrival Station\" ,subtitle =\"Year :{frame_time}\")\n\nanimate(p,nframes=4,fps=1)"
  }
]