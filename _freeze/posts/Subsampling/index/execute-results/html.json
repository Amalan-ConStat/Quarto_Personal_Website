{
  "hash": "164b0d53be9bfc7b8fc3a48ed3edcf66",
  "result": {
    "markdown": "---\ntitle: \"Number of subsamples, that is sampling\"\ndate: '2023-02-05'\ncategories:\n  - Blog\n  - Subsampling\n  - Big data\ndraft: false\nnumber-sections: false\nformat:\n  html:\n    fig-cap-location: bottom\n---\n\n\nAnalysing big data is computationally intensive and to resolve this scalable methods are developed, for example subsampling.\nObtaining a subset of the big data based on a research aim and analysing this subset is called subsampling. \nObviously it is not possible to identify the most optimal or informative subsample, however there are methods that can provide sub-optimal subsamples.\nA common technique is identifying a distribution for the subsampling probabilities that assigned to each observation of the big data and obtain a subsample based on these probabilities.\n\nIn this blog post I explore the relationship between number of subsamples with the big data and subsample size. \nI consider two possible scenarios, 1) subsample without repetition and 2) subsample with repetition. \nThe number of subsamples for the first scenario can be obtained by $P(N,n)=\\frac{N!}{(N-n)!}$ and for the latter through $P_{Rep}(N,n)=\\frac{(N+n-1)!}{n!(N-1)!}$, where $N$ is the big data size and $n$ is the subsample size. \nUsing subsampling probabilities is similar to with repetition. \n\nComparing $N,n$ with $P(N,n)$ and $P_{Rep}(N,n)$ under big data is computationally impossible. \nAmid this to complete the comparison the following small values $N=40,50,\\ldots,100$ and $n_i=N_i/2,N_i/2+5,N_i/2+10,\\ldots,N_i-1$ are used.\nHere $n_i$ represents the $i$-th element of the sequence, starting from $N_i/2$ and increasing by $5$ each step until reaching or surpassing $N_i−1$. \nNote that the sequence ends before $N_i$, as it is specified to go up to $N_i−1$.\n\nBased on the setup the number of subsamples were calculated and are plotted below. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\nN_size<-seq(40,100,10)\nn_size<-Final_Result_No_Repetition<-Final_Result_Repetition<-list()\n\nfor (i in 1:length(N_size)) \n{\n  Temp_n_values<-as.matrix(seq((N_size[1]/2),(N_size[i]-1),5))\n  n_size<-apply(Temp_n_values,1,function(Temp_n_values){\n    factorial(N_size[i])/(factorial(Temp_n_values)*factorial(N_size[i]-Temp_n_values)) })\n  \n  Final_Result_No_Repetition[[i]]<-data.frame(\"Big_data_size\"=paste0(\"N = \",N_size[i]),\n                                              \"Subsample_size\"=Temp_n_values,\n                                              \"No_of_Subsamples\"=n_size)\n  \n  n_size<-apply(Temp_n_values,1,function(Temp_n_values){\n    factorial(N_size[i]+Temp_n_values -1)/(factorial(Temp_n_values)*factorial(N_size[i]-1)) })\n  \n  Final_Result_Repetition[[i]]<-data.frame(\"Big_data_size\"=factor(paste0(\"N = \",N_size[i])),\n                                           \"Subsample_size\"=Temp_n_values,\n                                           \"No_of_Subsamples\"=n_size)\n}\n\nFinal_Result_No_Repetition<-do.call(rbind.data.frame,Final_Result_No_Repetition)\nFinal_Result_Repetition<-do.call(rbind.data.frame,Final_Result_Repetition)\n\nFinal_Result_Repetition$Big_data_size<-factor(Final_Result_Repetition$Big_data_size,\n                                              levels = paste0(\"N = \",N_size),\n                                              labels = paste0(\"N = \",N_size))\n\nFinal_Result_No_Repetition$Big_data_size<-factor(Final_Result_No_Repetition$Big_data_size,\n                                                 levels = paste0(\"N = \",N_size),\n                                                 labels = paste0(\"N = \",N_size))\n```\n:::\n\n\nBased on subsampling without replacement it seems that the number of samples increase until the $n=N/2$, and after this the number of samples descreases.\nThis highest no of subsamples is still large even if $N$ and $n$ small values, for actual big data this value might not be calculable. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(Final_Result_No_Repetition,\n       aes(x=factor(Subsample_size),y=log10(No_of_Subsamples),group=Big_data_size,color=Big_data_size))+\n  geom_point()+geom_line()+\n  xlab(\"n (Subsample size)\")+ylab(\"log10(No of subsamples)\")+\n  ggtitle(\"Changes in number of subsamples for different big data and subsample sizes\",\n          subtitle = \"Subsamples without replacement\")+\n  scale_color_viridis_d()+labs(color=\"Big data size\")+\n  theme_bw()+theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/Subsampling without replacement-1.png){width=768}\n:::\n:::\n\n\nFor subsapling with replacement the number of subsamples increase gradually with the subsequent subsample sizes.\nCompared to subsampling without replacement with replacement has higher number of subsamples under the same set of big data and subsample sizes. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(Final_Result_Repetition,\n       aes(x=factor(Subsample_size),y=log10(No_of_Subsamples),group=Big_data_size,color=Big_data_size))+\n  geom_point()+geom_line()+\n  xlab(\"n (Subsample size)\")+ylab(\"log10(No of Subsamples)\")+\n  ggtitle(\"Changes in number of subsamples for different big data and subsample sizes\",\n          subtitle = \"Subsamples with replacement\")+\n  scale_color_viridis_d()+labs(color=\"Big data size\")+\n  theme_bw()+theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/Subsampling with replacement-1.png){width=768}\n:::\n:::\n\n\nSo in conclusion it is difficult to find an informative subsample as the number of subsamples increase exponentially. \nThis leads to the necessity of subsampling methods that efficiently identify subsamples. \n\n\nFurther we explore the the number of subsamples under a reasonable big data and different subsample sizes with the help of the R package *RcppAlgos*.\nHere rather than focusing on the number of subsamples we look at the number of digits in the possible number of subsamples. \nFor this we set $N=10000,20000,\\ldots,50000$ and $n=200,300,\\ldots,1500$, which are common values throughout articles related to subsampling for big data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(RcppAlgos)\n\nN_size<-c(1:5)*10000\nn_size<-c(2:15)*100\nFinal_Result_No_Repetition<-Final_Result_Repetition<-NULL\nFinal_Results<-list()\n\nfor (i in 1:length(N_size)) \n{\n  for (j in 1:length(n_size)) \n  {\n  Final_Result_No_Repetition[j]<-gmp::log10.bigz(comboCount(v=N_size[i],m=n_size[j],repetition=FALSE))\n  Final_Result_Repetition[j]<-gmp::log10.bigz(comboCount(v=N_size[i],m=n_size[j],repetition=TRUE))\n  }\n  \n  data.frame(\"Big_data_size\"=factor(N_size[i]),\"Subsample_size\"=rep(n_size,2),\n             \"Type\"=factor(rep(c(\"Without Repetition\",\"With Repetition\"),each=length(n_size))),\n             \"No_of_digits\"=c(Final_Result_No_Repetition,Final_Result_Repetition))->Final_Results[[i]]\n}\n\nFinal_Results<-do.call(rbind,Final_Results)\n\nggplot(Final_Results,aes(x=factor(Subsample_size),y=No_of_digits,color=Big_data_size,shape=Type,linetype=Type,\n                         group=interaction(Big_data_size,Type)))+\n  geom_point()+geom_line()+\n  scale_shape_manual(values = c(8,16) )+scale_linetype_manual(values = c(\"dotted\",\"dashed\"))+\n  xlab(\"n (Subsample size)\")+ylab(\"log10(No of digits)\")+\n  ggtitle(\"Changes in number of subsamples for different big data and subsample sizes\",\n          subtitle = \"Subsamples with and without replacement\")+\n  scale_color_viridis_d()+labs(color=\"With/without repetition\")+\n  theme_bw()+theme(legend.position = \"bottom\") +\n  guides(color=guide_legend(nrow = 2))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/Calculate the number of subsamples big data-1.png){width=720}\n:::\n:::\n\n\nAccording to the above plot the number of subsamples by with repetition has more number of digits than without repetition.\nThe number of digits increase when the subsample size increase and they further have higher values when the big data size is larger as well.\nFor example, for $N=50000$ and $n=400$ the number of subsamples has close to 1000 digits under log scale of power 10, that is the number of subsamples is close to $10^1000$. \nFor clarity 1 trillion is $10^12$, hence why we need subsampling methods as it is impossible to find the best subsample through going through all possible subsamples.\n\n*THANK YOU*",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}